{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hdf5 is not supported on this machine (please install/reinstall h5py for optimal experience)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tflearn as tflearn\n",
    "from clean_data import clean_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>Moran, Mr. James</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>330877</td>\n",
       "      <td>8.4583</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>McCarthy, Mr. Timothy J</td>\n",
       "      <td>male</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17463</td>\n",
       "      <td>51.8625</td>\n",
       "      <td>E46</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>Palsson, Master. Gosta Leonard</td>\n",
       "      <td>male</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>349909</td>\n",
       "      <td>21.0750</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)</td>\n",
       "      <td>female</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>347742</td>\n",
       "      <td>11.1333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>Nasser, Mrs. Nicholas (Adele Achem)</td>\n",
       "      <td>female</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>237736</td>\n",
       "      <td>30.0708</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>Sandstrom, Miss. Marguerite Rut</td>\n",
       "      <td>female</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>PP 9549</td>\n",
       "      <td>16.7000</td>\n",
       "      <td>G6</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>Bonnell, Miss. Elizabeth</td>\n",
       "      <td>female</td>\n",
       "      <td>58.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>113783</td>\n",
       "      <td>26.5500</td>\n",
       "      <td>C103</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>Saundercock, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5. 2151</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "      <td>Andersson, Mr. Anders Johan</td>\n",
       "      <td>male</td>\n",
       "      <td>39.0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>347082</td>\n",
       "      <td>31.2750</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>Vestrom, Miss. Hulda Amanda Adolfina</td>\n",
       "      <td>female</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>350406</td>\n",
       "      <td>7.8542</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>Hewlett, Mrs. (Mary D Kingcome)</td>\n",
       "      <td>female</td>\n",
       "      <td>55.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>248706</td>\n",
       "      <td>16.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>3</td>\n",
       "      <td>Rice, Master. Eugene</td>\n",
       "      <td>male</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>382652</td>\n",
       "      <td>29.1250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>2</td>\n",
       "      <td>Williams, Mr. Charles Eugene</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>244373</td>\n",
       "      <td>13.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>3</td>\n",
       "      <td>Vander Planke, Mrs. Julius (Emelia Maria Vande...</td>\n",
       "      <td>female</td>\n",
       "      <td>31.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>345763</td>\n",
       "      <td>18.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>3</td>\n",
       "      <td>Masselmani, Mrs. Fatima</td>\n",
       "      <td>female</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2649</td>\n",
       "      <td>7.2250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>2</td>\n",
       "      <td>Fynney, Mr. Joseph J</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>239865</td>\n",
       "      <td>26.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>2</td>\n",
       "      <td>Beesley, Mr. Lawrence</td>\n",
       "      <td>male</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>248698</td>\n",
       "      <td>13.0000</td>\n",
       "      <td>D56</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>3</td>\n",
       "      <td>McGowan, Miss. Anna \"Annie\"</td>\n",
       "      <td>female</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>330923</td>\n",
       "      <td>8.0292</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>Sloper, Mr. William Thompson</td>\n",
       "      <td>male</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>113788</td>\n",
       "      <td>35.5000</td>\n",
       "      <td>A6</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>3</td>\n",
       "      <td>Palsson, Miss. Torborg Danira</td>\n",
       "      <td>female</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>349909</td>\n",
       "      <td>21.0750</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>3</td>\n",
       "      <td>Asplund, Mrs. Carl Oscar (Selma Augusta Emilia...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>347077</td>\n",
       "      <td>31.3875</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>3</td>\n",
       "      <td>Emir, Mr. Farred Chehab</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2631</td>\n",
       "      <td>7.2250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "      <td>Fortune, Mr. Charles Alexander</td>\n",
       "      <td>male</td>\n",
       "      <td>19.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>19950</td>\n",
       "      <td>263.0000</td>\n",
       "      <td>C23 C25 C27</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29</td>\n",
       "      <td>3</td>\n",
       "      <td>O'Dwyer, Miss. Ellen \"Nellie\"</td>\n",
       "      <td>female</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>330959</td>\n",
       "      <td>7.8792</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30</td>\n",
       "      <td>3</td>\n",
       "      <td>Todoroff, Mr. Lalio</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>349216</td>\n",
       "      <td>7.8958</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>861</th>\n",
       "      <td>862</td>\n",
       "      <td>2</td>\n",
       "      <td>Giles, Mr. Frederick Edward</td>\n",
       "      <td>male</td>\n",
       "      <td>21.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>28134</td>\n",
       "      <td>11.5000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>862</th>\n",
       "      <td>863</td>\n",
       "      <td>1</td>\n",
       "      <td>Swift, Mrs. Frederick Joel (Margaret Welles Ba...</td>\n",
       "      <td>female</td>\n",
       "      <td>48.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17466</td>\n",
       "      <td>25.9292</td>\n",
       "      <td>D17</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>863</th>\n",
       "      <td>864</td>\n",
       "      <td>3</td>\n",
       "      <td>Sage, Miss. Dorothy Edith \"Dolly\"</td>\n",
       "      <td>female</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>CA. 2343</td>\n",
       "      <td>69.5500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>864</th>\n",
       "      <td>865</td>\n",
       "      <td>2</td>\n",
       "      <td>Gill, Mr. John William</td>\n",
       "      <td>male</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>233866</td>\n",
       "      <td>13.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>865</th>\n",
       "      <td>866</td>\n",
       "      <td>2</td>\n",
       "      <td>Bystrom, Mrs. (Karolina)</td>\n",
       "      <td>female</td>\n",
       "      <td>42.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>236852</td>\n",
       "      <td>13.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>866</th>\n",
       "      <td>867</td>\n",
       "      <td>2</td>\n",
       "      <td>Duran y More, Miss. Asuncion</td>\n",
       "      <td>female</td>\n",
       "      <td>27.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>SC/PARIS 2149</td>\n",
       "      <td>13.8583</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>867</th>\n",
       "      <td>868</td>\n",
       "      <td>1</td>\n",
       "      <td>Roebling, Mr. Washington Augustus II</td>\n",
       "      <td>male</td>\n",
       "      <td>31.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17590</td>\n",
       "      <td>50.4958</td>\n",
       "      <td>A24</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>868</th>\n",
       "      <td>869</td>\n",
       "      <td>3</td>\n",
       "      <td>van Melkebeke, Mr. Philemon</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>345777</td>\n",
       "      <td>9.5000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>869</th>\n",
       "      <td>870</td>\n",
       "      <td>3</td>\n",
       "      <td>Johnson, Master. Harold Theodor</td>\n",
       "      <td>male</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>347742</td>\n",
       "      <td>11.1333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>870</th>\n",
       "      <td>871</td>\n",
       "      <td>3</td>\n",
       "      <td>Balkic, Mr. Cerin</td>\n",
       "      <td>male</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>349248</td>\n",
       "      <td>7.8958</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>871</th>\n",
       "      <td>872</td>\n",
       "      <td>1</td>\n",
       "      <td>Beckwith, Mrs. Richard Leonard (Sallie Monypeny)</td>\n",
       "      <td>female</td>\n",
       "      <td>47.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>11751</td>\n",
       "      <td>52.5542</td>\n",
       "      <td>D35</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>872</th>\n",
       "      <td>873</td>\n",
       "      <td>1</td>\n",
       "      <td>Carlsson, Mr. Frans Olof</td>\n",
       "      <td>male</td>\n",
       "      <td>33.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>695</td>\n",
       "      <td>5.0000</td>\n",
       "      <td>B51 B53 B55</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>873</th>\n",
       "      <td>874</td>\n",
       "      <td>3</td>\n",
       "      <td>Vander Cruyssen, Mr. Victor</td>\n",
       "      <td>male</td>\n",
       "      <td>47.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>345765</td>\n",
       "      <td>9.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>874</th>\n",
       "      <td>875</td>\n",
       "      <td>2</td>\n",
       "      <td>Abelson, Mrs. Samuel (Hannah Wizosky)</td>\n",
       "      <td>female</td>\n",
       "      <td>28.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>P/PP 3381</td>\n",
       "      <td>24.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>875</th>\n",
       "      <td>876</td>\n",
       "      <td>3</td>\n",
       "      <td>Najib, Miss. Adele Kiamie \"Jane\"</td>\n",
       "      <td>female</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2667</td>\n",
       "      <td>7.2250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>876</th>\n",
       "      <td>877</td>\n",
       "      <td>3</td>\n",
       "      <td>Gustafsson, Mr. Alfred Ossian</td>\n",
       "      <td>male</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7534</td>\n",
       "      <td>9.8458</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>877</th>\n",
       "      <td>878</td>\n",
       "      <td>3</td>\n",
       "      <td>Petroff, Mr. Nedelio</td>\n",
       "      <td>male</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>349212</td>\n",
       "      <td>7.8958</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>878</th>\n",
       "      <td>879</td>\n",
       "      <td>3</td>\n",
       "      <td>Laleff, Mr. Kristo</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>349217</td>\n",
       "      <td>7.8958</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>879</th>\n",
       "      <td>880</td>\n",
       "      <td>1</td>\n",
       "      <td>Potter, Mrs. Thomas Jr (Lily Alexenia Wilson)</td>\n",
       "      <td>female</td>\n",
       "      <td>56.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>11767</td>\n",
       "      <td>83.1583</td>\n",
       "      <td>C50</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>880</th>\n",
       "      <td>881</td>\n",
       "      <td>2</td>\n",
       "      <td>Shelley, Mrs. William (Imanita Parrish Hall)</td>\n",
       "      <td>female</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>230433</td>\n",
       "      <td>26.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>881</th>\n",
       "      <td>882</td>\n",
       "      <td>3</td>\n",
       "      <td>Markun, Mr. Johann</td>\n",
       "      <td>male</td>\n",
       "      <td>33.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>349257</td>\n",
       "      <td>7.8958</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>882</th>\n",
       "      <td>883</td>\n",
       "      <td>3</td>\n",
       "      <td>Dahlberg, Miss. Gerda Ulrika</td>\n",
       "      <td>female</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7552</td>\n",
       "      <td>10.5167</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>883</th>\n",
       "      <td>884</td>\n",
       "      <td>2</td>\n",
       "      <td>Banfield, Mr. Frederick James</td>\n",
       "      <td>male</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>C.A./SOTON 34068</td>\n",
       "      <td>10.5000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>884</th>\n",
       "      <td>885</td>\n",
       "      <td>3</td>\n",
       "      <td>Sutehall, Mr. Henry Jr</td>\n",
       "      <td>male</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>SOTON/OQ 392076</td>\n",
       "      <td>7.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>885</th>\n",
       "      <td>886</td>\n",
       "      <td>3</td>\n",
       "      <td>Rice, Mrs. William (Margaret Norton)</td>\n",
       "      <td>female</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>382652</td>\n",
       "      <td>29.1250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>886</th>\n",
       "      <td>887</td>\n",
       "      <td>2</td>\n",
       "      <td>Montvila, Rev. Juozas</td>\n",
       "      <td>male</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>211536</td>\n",
       "      <td>13.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>888</td>\n",
       "      <td>1</td>\n",
       "      <td>Graham, Miss. Margaret Edith</td>\n",
       "      <td>female</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>112053</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>B42</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>889</td>\n",
       "      <td>3</td>\n",
       "      <td>Johnston, Miss. Catherine Helen \"Carrie\"</td>\n",
       "      <td>female</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>W./C. 6607</td>\n",
       "      <td>23.4500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>890</td>\n",
       "      <td>1</td>\n",
       "      <td>Behr, Mr. Karl Howell</td>\n",
       "      <td>male</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>111369</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>C148</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>891</td>\n",
       "      <td>3</td>\n",
       "      <td>Dooley, Mr. Patrick</td>\n",
       "      <td>male</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>370376</td>\n",
       "      <td>7.7500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>891 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     PassengerId  Pclass                                               Name  \\\n",
       "0              1       3                            Braund, Mr. Owen Harris   \n",
       "1              2       1  Cumings, Mrs. John Bradley (Florence Briggs Th...   \n",
       "2              3       3                             Heikkinen, Miss. Laina   \n",
       "3              4       1       Futrelle, Mrs. Jacques Heath (Lily May Peel)   \n",
       "4              5       3                           Allen, Mr. William Henry   \n",
       "5              6       3                                   Moran, Mr. James   \n",
       "6              7       1                            McCarthy, Mr. Timothy J   \n",
       "7              8       3                     Palsson, Master. Gosta Leonard   \n",
       "8              9       3  Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)   \n",
       "9             10       2                Nasser, Mrs. Nicholas (Adele Achem)   \n",
       "10            11       3                    Sandstrom, Miss. Marguerite Rut   \n",
       "11            12       1                           Bonnell, Miss. Elizabeth   \n",
       "12            13       3                     Saundercock, Mr. William Henry   \n",
       "13            14       3                        Andersson, Mr. Anders Johan   \n",
       "14            15       3               Vestrom, Miss. Hulda Amanda Adolfina   \n",
       "15            16       2                   Hewlett, Mrs. (Mary D Kingcome)    \n",
       "16            17       3                               Rice, Master. Eugene   \n",
       "17            18       2                       Williams, Mr. Charles Eugene   \n",
       "18            19       3  Vander Planke, Mrs. Julius (Emelia Maria Vande...   \n",
       "19            20       3                            Masselmani, Mrs. Fatima   \n",
       "20            21       2                               Fynney, Mr. Joseph J   \n",
       "21            22       2                              Beesley, Mr. Lawrence   \n",
       "22            23       3                        McGowan, Miss. Anna \"Annie\"   \n",
       "23            24       1                       Sloper, Mr. William Thompson   \n",
       "24            25       3                      Palsson, Miss. Torborg Danira   \n",
       "25            26       3  Asplund, Mrs. Carl Oscar (Selma Augusta Emilia...   \n",
       "26            27       3                            Emir, Mr. Farred Chehab   \n",
       "27            28       1                     Fortune, Mr. Charles Alexander   \n",
       "28            29       3                      O'Dwyer, Miss. Ellen \"Nellie\"   \n",
       "29            30       3                                Todoroff, Mr. Lalio   \n",
       "..           ...     ...                                                ...   \n",
       "861          862       2                        Giles, Mr. Frederick Edward   \n",
       "862          863       1  Swift, Mrs. Frederick Joel (Margaret Welles Ba...   \n",
       "863          864       3                  Sage, Miss. Dorothy Edith \"Dolly\"   \n",
       "864          865       2                             Gill, Mr. John William   \n",
       "865          866       2                           Bystrom, Mrs. (Karolina)   \n",
       "866          867       2                       Duran y More, Miss. Asuncion   \n",
       "867          868       1               Roebling, Mr. Washington Augustus II   \n",
       "868          869       3                        van Melkebeke, Mr. Philemon   \n",
       "869          870       3                    Johnson, Master. Harold Theodor   \n",
       "870          871       3                                  Balkic, Mr. Cerin   \n",
       "871          872       1   Beckwith, Mrs. Richard Leonard (Sallie Monypeny)   \n",
       "872          873       1                           Carlsson, Mr. Frans Olof   \n",
       "873          874       3                        Vander Cruyssen, Mr. Victor   \n",
       "874          875       2              Abelson, Mrs. Samuel (Hannah Wizosky)   \n",
       "875          876       3                   Najib, Miss. Adele Kiamie \"Jane\"   \n",
       "876          877       3                      Gustafsson, Mr. Alfred Ossian   \n",
       "877          878       3                               Petroff, Mr. Nedelio   \n",
       "878          879       3                                 Laleff, Mr. Kristo   \n",
       "879          880       1      Potter, Mrs. Thomas Jr (Lily Alexenia Wilson)   \n",
       "880          881       2       Shelley, Mrs. William (Imanita Parrish Hall)   \n",
       "881          882       3                                 Markun, Mr. Johann   \n",
       "882          883       3                       Dahlberg, Miss. Gerda Ulrika   \n",
       "883          884       2                      Banfield, Mr. Frederick James   \n",
       "884          885       3                             Sutehall, Mr. Henry Jr   \n",
       "885          886       3               Rice, Mrs. William (Margaret Norton)   \n",
       "886          887       2                              Montvila, Rev. Juozas   \n",
       "887          888       1                       Graham, Miss. Margaret Edith   \n",
       "888          889       3           Johnston, Miss. Catherine Helen \"Carrie\"   \n",
       "889          890       1                              Behr, Mr. Karl Howell   \n",
       "890          891       3                                Dooley, Mr. Patrick   \n",
       "\n",
       "        Sex   Age  SibSp  Parch            Ticket      Fare        Cabin  \\\n",
       "0      male  22.0      1      0         A/5 21171    7.2500          NaN   \n",
       "1    female  38.0      1      0          PC 17599   71.2833          C85   \n",
       "2    female  26.0      0      0  STON/O2. 3101282    7.9250          NaN   \n",
       "3    female  35.0      1      0            113803   53.1000         C123   \n",
       "4      male  35.0      0      0            373450    8.0500          NaN   \n",
       "5      male   NaN      0      0            330877    8.4583          NaN   \n",
       "6      male  54.0      0      0             17463   51.8625          E46   \n",
       "7      male   2.0      3      1            349909   21.0750          NaN   \n",
       "8    female  27.0      0      2            347742   11.1333          NaN   \n",
       "9    female  14.0      1      0            237736   30.0708          NaN   \n",
       "10   female   4.0      1      1           PP 9549   16.7000           G6   \n",
       "11   female  58.0      0      0            113783   26.5500         C103   \n",
       "12     male  20.0      0      0         A/5. 2151    8.0500          NaN   \n",
       "13     male  39.0      1      5            347082   31.2750          NaN   \n",
       "14   female  14.0      0      0            350406    7.8542          NaN   \n",
       "15   female  55.0      0      0            248706   16.0000          NaN   \n",
       "16     male   2.0      4      1            382652   29.1250          NaN   \n",
       "17     male   NaN      0      0            244373   13.0000          NaN   \n",
       "18   female  31.0      1      0            345763   18.0000          NaN   \n",
       "19   female   NaN      0      0              2649    7.2250          NaN   \n",
       "20     male  35.0      0      0            239865   26.0000          NaN   \n",
       "21     male  34.0      0      0            248698   13.0000          D56   \n",
       "22   female  15.0      0      0            330923    8.0292          NaN   \n",
       "23     male  28.0      0      0            113788   35.5000           A6   \n",
       "24   female   8.0      3      1            349909   21.0750          NaN   \n",
       "25   female  38.0      1      5            347077   31.3875          NaN   \n",
       "26     male   NaN      0      0              2631    7.2250          NaN   \n",
       "27     male  19.0      3      2             19950  263.0000  C23 C25 C27   \n",
       "28   female   NaN      0      0            330959    7.8792          NaN   \n",
       "29     male   NaN      0      0            349216    7.8958          NaN   \n",
       "..      ...   ...    ...    ...               ...       ...          ...   \n",
       "861    male  21.0      1      0             28134   11.5000          NaN   \n",
       "862  female  48.0      0      0             17466   25.9292          D17   \n",
       "863  female   NaN      8      2          CA. 2343   69.5500          NaN   \n",
       "864    male  24.0      0      0            233866   13.0000          NaN   \n",
       "865  female  42.0      0      0            236852   13.0000          NaN   \n",
       "866  female  27.0      1      0     SC/PARIS 2149   13.8583          NaN   \n",
       "867    male  31.0      0      0          PC 17590   50.4958          A24   \n",
       "868    male   NaN      0      0            345777    9.5000          NaN   \n",
       "869    male   4.0      1      1            347742   11.1333          NaN   \n",
       "870    male  26.0      0      0            349248    7.8958          NaN   \n",
       "871  female  47.0      1      1             11751   52.5542          D35   \n",
       "872    male  33.0      0      0               695    5.0000  B51 B53 B55   \n",
       "873    male  47.0      0      0            345765    9.0000          NaN   \n",
       "874  female  28.0      1      0         P/PP 3381   24.0000          NaN   \n",
       "875  female  15.0      0      0              2667    7.2250          NaN   \n",
       "876    male  20.0      0      0              7534    9.8458          NaN   \n",
       "877    male  19.0      0      0            349212    7.8958          NaN   \n",
       "878    male   NaN      0      0            349217    7.8958          NaN   \n",
       "879  female  56.0      0      1             11767   83.1583          C50   \n",
       "880  female  25.0      0      1            230433   26.0000          NaN   \n",
       "881    male  33.0      0      0            349257    7.8958          NaN   \n",
       "882  female  22.0      0      0              7552   10.5167          NaN   \n",
       "883    male  28.0      0      0  C.A./SOTON 34068   10.5000          NaN   \n",
       "884    male  25.0      0      0   SOTON/OQ 392076    7.0500          NaN   \n",
       "885  female  39.0      0      5            382652   29.1250          NaN   \n",
       "886    male  27.0      0      0            211536   13.0000          NaN   \n",
       "887  female  19.0      0      0            112053   30.0000          B42   \n",
       "888  female   NaN      1      2        W./C. 6607   23.4500          NaN   \n",
       "889    male  26.0      0      0            111369   30.0000         C148   \n",
       "890    male  32.0      0      0            370376    7.7500          NaN   \n",
       "\n",
       "    Embarked  \n",
       "0          S  \n",
       "1          C  \n",
       "2          S  \n",
       "3          S  \n",
       "4          S  \n",
       "5          Q  \n",
       "6          S  \n",
       "7          S  \n",
       "8          S  \n",
       "9          C  \n",
       "10         S  \n",
       "11         S  \n",
       "12         S  \n",
       "13         S  \n",
       "14         S  \n",
       "15         S  \n",
       "16         Q  \n",
       "17         S  \n",
       "18         S  \n",
       "19         C  \n",
       "20         S  \n",
       "21         S  \n",
       "22         Q  \n",
       "23         S  \n",
       "24         S  \n",
       "25         S  \n",
       "26         C  \n",
       "27         S  \n",
       "28         Q  \n",
       "29         S  \n",
       "..       ...  \n",
       "861        S  \n",
       "862        S  \n",
       "863        S  \n",
       "864        S  \n",
       "865        S  \n",
       "866        C  \n",
       "867        S  \n",
       "868        S  \n",
       "869        S  \n",
       "870        S  \n",
       "871        S  \n",
       "872        S  \n",
       "873        S  \n",
       "874        C  \n",
       "875        C  \n",
       "876        S  \n",
       "877        S  \n",
       "878        S  \n",
       "879        C  \n",
       "880        S  \n",
       "881        S  \n",
       "882        S  \n",
       "883        S  \n",
       "884        S  \n",
       "885        Q  \n",
       "886        S  \n",
       "887        S  \n",
       "888        S  \n",
       "889        C  \n",
       "890        Q  \n",
       "\n",
       "[891 rows x 11 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../titanic_kaggle/train.csv')\n",
    "labels = pd.get_dummies(df.Survived)\n",
    "df = df.drop('Survived', 1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 22.        ,   1.        ,   0.        , ...,   1.        ,\n",
       "          0.        ,   1.        ],\n",
       "       [ 38.        ,   1.        ,   0.        , ...,   0.        ,\n",
       "          0.        ,   0.        ],\n",
       "       [ 26.        ,   0.        ,   0.        , ...,   1.        ,\n",
       "          0.        ,   1.        ],\n",
       "       ..., \n",
       "       [ 29.69911766,   1.        ,   2.        , ...,   1.        ,\n",
       "          0.        ,   1.        ],\n",
       "       [ 26.        ,   0.        ,   0.        , ...,   0.        ,\n",
       "          0.        ,   0.        ],\n",
       "       [ 32.        ,   0.        ,   0.        , ...,   0.        ,\n",
       "          0.        ,   1.        ]], dtype=float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_csv('../titanic_kaggle/test.csv')\n",
    "df = clean_data(df)\n",
    "test = clean_data(test)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels = np.array(labels, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "Run id: AJGFGU\n",
      "Log directory: /tmp/tflearn_logs/\n",
      "INFO:tensorflow:Summary name Accuracy/ (raw) is illegal; using Accuracy/__raw_ instead.\n",
      "---------------------------------\n",
      "Training samples: 891\n",
      "Validation samples: 0\n",
      "--\n",
      "Training Step: 1  | time: 0.125s\n",
      "| Adam | epoch: 001 | loss: 0.00000 - acc: 0.0000 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2  | total loss: \u001b[1m\u001b[32m0.62382\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 002 | loss: 0.62382 - acc: 0.5455 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3  | total loss: \u001b[1m\u001b[32m0.68006\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 003 | loss: 0.68006 - acc: 0.6033 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4  | total loss: \u001b[1m\u001b[32m0.68856\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 004 | loss: 0.68856 - acc: 0.6129 -- iter: 891/891\n",
      "--\n",
      "Training Step: 5  | total loss: \u001b[1m\u001b[32m0.68981\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 005 | loss: 0.68981 - acc: 0.6152 -- iter: 891/891\n",
      "--\n",
      "Training Step: 6  | total loss: \u001b[1m\u001b[32m0.68813\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 006 | loss: 0.68813 - acc: 0.6158 -- iter: 891/891\n",
      "--\n",
      "Training Step: 7  | total loss: \u001b[1m\u001b[32m0.68647\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 007 | loss: 0.68647 - acc: 0.6160 -- iter: 891/891\n",
      "--\n",
      "Training Step: 8  | total loss: \u001b[1m\u001b[32m0.68283\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 008 | loss: 0.68283 - acc: 0.6161 -- iter: 891/891\n",
      "--\n",
      "Training Step: 9  | total loss: \u001b[1m\u001b[32m0.67947\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 009 | loss: 0.67947 - acc: 0.6161 -- iter: 891/891\n",
      "--\n",
      "Training Step: 10  | total loss: \u001b[1m\u001b[32m0.67394\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 010 | loss: 0.67394 - acc: 0.6161 -- iter: 891/891\n",
      "--\n",
      "Training Step: 11  | total loss: \u001b[1m\u001b[32m0.67288\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 011 | loss: 0.67288 - acc: 0.6162 -- iter: 891/891\n",
      "--\n",
      "Training Step: 12  | total loss: \u001b[1m\u001b[32m0.67379\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 012 | loss: 0.67379 - acc: 0.6162 -- iter: 891/891\n",
      "--\n",
      "Training Step: 13  | total loss: \u001b[1m\u001b[32m0.66521\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 013 | loss: 0.66521 - acc: 0.6181 -- iter: 891/891\n",
      "--\n",
      "Training Step: 14  | total loss: \u001b[1m\u001b[32m0.66732\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 014 | loss: 0.66732 - acc: 0.6173 -- iter: 891/891\n",
      "--\n",
      "Training Step: 15  | total loss: \u001b[1m\u001b[32m0.65889\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 015 | loss: 0.65889 - acc: 0.6243 -- iter: 891/891\n",
      "--\n",
      "Training Step: 16  | total loss: \u001b[1m\u001b[32m0.66535\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 016 | loss: 0.66535 - acc: 0.6133 -- iter: 891/891\n",
      "--\n",
      "Training Step: 17  | total loss: \u001b[1m\u001b[32m0.65532\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 017 | loss: 0.65532 - acc: 0.6280 -- iter: 891/891\n",
      "--\n",
      "Training Step: 18  | total loss: \u001b[1m\u001b[32m0.66286\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 018 | loss: 0.66286 - acc: 0.6189 -- iter: 891/891\n",
      "--\n",
      "Training Step: 19  | total loss: \u001b[1m\u001b[32m0.65135\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 019 | loss: 0.65135 - acc: 0.6352 -- iter: 891/891\n",
      "--\n",
      "Training Step: 20  | total loss: \u001b[1m\u001b[32m0.66474\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 020 | loss: 0.66474 - acc: 0.6172 -- iter: 891/891\n",
      "--\n",
      "Training Step: 21  | total loss: \u001b[1m\u001b[32m0.65216\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 021 | loss: 0.65216 - acc: 0.6325 -- iter: 891/891\n",
      "--\n",
      "Training Step: 22  | total loss: \u001b[1m\u001b[32m0.66532\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 022 | loss: 0.66532 - acc: 0.6216 -- iter: 891/891\n",
      "--\n",
      "Training Step: 23  | total loss: \u001b[1m\u001b[32m0.65391\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 023 | loss: 0.65391 - acc: 0.6320 -- iter: 891/891\n",
      "--\n",
      "Training Step: 24  | total loss: \u001b[1m\u001b[32m0.65887\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 024 | loss: 0.65887 - acc: 0.6244 -- iter: 891/891\n",
      "--\n",
      "Training Step: 25  | total loss: \u001b[1m\u001b[32m0.65103\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 025 | loss: 0.65103 - acc: 0.6320 -- iter: 891/891\n",
      "--\n",
      "Training Step: 26  | total loss: \u001b[1m\u001b[32m0.65553\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 026 | loss: 0.65553 - acc: 0.6290 -- iter: 891/891\n",
      "--\n",
      "Training Step: 27  | total loss: \u001b[1m\u001b[32m0.64978\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 027 | loss: 0.64978 - acc: 0.6335 -- iter: 891/891\n",
      "--\n",
      "Training Step: 28  | total loss: \u001b[1m\u001b[32m0.65535\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 028 | loss: 0.65535 - acc: 0.6277 -- iter: 891/891\n",
      "--\n",
      "Training Step: 29  | total loss: \u001b[1m\u001b[32m0.65030\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 029 | loss: 0.65030 - acc: 0.6304 -- iter: 891/891\n",
      "--\n",
      "Training Step: 30  | total loss: \u001b[1m\u001b[32m0.65407\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 030 | loss: 0.65407 - acc: 0.6246 -- iter: 891/891\n",
      "--\n",
      "Training Step: 31  | total loss: \u001b[1m\u001b[32m0.64950\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 031 | loss: 0.64950 - acc: 0.6278 -- iter: 891/891\n",
      "--\n",
      "Training Step: 32  | total loss: \u001b[1m\u001b[32m0.65420\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 032 | loss: 0.65420 - acc: 0.6235 -- iter: 891/891\n",
      "--\n",
      "Training Step: 33  | total loss: \u001b[1m\u001b[32m0.64949\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 033 | loss: 0.64949 - acc: 0.6268 -- iter: 891/891\n",
      "--\n",
      "Training Step: 34  | total loss: \u001b[1m\u001b[32m0.65493\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 034 | loss: 0.65493 - acc: 0.6233 -- iter: 891/891\n",
      "--\n",
      "Training Step: 35  | total loss: \u001b[1m\u001b[32m0.64991\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 035 | loss: 0.64991 - acc: 0.6267 -- iter: 891/891\n",
      "--\n",
      "Training Step: 36  | total loss: \u001b[1m\u001b[32m0.65529\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 036 | loss: 0.65529 - acc: 0.6227 -- iter: 891/891\n",
      "--\n",
      "Training Step: 37  | total loss: \u001b[1m\u001b[32m0.65016\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 037 | loss: 0.65016 - acc: 0.6273 -- iter: 891/891\n",
      "--\n",
      "Training Step: 38  | total loss: \u001b[1m\u001b[32m0.64587\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 038 | loss: 0.64587 - acc: 0.6310 -- iter: 891/891\n",
      "--\n",
      "Training Step: 39  | total loss: \u001b[1m\u001b[32m0.64193\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 039 | loss: 0.64193 - acc: 0.6357 -- iter: 891/891\n",
      "--\n",
      "Training Step: 40  | total loss: \u001b[1m\u001b[32m0.63805\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 040 | loss: 0.63805 - acc: 0.6402 -- iter: 891/891\n",
      "--\n",
      "Training Step: 41  | total loss: \u001b[1m\u001b[32m0.63416\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 041 | loss: 0.63416 - acc: 0.6459 -- iter: 891/891\n",
      "--\n",
      "Training Step: 42  | total loss: \u001b[1m\u001b[32m0.63040\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 042 | loss: 0.63040 - acc: 0.6513 -- iter: 891/891\n",
      "--\n",
      "Training Step: 43  | total loss: \u001b[1m\u001b[32m0.62700\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 043 | loss: 0.62700 - acc: 0.6568 -- iter: 891/891\n",
      "--\n",
      "Training Step: 44  | total loss: \u001b[1m\u001b[32m0.62424\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 044 | loss: 0.62424 - acc: 0.6631 -- iter: 891/891\n",
      "--\n",
      "Training Step: 45  | total loss: \u001b[1m\u001b[32m0.62224\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 045 | loss: 0.62224 - acc: 0.6683 -- iter: 891/891\n",
      "--\n",
      "Training Step: 46  | total loss: \u001b[1m\u001b[32m0.62087\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 046 | loss: 0.62087 - acc: 0.6731 -- iter: 891/891\n",
      "--\n",
      "Training Step: 47  | total loss: \u001b[1m\u001b[32m0.61968\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 047 | loss: 0.61968 - acc: 0.6763 -- iter: 891/891\n",
      "--\n",
      "Training Step: 48  | total loss: \u001b[1m\u001b[32m0.64705\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 048 | loss: 0.64705 - acc: 0.6587 -- iter: 891/891\n",
      "--\n",
      "Training Step: 49  | total loss: \u001b[1m\u001b[32m0.64077\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 049 | loss: 0.64077 - acc: 0.6652 -- iter: 891/891\n",
      "--\n",
      "Training Step: 50  | total loss: \u001b[1m\u001b[32m0.63593\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 050 | loss: 0.63593 - acc: 0.6672 -- iter: 891/891\n",
      "--\n",
      "Training Step: 51  | total loss: \u001b[1m\u001b[32m0.63254\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 051 | loss: 0.63254 - acc: 0.6680 -- iter: 891/891\n",
      "--\n",
      "Training Step: 52  | total loss: \u001b[1m\u001b[32m0.63025\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 052 | loss: 0.63025 - acc: 0.6676 -- iter: 891/891\n",
      "--\n",
      "Training Step: 53  | total loss: \u001b[1m\u001b[32m0.62865\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 053 | loss: 0.62865 - acc: 0.6658 -- iter: 891/891\n",
      "--\n",
      "Training Step: 54  | total loss: \u001b[1m\u001b[32m0.62744\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 054 | loss: 0.62744 - acc: 0.6635 -- iter: 891/891\n",
      "--\n",
      "Training Step: 55  | total loss: \u001b[1m\u001b[32m0.62636\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 055 | loss: 0.62636 - acc: 0.6611 -- iter: 891/891\n",
      "--\n",
      "Training Step: 56  | total loss: \u001b[1m\u001b[32m0.63486\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 056 | loss: 0.63486 - acc: 0.6530 -- iter: 891/891\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 57  | total loss: \u001b[1m\u001b[32m0.63256\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 057 | loss: 0.63256 - acc: 0.6521 -- iter: 891/891\n",
      "--\n",
      "Training Step: 58  | total loss: \u001b[1m\u001b[32m0.64104\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 058 | loss: 0.64104 - acc: 0.6445 -- iter: 891/891\n",
      "--\n",
      "Training Step: 59  | total loss: \u001b[1m\u001b[32m0.63796\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 059 | loss: 0.63796 - acc: 0.6453 -- iter: 891/891\n",
      "--\n",
      "Training Step: 60  | total loss: \u001b[1m\u001b[32m0.64369\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 060 | loss: 0.64369 - acc: 0.6391 -- iter: 891/891\n",
      "--\n",
      "Training Step: 61  | total loss: \u001b[1m\u001b[32m0.64067\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 061 | loss: 0.64067 - acc: 0.6411 -- iter: 891/891\n",
      "--\n",
      "Training Step: 62  | total loss: \u001b[1m\u001b[32m0.63810\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 062 | loss: 0.63810 - acc: 0.6435 -- iter: 891/891\n",
      "--\n",
      "Training Step: 63  | total loss: \u001b[1m\u001b[32m0.63564\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 063 | loss: 0.63564 - acc: 0.6462 -- iter: 891/891\n",
      "--\n",
      "Training Step: 64  | total loss: \u001b[1m\u001b[32m0.63300\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 064 | loss: 0.63300 - acc: 0.6489 -- iter: 891/891\n",
      "--\n",
      "Training Step: 65  | total loss: \u001b[1m\u001b[32m0.62999\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 065 | loss: 0.62999 - acc: 0.6512 -- iter: 891/891\n",
      "--\n",
      "Training Step: 66  | total loss: \u001b[1m\u001b[32m0.62658\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 066 | loss: 0.62658 - acc: 0.6538 -- iter: 891/891\n",
      "--\n",
      "Training Step: 67  | total loss: \u001b[1m\u001b[32m0.62289\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 067 | loss: 0.62289 - acc: 0.6568 -- iter: 891/891\n",
      "--\n",
      "Training Step: 68  | total loss: \u001b[1m\u001b[32m0.61919\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 068 | loss: 0.61919 - acc: 0.6605 -- iter: 891/891\n",
      "--\n",
      "Training Step: 69  | total loss: \u001b[1m\u001b[32m0.61574\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 069 | loss: 0.61574 - acc: 0.6649 -- iter: 891/891\n",
      "--\n",
      "Training Step: 70  | total loss: \u001b[1m\u001b[32m0.61276\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 070 | loss: 0.61276 - acc: 0.6696 -- iter: 891/891\n",
      "--\n",
      "Training Step: 71  | total loss: \u001b[1m\u001b[32m0.61033\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 071 | loss: 0.61033 - acc: 0.6744 -- iter: 891/891\n",
      "--\n",
      "Training Step: 72  | total loss: \u001b[1m\u001b[32m0.60824\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 072 | loss: 0.60824 - acc: 0.6782 -- iter: 891/891\n",
      "--\n",
      "Training Step: 73  | total loss: \u001b[1m\u001b[32m0.60592\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 073 | loss: 0.60592 - acc: 0.6818 -- iter: 891/891\n",
      "--\n",
      "Training Step: 74  | total loss: \u001b[1m\u001b[32m0.63199\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 074 | loss: 0.63199 - acc: 0.6703 -- iter: 891/891\n",
      "--\n",
      "Training Step: 75  | total loss: \u001b[1m\u001b[32m0.62587\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 075 | loss: 0.62587 - acc: 0.6732 -- iter: 891/891\n",
      "--\n",
      "Training Step: 76  | total loss: \u001b[1m\u001b[32m0.63729\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 076 | loss: 0.63729 - acc: 0.6614 -- iter: 891/891\n",
      "--\n",
      "Training Step: 77  | total loss: \u001b[1m\u001b[32m0.63321\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 077 | loss: 0.63321 - acc: 0.6620 -- iter: 891/891\n",
      "--\n",
      "Training Step: 78  | total loss: \u001b[1m\u001b[32m0.63756\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 078 | loss: 0.63756 - acc: 0.6563 -- iter: 891/891\n",
      "--\n",
      "Training Step: 79  | total loss: \u001b[1m\u001b[32m0.63706\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 079 | loss: 0.63706 - acc: 0.6523 -- iter: 891/891\n",
      "--\n",
      "Training Step: 80  | total loss: \u001b[1m\u001b[32m0.63770\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 080 | loss: 0.63770 - acc: 0.6486 -- iter: 891/891\n",
      "--\n",
      "Training Step: 81  | total loss: \u001b[1m\u001b[32m0.63883\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 081 | loss: 0.63883 - acc: 0.6453 -- iter: 891/891\n",
      "--\n",
      "Training Step: 82  | total loss: \u001b[1m\u001b[32m0.64002\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 082 | loss: 0.64002 - acc: 0.6424 -- iter: 891/891\n",
      "--\n",
      "Training Step: 83  | total loss: \u001b[1m\u001b[32m0.64102\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 083 | loss: 0.64102 - acc: 0.6398 -- iter: 891/891\n",
      "--\n",
      "Training Step: 84  | total loss: \u001b[1m\u001b[32m0.64164\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 084 | loss: 0.64164 - acc: 0.6374 -- iter: 891/891\n",
      "--\n",
      "Training Step: 85  | total loss: \u001b[1m\u001b[32m0.64174\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 085 | loss: 0.64174 - acc: 0.6353 -- iter: 891/891\n",
      "--\n",
      "Training Step: 86  | total loss: \u001b[1m\u001b[32m0.64485\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 086 | loss: 0.64485 - acc: 0.6337 -- iter: 891/891\n",
      "--\n",
      "Training Step: 87  | total loss: \u001b[1m\u001b[32m0.64340\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 087 | loss: 0.64340 - acc: 0.6329 -- iter: 891/891\n",
      "--\n",
      "Training Step: 88  | total loss: \u001b[1m\u001b[32m0.64131\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 088 | loss: 0.64131 - acc: 0.6327 -- iter: 891/891\n",
      "--\n",
      "Training Step: 89  | total loss: \u001b[1m\u001b[32m0.63847\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 089 | loss: 0.63847 - acc: 0.6336 -- iter: 891/891\n",
      "--\n",
      "Training Step: 90  | total loss: \u001b[1m\u001b[32m0.64430\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 090 | loss: 0.64430 - acc: 0.6302 -- iter: 891/891\n",
      "--\n",
      "Training Step: 91  | total loss: \u001b[1m\u001b[32m0.63950\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 091 | loss: 0.63950 - acc: 0.6338 -- iter: 891/891\n",
      "--\n",
      "Training Step: 92  | total loss: \u001b[1m\u001b[32m0.64468\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 092 | loss: 0.64468 - acc: 0.6309 -- iter: 891/891\n",
      "--\n",
      "Training Step: 93  | total loss: \u001b[1m\u001b[32m0.63878\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 093 | loss: 0.63878 - acc: 0.6360 -- iter: 891/891\n",
      "--\n",
      "Training Step: 94  | total loss: \u001b[1m\u001b[32m0.63302\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 094 | loss: 0.63302 - acc: 0.6413 -- iter: 891/891\n",
      "--\n",
      "Training Step: 95  | total loss: \u001b[1m\u001b[32m0.62734\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 095 | loss: 0.62734 - acc: 0.6472 -- iter: 891/891\n",
      "--\n",
      "Training Step: 96  | total loss: \u001b[1m\u001b[32m0.62177\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 096 | loss: 0.62177 - acc: 0.6531 -- iter: 891/891\n",
      "--\n",
      "Training Step: 97  | total loss: \u001b[1m\u001b[32m0.61640\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 097 | loss: 0.61640 - acc: 0.6594 -- iter: 891/891\n",
      "--\n",
      "Training Step: 98  | total loss: \u001b[1m\u001b[32m0.63387\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 098 | loss: 0.63387 - acc: 0.6499 -- iter: 891/891\n",
      "--\n",
      "Training Step: 99  | total loss: \u001b[1m\u001b[32m0.62693\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 099 | loss: 0.62693 - acc: 0.6571 -- iter: 891/891\n",
      "--\n",
      "Training Step: 100  | total loss: \u001b[1m\u001b[32m0.62052\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 100 | loss: 0.62052 - acc: 0.6630 -- iter: 891/891\n",
      "--\n",
      "Training Step: 101  | total loss: \u001b[1m\u001b[32m0.61460\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 101 | loss: 0.61460 - acc: 0.6680 -- iter: 891/891\n",
      "--\n",
      "Training Step: 102  | total loss: \u001b[1m\u001b[32m0.62592\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 102 | loss: 0.62592 - acc: 0.6623 -- iter: 891/891\n",
      "--\n",
      "Training Step: 103  | total loss: \u001b[1m\u001b[32m0.61952\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 103 | loss: 0.61952 - acc: 0.6673 -- iter: 891/891\n",
      "--\n",
      "Training Step: 104  | total loss: \u001b[1m\u001b[32m0.61401\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 104 | loss: 0.61401 - acc: 0.6718 -- iter: 891/891\n",
      "--\n",
      "Training Step: 105  | total loss: \u001b[1m\u001b[32m0.60917\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 105 | loss: 0.60917 - acc: 0.6755 -- iter: 891/891\n",
      "--\n",
      "Training Step: 106  | total loss: \u001b[1m\u001b[32m0.61991\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 106 | loss: 0.61991 - acc: 0.6661 -- iter: 891/891\n",
      "--\n",
      "Training Step: 107  | total loss: \u001b[1m\u001b[32m0.61487\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 107 | loss: 0.61487 - acc: 0.6704 -- iter: 891/891\n",
      "--\n",
      "Training Step: 108  | total loss: \u001b[1m\u001b[32m0.62306\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 108 | loss: 0.62306 - acc: 0.6621 -- iter: 891/891\n",
      "--\n",
      "Training Step: 109  | total loss: \u001b[1m\u001b[32m0.61862\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 109 | loss: 0.61862 - acc: 0.6678 -- iter: 891/891\n",
      "--\n",
      "Training Step: 110  | total loss: \u001b[1m\u001b[32m0.61482\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 110 | loss: 0.61482 - acc: 0.6725 -- iter: 891/891\n",
      "--\n",
      "Training Step: 111  | total loss: \u001b[1m\u001b[32m0.61098\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 111 | loss: 0.61098 - acc: 0.6774 -- iter: 891/891\n",
      "--\n",
      "Training Step: 112  | total loss: \u001b[1m\u001b[32m0.61988\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 112 | loss: 0.61988 - acc: 0.6676 -- iter: 891/891\n",
      "--\n",
      "Training Step: 113  | total loss: \u001b[1m\u001b[32m0.61421\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 113 | loss: 0.61421 - acc: 0.6745 -- iter: 891/891\n",
      "--\n",
      "Training Step: 114  | total loss: \u001b[1m\u001b[32m0.60828\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 114 | loss: 0.60828 - acc: 0.6817 -- iter: 891/891\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 115  | total loss: \u001b[1m\u001b[32m0.60185\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 115 | loss: 0.60185 - acc: 0.6898 -- iter: 891/891\n",
      "--\n",
      "Training Step: 116  | total loss: \u001b[1m\u001b[32m0.61581\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 116 | loss: 0.61581 - acc: 0.6780 -- iter: 891/891\n",
      "--\n",
      "Training Step: 117  | total loss: \u001b[1m\u001b[32m0.60740\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 117 | loss: 0.60740 - acc: 0.6866 -- iter: 891/891\n",
      "--\n",
      "Training Step: 118  | total loss: \u001b[1m\u001b[32m0.62158\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 118 | loss: 0.62158 - acc: 0.6751 -- iter: 891/891\n",
      "--\n",
      "Training Step: 119  | total loss: \u001b[1m\u001b[32m0.61254\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 119 | loss: 0.61254 - acc: 0.6839 -- iter: 891/891\n",
      "--\n",
      "Training Step: 120  | total loss: \u001b[1m\u001b[32m0.60453\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 120 | loss: 0.60453 - acc: 0.6925 -- iter: 891/891\n",
      "--\n",
      "Training Step: 121  | total loss: \u001b[1m\u001b[32m0.59724\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 121 | loss: 0.59724 - acc: 0.6999 -- iter: 891/891\n",
      "--\n",
      "Training Step: 122  | total loss: \u001b[1m\u001b[32m0.59025\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 122 | loss: 0.59025 - acc: 0.7066 -- iter: 891/891\n",
      "--\n",
      "Training Step: 123  | total loss: \u001b[1m\u001b[32m0.58329\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 123 | loss: 0.58329 - acc: 0.7134 -- iter: 891/891\n",
      "--\n",
      "Training Step: 124  | total loss: \u001b[1m\u001b[32m0.57636\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 124 | loss: 0.57636 - acc: 0.7203 -- iter: 891/891\n",
      "--\n",
      "Training Step: 125  | total loss: \u001b[1m\u001b[32m0.56955\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 125 | loss: 0.56955 - acc: 0.7266 -- iter: 891/891\n",
      "--\n",
      "Training Step: 126  | total loss: \u001b[1m\u001b[32m0.56316\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 126 | loss: 0.56316 - acc: 0.7322 -- iter: 891/891\n",
      "--\n",
      "Training Step: 127  | total loss: \u001b[1m\u001b[32m0.55722\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 127 | loss: 0.55722 - acc: 0.7368 -- iter: 891/891\n",
      "--\n",
      "Training Step: 128  | total loss: \u001b[1m\u001b[32m0.59548\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 128 | loss: 0.59548 - acc: 0.7185 -- iter: 891/891\n",
      "--\n",
      "Training Step: 129  | total loss: \u001b[1m\u001b[32m0.58535\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 129 | loss: 0.58535 - acc: 0.7255 -- iter: 891/891\n",
      "--\n",
      "Training Step: 130  | total loss: \u001b[1m\u001b[32m0.61304\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 130 | loss: 0.61304 - acc: 0.7073 -- iter: 891/891\n",
      "--\n",
      "Training Step: 131  | total loss: \u001b[1m\u001b[32m0.60525\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 131 | loss: 0.60525 - acc: 0.7143 -- iter: 891/891\n",
      "--\n",
      "Training Step: 132  | total loss: \u001b[1m\u001b[32m0.62109\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 132 | loss: 0.62109 - acc: 0.6962 -- iter: 891/891\n",
      "--\n",
      "Training Step: 133  | total loss: \u001b[1m\u001b[32m0.61704\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 133 | loss: 0.61704 - acc: 0.6979 -- iter: 891/891\n",
      "--\n",
      "Training Step: 134  | total loss: \u001b[1m\u001b[32m0.61250\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 134 | loss: 0.61250 - acc: 0.7032 -- iter: 891/891\n",
      "--\n",
      "Training Step: 135  | total loss: \u001b[1m\u001b[32m0.60639\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 135 | loss: 0.60639 - acc: 0.7115 -- iter: 891/891\n",
      "--\n",
      "Training Step: 136  | total loss: \u001b[1m\u001b[32m0.59925\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 136 | loss: 0.59925 - acc: 0.7211 -- iter: 891/891\n",
      "--\n",
      "Training Step: 137  | total loss: \u001b[1m\u001b[32m0.59184\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 137 | loss: 0.59184 - acc: 0.7280 -- iter: 891/891\n",
      "--\n",
      "Training Step: 138  | total loss: \u001b[1m\u001b[32m0.58475\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 138 | loss: 0.58475 - acc: 0.7328 -- iter: 891/891\n",
      "--\n",
      "Training Step: 139  | total loss: \u001b[1m\u001b[32m0.57862\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 139 | loss: 0.57862 - acc: 0.7356 -- iter: 891/891\n",
      "--\n",
      "Training Step: 140  | total loss: \u001b[1m\u001b[32m0.57366\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 140 | loss: 0.57366 - acc: 0.7379 -- iter: 891/891\n",
      "--\n",
      "Training Step: 141  | total loss: \u001b[1m\u001b[32m0.56966\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 141 | loss: 0.56966 - acc: 0.7398 -- iter: 891/891\n",
      "--\n",
      "Training Step: 142  | total loss: \u001b[1m\u001b[32m0.56638\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 142 | loss: 0.56638 - acc: 0.7420 -- iter: 891/891\n",
      "--\n",
      "Training Step: 143  | total loss: \u001b[1m\u001b[32m0.56349\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 143 | loss: 0.56349 - acc: 0.7440 -- iter: 891/891\n",
      "--\n",
      "Training Step: 144  | total loss: \u001b[1m\u001b[32m0.56031\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 144 | loss: 0.56031 - acc: 0.7458 -- iter: 891/891\n",
      "--\n",
      "Training Step: 145  | total loss: \u001b[1m\u001b[32m0.55646\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 145 | loss: 0.55646 - acc: 0.7488 -- iter: 891/891\n",
      "--\n",
      "Training Step: 146  | total loss: \u001b[1m\u001b[32m0.55204\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 146 | loss: 0.55204 - acc: 0.7521 -- iter: 891/891\n",
      "--\n",
      "Training Step: 147  | total loss: \u001b[1m\u001b[32m0.54720\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 147 | loss: 0.54720 - acc: 0.7549 -- iter: 891/891\n",
      "--\n",
      "Training Step: 148  | total loss: \u001b[1m\u001b[32m0.57623\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 148 | loss: 0.57623 - acc: 0.7350 -- iter: 891/891\n",
      "--\n",
      "Training Step: 149  | total loss: \u001b[1m\u001b[32m0.56861\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 149 | loss: 0.56861 - acc: 0.7420 -- iter: 891/891\n",
      "--\n",
      "Training Step: 150  | total loss: \u001b[1m\u001b[32m0.56316\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 150 | loss: 0.56316 - acc: 0.7487 -- iter: 891/891\n",
      "--\n",
      "Training Step: 151  | total loss: \u001b[1m\u001b[32m0.55957\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 151 | loss: 0.55957 - acc: 0.7541 -- iter: 891/891\n",
      "--\n",
      "Training Step: 152  | total loss: \u001b[1m\u001b[32m0.55651\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 152 | loss: 0.55651 - acc: 0.7591 -- iter: 891/891\n",
      "--\n",
      "Training Step: 153  | total loss: \u001b[1m\u001b[32m0.55239\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 153 | loss: 0.55239 - acc: 0.7635 -- iter: 891/891\n",
      "--\n",
      "Training Step: 154  | total loss: \u001b[1m\u001b[32m0.57514\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 154 | loss: 0.57514 - acc: 0.7430 -- iter: 891/891\n",
      "--\n",
      "Training Step: 155  | total loss: \u001b[1m\u001b[32m0.56668\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 155 | loss: 0.56668 - acc: 0.7493 -- iter: 891/891\n",
      "--\n",
      "Training Step: 156  | total loss: \u001b[1m\u001b[32m0.55838\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 156 | loss: 0.55838 - acc: 0.7551 -- iter: 891/891\n",
      "--\n",
      "Training Step: 157  | total loss: \u001b[1m\u001b[32m0.55018\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 157 | loss: 0.55018 - acc: 0.7600 -- iter: 891/891\n",
      "--\n",
      "Training Step: 158  | total loss: \u001b[1m\u001b[32m0.54251\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 158 | loss: 0.54251 - acc: 0.7647 -- iter: 891/891\n",
      "--\n",
      "Training Step: 159  | total loss: \u001b[1m\u001b[32m0.53560\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 159 | loss: 0.53560 - acc: 0.7691 -- iter: 891/891\n",
      "--\n",
      "Training Step: 160  | total loss: \u001b[1m\u001b[32m0.52952\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 160 | loss: 0.52952 - acc: 0.7730 -- iter: 891/891\n",
      "--\n",
      "Training Step: 161  | total loss: \u001b[1m\u001b[32m0.52405\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 161 | loss: 0.52405 - acc: 0.7765 -- iter: 891/891\n",
      "--\n",
      "Training Step: 162  | total loss: \u001b[1m\u001b[32m0.57468\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 162 | loss: 0.57468 - acc: 0.7531 -- iter: 891/891\n",
      "--\n",
      "Training Step: 163  | total loss: \u001b[1m\u001b[32m0.56358\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 163 | loss: 0.56358 - acc: 0.7587 -- iter: 891/891\n",
      "--\n",
      "Training Step: 164  | total loss: \u001b[1m\u001b[32m0.55354\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 164 | loss: 0.55354 - acc: 0.7646 -- iter: 891/891\n",
      "--\n",
      "Training Step: 165  | total loss: \u001b[1m\u001b[32m0.54504\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 165 | loss: 0.54504 - acc: 0.7695 -- iter: 891/891\n",
      "--\n",
      "Training Step: 166  | total loss: \u001b[1m\u001b[32m0.57454\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 166 | loss: 0.57454 - acc: 0.7464 -- iter: 891/891\n",
      "--\n",
      "Training Step: 167  | total loss: \u001b[1m\u001b[32m0.56604\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 167 | loss: 0.56604 - acc: 0.7525 -- iter: 891/891\n",
      "--\n",
      "Training Step: 168  | total loss: \u001b[1m\u001b[32m0.55877\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 168 | loss: 0.55877 - acc: 0.7581 -- iter: 891/891\n",
      "--\n",
      "Training Step: 169  | total loss: \u001b[1m\u001b[32m0.55121\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 169 | loss: 0.55121 - acc: 0.7630 -- iter: 891/891\n",
      "--\n",
      "Training Step: 170  | total loss: \u001b[1m\u001b[32m0.54310\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 170 | loss: 0.54310 - acc: 0.7676 -- iter: 891/891\n",
      "--\n",
      "Training Step: 171  | total loss: \u001b[1m\u001b[32m0.53518\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 171 | loss: 0.53518 - acc: 0.7722 -- iter: 891/891\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 172  | total loss: \u001b[1m\u001b[32m0.57376\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 172 | loss: 0.57376 - acc: 0.7498 -- iter: 891/891\n",
      "--\n",
      "Training Step: 173  | total loss: \u001b[1m\u001b[32m0.56340\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 173 | loss: 0.56340 - acc: 0.7554 -- iter: 891/891\n",
      "--\n",
      "Training Step: 174  | total loss: \u001b[1m\u001b[32m0.55393\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 174 | loss: 0.55393 - acc: 0.7606 -- iter: 891/891\n",
      "--\n",
      "Training Step: 175  | total loss: \u001b[1m\u001b[32m0.54549\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 175 | loss: 0.54549 - acc: 0.7650 -- iter: 891/891\n",
      "--\n",
      "Training Step: 176  | total loss: \u001b[1m\u001b[32m0.53763\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 176 | loss: 0.53763 - acc: 0.7687 -- iter: 891/891\n",
      "--\n",
      "Training Step: 177  | total loss: \u001b[1m\u001b[32m0.53018\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 177 | loss: 0.53018 - acc: 0.7731 -- iter: 891/891\n",
      "--\n",
      "Training Step: 178  | total loss: \u001b[1m\u001b[32m0.52333\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 178 | loss: 0.52333 - acc: 0.7772 -- iter: 891/891\n",
      "--\n",
      "Training Step: 179  | total loss: \u001b[1m\u001b[32m0.51673\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 179 | loss: 0.51673 - acc: 0.7803 -- iter: 891/891\n",
      "--\n",
      "Training Step: 180  | total loss: \u001b[1m\u001b[32m0.51063\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 180 | loss: 0.51063 - acc: 0.7820 -- iter: 891/891\n",
      "--\n",
      "Training Step: 181  | total loss: \u001b[1m\u001b[32m0.50494\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 181 | loss: 0.50494 - acc: 0.7838 -- iter: 891/891\n",
      "--\n",
      "Training Step: 182  | total loss: \u001b[1m\u001b[32m0.49968\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 182 | loss: 0.49968 - acc: 0.7860 -- iter: 891/891\n",
      "--\n",
      "Training Step: 183  | total loss: \u001b[1m\u001b[32m0.49487\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 183 | loss: 0.49487 - acc: 0.7879 -- iter: 891/891\n",
      "--\n",
      "Training Step: 184  | total loss: \u001b[1m\u001b[32m0.49057\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 184 | loss: 0.49057 - acc: 0.7894 -- iter: 891/891\n",
      "--\n",
      "Training Step: 185  | total loss: \u001b[1m\u001b[32m0.48677\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 185 | loss: 0.48677 - acc: 0.7907 -- iter: 891/891\n",
      "--\n",
      "Training Step: 186  | total loss: \u001b[1m\u001b[32m0.48350\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 186 | loss: 0.48350 - acc: 0.7921 -- iter: 891/891\n",
      "--\n",
      "Training Step: 187  | total loss: \u001b[1m\u001b[32m0.48048\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 187 | loss: 0.48048 - acc: 0.7933 -- iter: 891/891\n",
      "--\n",
      "Training Step: 188  | total loss: \u001b[1m\u001b[32m0.47772\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 188 | loss: 0.47772 - acc: 0.7941 -- iter: 891/891\n",
      "--\n",
      "Training Step: 189  | total loss: \u001b[1m\u001b[32m0.47501\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 189 | loss: 0.47501 - acc: 0.7951 -- iter: 891/891\n",
      "--\n",
      "Training Step: 190  | total loss: \u001b[1m\u001b[32m0.47252\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 190 | loss: 0.47252 - acc: 0.7959 -- iter: 891/891\n",
      "--\n",
      "Training Step: 191  | total loss: \u001b[1m\u001b[32m0.47013\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 191 | loss: 0.47013 - acc: 0.7965 -- iter: 891/891\n",
      "--\n",
      "Training Step: 192  | total loss: \u001b[1m\u001b[32m0.46797\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 192 | loss: 0.46797 - acc: 0.7969 -- iter: 891/891\n",
      "--\n",
      "Training Step: 193  | total loss: \u001b[1m\u001b[32m0.46594\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 193 | loss: 0.46594 - acc: 0.7976 -- iter: 891/891\n",
      "--\n",
      "Training Step: 194  | total loss: \u001b[1m\u001b[32m0.46413\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 194 | loss: 0.46413 - acc: 0.7986 -- iter: 891/891\n",
      "--\n",
      "Training Step: 195  | total loss: \u001b[1m\u001b[32m0.46249\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 195 | loss: 0.46249 - acc: 0.7992 -- iter: 891/891\n",
      "--\n",
      "Training Step: 196  | total loss: \u001b[1m\u001b[32m0.50637\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 196 | loss: 0.50637 - acc: 0.7744 -- iter: 891/891\n",
      "--\n",
      "Training Step: 197  | total loss: \u001b[1m\u001b[32m0.50077\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 197 | loss: 0.50077 - acc: 0.7774 -- iter: 891/891\n",
      "--\n",
      "Training Step: 198  | total loss: \u001b[1m\u001b[32m0.49620\u001b[0m\u001b[0m | time: 0.011s\n",
      "| Adam | epoch: 198 | loss: 0.49620 - acc: 0.7803 -- iter: 891/891\n",
      "--\n",
      "Training Step: 199  | total loss: \u001b[1m\u001b[32m0.49224\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 199 | loss: 0.49224 - acc: 0.7827 -- iter: 891/891\n",
      "--\n",
      "Training Step: 200  | total loss: \u001b[1m\u001b[32m0.53003\u001b[0m\u001b[0m | time: 0.011s\n",
      "| Adam | epoch: 200 | loss: 0.53003 - acc: 0.7575 -- iter: 891/891\n",
      "--\n",
      "Training Step: 201  | total loss: \u001b[1m\u001b[32m0.52336\u001b[0m\u001b[0m | time: 0.011s\n",
      "| Adam | epoch: 201 | loss: 0.52336 - acc: 0.7622 -- iter: 891/891\n",
      "--\n",
      "Training Step: 202  | total loss: \u001b[1m\u001b[32m0.51769\u001b[0m\u001b[0m | time: 0.011s\n",
      "| Adam | epoch: 202 | loss: 0.51769 - acc: 0.7665 -- iter: 891/891\n",
      "--\n",
      "Training Step: 203  | total loss: \u001b[1m\u001b[32m0.51234\u001b[0m\u001b[0m | time: 0.011s\n",
      "| Adam | epoch: 203 | loss: 0.51234 - acc: 0.7701 -- iter: 891/891\n",
      "--\n",
      "Training Step: 204  | total loss: \u001b[1m\u001b[32m0.50707\u001b[0m\u001b[0m | time: 0.011s\n",
      "| Adam | epoch: 204 | loss: 0.50707 - acc: 0.7737 -- iter: 891/891\n",
      "--\n",
      "Training Step: 205  | total loss: \u001b[1m\u001b[32m0.50199\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 205 | loss: 0.50199 - acc: 0.7769 -- iter: 891/891\n",
      "--\n",
      "Training Step: 206  | total loss: \u001b[1m\u001b[32m0.49729\u001b[0m\u001b[0m | time: 0.011s\n",
      "| Adam | epoch: 206 | loss: 0.49729 - acc: 0.7793 -- iter: 891/891\n",
      "--\n",
      "Training Step: 207  | total loss: \u001b[1m\u001b[32m0.49305\u001b[0m\u001b[0m | time: 0.011s\n",
      "| Adam | epoch: 207 | loss: 0.49305 - acc: 0.7817 -- iter: 891/891\n",
      "--\n",
      "Training Step: 208  | total loss: \u001b[1m\u001b[32m0.48915\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 208 | loss: 0.48915 - acc: 0.7838 -- iter: 891/891\n",
      "--\n",
      "Training Step: 209  | total loss: \u001b[1m\u001b[32m0.48537\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 209 | loss: 0.48537 - acc: 0.7855 -- iter: 891/891\n",
      "--\n",
      "Training Step: 210  | total loss: \u001b[1m\u001b[32m0.53264\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 210 | loss: 0.53264 - acc: 0.7625 -- iter: 891/891\n",
      "--\n",
      "Training Step: 211  | total loss: \u001b[1m\u001b[32m0.52422\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 211 | loss: 0.52422 - acc: 0.7670 -- iter: 891/891\n",
      "--\n",
      "Training Step: 212  | total loss: \u001b[1m\u001b[32m0.51729\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 212 | loss: 0.51729 - acc: 0.7710 -- iter: 891/891\n",
      "--\n",
      "Training Step: 213  | total loss: \u001b[1m\u001b[32m0.51163\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 213 | loss: 0.51163 - acc: 0.7739 -- iter: 891/891\n",
      "--\n",
      "Training Step: 214  | total loss: \u001b[1m\u001b[32m0.50562\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 214 | loss: 0.50562 - acc: 0.7769 -- iter: 891/891\n",
      "--\n",
      "Training Step: 215  | total loss: \u001b[1m\u001b[32m0.49974\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 215 | loss: 0.49974 - acc: 0.7799 -- iter: 891/891\n",
      "--\n",
      "Training Step: 216  | total loss: \u001b[1m\u001b[32m0.49442\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 216 | loss: 0.49442 - acc: 0.7827 -- iter: 891/891\n",
      "--\n",
      "Training Step: 217  | total loss: \u001b[1m\u001b[32m0.49006\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 217 | loss: 0.49006 - acc: 0.7844 -- iter: 891/891\n",
      "--\n",
      "Training Step: 218  | total loss: \u001b[1m\u001b[32m0.48635\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 218 | loss: 0.48635 - acc: 0.7857 -- iter: 891/891\n",
      "--\n",
      "Training Step: 219  | total loss: \u001b[1m\u001b[32m0.48284\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 219 | loss: 0.48284 - acc: 0.7876 -- iter: 891/891\n",
      "--\n",
      "Training Step: 220  | total loss: \u001b[1m\u001b[32m0.53011\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 220 | loss: 0.53011 - acc: 0.7636 -- iter: 891/891\n",
      "--\n",
      "Training Step: 221  | total loss: \u001b[1m\u001b[32m0.52185\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 221 | loss: 0.52185 - acc: 0.7677 -- iter: 891/891\n",
      "--\n",
      "Training Step: 222  | total loss: \u001b[1m\u001b[32m0.55731\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 222 | loss: 0.55731 - acc: 0.7428 -- iter: 891/891\n",
      "--\n",
      "Training Step: 223  | total loss: \u001b[1m\u001b[32m0.55027\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 223 | loss: 0.55027 - acc: 0.7486 -- iter: 891/891\n",
      "--\n",
      "Training Step: 224  | total loss: \u001b[1m\u001b[32m0.54590\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 224 | loss: 0.54590 - acc: 0.7533 -- iter: 891/891\n",
      "--\n",
      "Training Step: 225  | total loss: \u001b[1m\u001b[32m0.54187\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 225 | loss: 0.54187 - acc: 0.7577 -- iter: 891/891\n",
      "--\n",
      "Training Step: 226  | total loss: \u001b[1m\u001b[32m0.56504\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 226 | loss: 0.56504 - acc: 0.7361 -- iter: 891/891\n",
      "--\n",
      "Training Step: 227  | total loss: \u001b[1m\u001b[32m0.55779\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 227 | loss: 0.55779 - acc: 0.7430 -- iter: 891/891\n",
      "--\n",
      "Training Step: 228  | total loss: \u001b[1m\u001b[32m0.57511\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 228 | loss: 0.57511 - acc: 0.7254 -- iter: 891/891\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 229  | total loss: \u001b[1m\u001b[32m0.56705\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 229 | loss: 0.56705 - acc: 0.7335 -- iter: 891/891\n",
      "--\n",
      "Training Step: 230  | total loss: \u001b[1m\u001b[32m0.58677\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 230 | loss: 0.58677 - acc: 0.7133 -- iter: 891/891\n",
      "--\n",
      "Training Step: 231  | total loss: \u001b[1m\u001b[32m0.57870\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 231 | loss: 0.57870 - acc: 0.7217 -- iter: 891/891\n",
      "--\n",
      "Training Step: 232  | total loss: \u001b[1m\u001b[32m0.57188\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 232 | loss: 0.57188 - acc: 0.7287 -- iter: 891/891\n",
      "--\n",
      "Training Step: 233  | total loss: \u001b[1m\u001b[32m0.56558\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 233 | loss: 0.56558 - acc: 0.7350 -- iter: 891/891\n",
      "--\n",
      "Training Step: 234  | total loss: \u001b[1m\u001b[32m0.58205\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 234 | loss: 0.58205 - acc: 0.7166 -- iter: 891/891\n",
      "--\n",
      "Training Step: 235  | total loss: \u001b[1m\u001b[32m0.57433\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 235 | loss: 0.57433 - acc: 0.7248 -- iter: 891/891\n",
      "--\n",
      "Training Step: 236  | total loss: \u001b[1m\u001b[32m0.56710\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 236 | loss: 0.56710 - acc: 0.7323 -- iter: 891/891\n",
      "--\n",
      "Training Step: 237  | total loss: \u001b[1m\u001b[32m0.55997\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 237 | loss: 0.55997 - acc: 0.7374 -- iter: 891/891\n",
      "--\n",
      "Training Step: 238  | total loss: \u001b[1m\u001b[32m0.55276\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 238 | loss: 0.55276 - acc: 0.7422 -- iter: 891/891\n",
      "--\n",
      "Training Step: 239  | total loss: \u001b[1m\u001b[32m0.54547\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 239 | loss: 0.54547 - acc: 0.7468 -- iter: 891/891\n",
      "--\n",
      "Training Step: 240  | total loss: \u001b[1m\u001b[32m0.53819\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 240 | loss: 0.53819 - acc: 0.7525 -- iter: 891/891\n",
      "--\n",
      "Training Step: 241  | total loss: \u001b[1m\u001b[32m0.53111\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 241 | loss: 0.53111 - acc: 0.7579 -- iter: 891/891\n",
      "--\n",
      "Training Step: 242  | total loss: \u001b[1m\u001b[32m0.57332\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 242 | loss: 0.57332 - acc: 0.7351 -- iter: 891/891\n",
      "--\n",
      "Training Step: 243  | total loss: \u001b[1m\u001b[32m0.56184\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 243 | loss: 0.56184 - acc: 0.7424 -- iter: 891/891\n",
      "--\n",
      "Training Step: 244  | total loss: \u001b[1m\u001b[32m0.59945\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 244 | loss: 0.59945 - acc: 0.7210 -- iter: 891/891\n",
      "--\n",
      "Training Step: 245  | total loss: \u001b[1m\u001b[32m0.58498\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 245 | loss: 0.58498 - acc: 0.7299 -- iter: 891/891\n",
      "--\n",
      "Training Step: 246  | total loss: \u001b[1m\u001b[32m0.61124\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 246 | loss: 0.61124 - acc: 0.7093 -- iter: 891/891\n",
      "--\n",
      "Training Step: 247  | total loss: \u001b[1m\u001b[32m0.59992\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 247 | loss: 0.59992 - acc: 0.7187 -- iter: 891/891\n",
      "--\n",
      "Training Step: 248  | total loss: \u001b[1m\u001b[32m0.59179\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 248 | loss: 0.59179 - acc: 0.7265 -- iter: 891/891\n",
      "--\n",
      "Training Step: 249  | total loss: \u001b[1m\u001b[32m0.58478\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 249 | loss: 0.58478 - acc: 0.7336 -- iter: 891/891\n",
      "--\n",
      "Training Step: 250  | total loss: \u001b[1m\u001b[32m0.57693\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 250 | loss: 0.57693 - acc: 0.7404 -- iter: 891/891\n",
      "--\n",
      "Training Step: 251  | total loss: \u001b[1m\u001b[32m0.56763\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 251 | loss: 0.56763 - acc: 0.7476 -- iter: 891/891\n",
      "--\n",
      "Training Step: 252  | total loss: \u001b[1m\u001b[32m0.59079\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 252 | loss: 0.59079 - acc: 0.7271 -- iter: 891/891\n",
      "--\n",
      "Training Step: 253  | total loss: \u001b[1m\u001b[32m0.57891\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 253 | loss: 0.57891 - acc: 0.7352 -- iter: 891/891\n",
      "--\n",
      "Training Step: 254  | total loss: \u001b[1m\u001b[32m0.60703\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 254 | loss: 0.60703 - acc: 0.7141 -- iter: 891/891\n",
      "--\n",
      "Training Step: 255  | total loss: \u001b[1m\u001b[32m0.59410\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 255 | loss: 0.59410 - acc: 0.7230 -- iter: 891/891\n",
      "--\n",
      "Training Step: 256  | total loss: \u001b[1m\u001b[32m0.61346\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 256 | loss: 0.61346 - acc: 0.7048 -- iter: 891/891\n",
      "--\n",
      "Training Step: 257  | total loss: \u001b[1m\u001b[32m0.60125\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 257 | loss: 0.60125 - acc: 0.7151 -- iter: 891/891\n",
      "--\n",
      "Training Step: 258  | total loss: \u001b[1m\u001b[32m0.61479\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 258 | loss: 0.61479 - acc: 0.6996 -- iter: 891/891\n",
      "--\n",
      "Training Step: 259  | total loss: \u001b[1m\u001b[32m0.60462\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 259 | loss: 0.60462 - acc: 0.7108 -- iter: 891/891\n",
      "--\n",
      "Training Step: 260  | total loss: \u001b[1m\u001b[32m0.61452\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 260 | loss: 0.61452 - acc: 0.6962 -- iter: 891/891\n",
      "--\n",
      "Training Step: 261  | total loss: \u001b[1m\u001b[32m0.60678\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 261 | loss: 0.60678 - acc: 0.7067 -- iter: 891/891\n",
      "--\n",
      "Training Step: 262  | total loss: \u001b[1m\u001b[32m0.61532\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 262 | loss: 0.61532 - acc: 0.6920 -- iter: 891/891\n",
      "--\n",
      "Training Step: 263  | total loss: \u001b[1m\u001b[32m0.60919\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 263 | loss: 0.60919 - acc: 0.7008 -- iter: 891/891\n",
      "--\n",
      "Training Step: 264  | total loss: \u001b[1m\u001b[32m0.60381\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 264 | loss: 0.60381 - acc: 0.7092 -- iter: 891/891\n",
      "--\n",
      "Training Step: 265  | total loss: \u001b[1m\u001b[32m0.59831\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 265 | loss: 0.59831 - acc: 0.7179 -- iter: 891/891\n",
      "--\n",
      "Training Step: 266  | total loss: \u001b[1m\u001b[32m0.59206\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 266 | loss: 0.59206 - acc: 0.7273 -- iter: 891/891\n",
      "--\n",
      "Training Step: 267  | total loss: \u001b[1m\u001b[32m0.58469\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 267 | loss: 0.58469 - acc: 0.7363 -- iter: 891/891\n",
      "--\n",
      "Training Step: 268  | total loss: \u001b[1m\u001b[32m0.57617\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 268 | loss: 0.57617 - acc: 0.7443 -- iter: 891/891\n",
      "--\n",
      "Training Step: 269  | total loss: \u001b[1m\u001b[32m0.56687\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 269 | loss: 0.56687 - acc: 0.7508 -- iter: 891/891\n",
      "--\n",
      "Training Step: 270  | total loss: \u001b[1m\u001b[32m0.59825\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 270 | loss: 0.59825 - acc: 0.7306 -- iter: 891/891\n",
      "--\n",
      "Training Step: 271  | total loss: \u001b[1m\u001b[32m0.58570\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 271 | loss: 0.58570 - acc: 0.7364 -- iter: 891/891\n",
      "--\n",
      "Training Step: 272  | total loss: \u001b[1m\u001b[32m0.57429\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 272 | loss: 0.57429 - acc: 0.7421 -- iter: 891/891\n",
      "--\n",
      "Training Step: 273  | total loss: \u001b[1m\u001b[32m0.56375\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 273 | loss: 0.56375 - acc: 0.7486 -- iter: 891/891\n",
      "--\n",
      "Training Step: 274  | total loss: \u001b[1m\u001b[32m0.60422\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 274 | loss: 0.60422 - acc: 0.7273 -- iter: 891/891\n",
      "--\n",
      "Training Step: 275  | total loss: \u001b[1m\u001b[32m0.58992\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 275 | loss: 0.58992 - acc: 0.7355 -- iter: 891/891\n",
      "--\n",
      "Training Step: 276  | total loss: \u001b[1m\u001b[32m0.57641\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 276 | loss: 0.57641 - acc: 0.7426 -- iter: 891/891\n",
      "--\n",
      "Training Step: 277  | total loss: \u001b[1m\u001b[32m0.56410\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 277 | loss: 0.56410 - acc: 0.7495 -- iter: 891/891\n",
      "--\n",
      "Training Step: 278  | total loss: \u001b[1m\u001b[32m0.55357\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 278 | loss: 0.55357 - acc: 0.7561 -- iter: 891/891\n",
      "--\n",
      "Training Step: 279  | total loss: \u001b[1m\u001b[32m0.54402\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 279 | loss: 0.54402 - acc: 0.7618 -- iter: 891/891\n",
      "--\n",
      "Training Step: 280  | total loss: \u001b[1m\u001b[32m0.58009\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 280 | loss: 0.58009 - acc: 0.7380 -- iter: 891/891\n",
      "--\n",
      "Training Step: 281  | total loss: \u001b[1m\u001b[32m0.56868\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 281 | loss: 0.56868 - acc: 0.7441 -- iter: 891/891\n",
      "--\n",
      "Training Step: 282  | total loss: \u001b[1m\u001b[32m0.55803\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 282 | loss: 0.55803 - acc: 0.7506 -- iter: 891/891\n",
      "--\n",
      "Training Step: 283  | total loss: \u001b[1m\u001b[32m0.54795\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 283 | loss: 0.54795 - acc: 0.7567 -- iter: 891/891\n",
      "--\n",
      "Training Step: 284  | total loss: \u001b[1m\u001b[32m0.53816\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 284 | loss: 0.53816 - acc: 0.7624 -- iter: 891/891\n",
      "--\n",
      "Training Step: 285  | total loss: \u001b[1m\u001b[32m0.52915\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 285 | loss: 0.52915 - acc: 0.7671 -- iter: 891/891\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 286  | total loss: \u001b[1m\u001b[32m0.57442\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 286 | loss: 0.57442 - acc: 0.7429 -- iter: 891/891\n",
      "--\n",
      "Training Step: 287  | total loss: \u001b[1m\u001b[32m0.56219\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 287 | loss: 0.56219 - acc: 0.7484 -- iter: 891/891\n",
      "--\n",
      "Training Step: 288  | total loss: \u001b[1m\u001b[32m0.59883\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 288 | loss: 0.59883 - acc: 0.7283 -- iter: 891/891\n",
      "--\n",
      "Training Step: 289  | total loss: \u001b[1m\u001b[32m0.58422\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 289 | loss: 0.58422 - acc: 0.7370 -- iter: 891/891\n",
      "--\n",
      "Training Step: 290  | total loss: \u001b[1m\u001b[32m0.60819\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 290 | loss: 0.60819 - acc: 0.7175 -- iter: 891/891\n",
      "--\n",
      "Training Step: 291  | total loss: \u001b[1m\u001b[32m0.59466\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 291 | loss: 0.59466 - acc: 0.7267 -- iter: 891/891\n",
      "--\n",
      "Training Step: 292  | total loss: \u001b[1m\u001b[32m0.61099\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 292 | loss: 0.61099 - acc: 0.7098 -- iter: 891/891\n",
      "--\n",
      "Training Step: 293  | total loss: \u001b[1m\u001b[32m0.60113\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 293 | loss: 0.60113 - acc: 0.7186 -- iter: 891/891\n",
      "--\n",
      "Training Step: 294  | total loss: \u001b[1m\u001b[32m0.59354\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 294 | loss: 0.59354 - acc: 0.7263 -- iter: 891/891\n",
      "--\n",
      "Training Step: 295  | total loss: \u001b[1m\u001b[32m0.58667\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 295 | loss: 0.58667 - acc: 0.7334 -- iter: 891/891\n",
      "--\n",
      "Training Step: 296  | total loss: \u001b[1m\u001b[32m0.57947\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 296 | loss: 0.57947 - acc: 0.7405 -- iter: 891/891\n",
      "--\n",
      "Training Step: 297  | total loss: \u001b[1m\u001b[32m0.57144\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 297 | loss: 0.57144 - acc: 0.7475 -- iter: 891/891\n",
      "--\n",
      "Training Step: 298  | total loss: \u001b[1m\u001b[32m0.56270\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 298 | loss: 0.56270 - acc: 0.7538 -- iter: 891/891\n",
      "--\n",
      "Training Step: 299  | total loss: \u001b[1m\u001b[32m0.55380\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 299 | loss: 0.55380 - acc: 0.7589 -- iter: 891/891\n",
      "--\n",
      "Training Step: 300  | total loss: \u001b[1m\u001b[32m0.54540\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 300 | loss: 0.54540 - acc: 0.7633 -- iter: 891/891\n",
      "--\n",
      "Training Step: 301  | total loss: \u001b[1m\u001b[32m0.53794\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 301 | loss: 0.53794 - acc: 0.7673 -- iter: 891/891\n",
      "--\n",
      "Training Step: 302  | total loss: \u001b[1m\u001b[32m0.58941\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 302 | loss: 0.58941 - acc: 0.7416 -- iter: 891/891\n",
      "--\n",
      "Training Step: 303  | total loss: \u001b[1m\u001b[32m0.57809\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 303 | loss: 0.57809 - acc: 0.7470 -- iter: 891/891\n",
      "--\n",
      "Training Step: 304  | total loss: \u001b[1m\u001b[32m0.62021\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 304 | loss: 0.62021 - acc: 0.7258 -- iter: 891/891\n",
      "--\n",
      "Training Step: 305  | total loss: \u001b[1m\u001b[32m0.60458\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 305 | loss: 0.60458 - acc: 0.7337 -- iter: 891/891\n",
      "--\n",
      "Training Step: 306  | total loss: \u001b[1m\u001b[32m0.58994\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 306 | loss: 0.58994 - acc: 0.7416 -- iter: 891/891\n",
      "--\n",
      "Training Step: 307  | total loss: \u001b[1m\u001b[32m0.57676\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 307 | loss: 0.57676 - acc: 0.7492 -- iter: 891/891\n",
      "--\n",
      "Training Step: 308  | total loss: \u001b[1m\u001b[32m0.56511\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 308 | loss: 0.56511 - acc: 0.7555 -- iter: 891/891\n",
      "--\n",
      "Training Step: 309  | total loss: \u001b[1m\u001b[32m0.55475\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 309 | loss: 0.55475 - acc: 0.7610 -- iter: 891/891\n",
      "--\n",
      "Training Step: 310  | total loss: \u001b[1m\u001b[32m0.54551\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 310 | loss: 0.54551 - acc: 0.7658 -- iter: 891/891\n",
      "--\n",
      "Training Step: 311  | total loss: \u001b[1m\u001b[32m0.53705\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 311 | loss: 0.53705 - acc: 0.7699 -- iter: 891/891\n",
      "--\n",
      "Training Step: 312  | total loss: \u001b[1m\u001b[32m0.52878\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 312 | loss: 0.52878 - acc: 0.7735 -- iter: 891/891\n",
      "--\n",
      "Training Step: 313  | total loss: \u001b[1m\u001b[32m0.52074\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 313 | loss: 0.52074 - acc: 0.7774 -- iter: 891/891\n",
      "--\n",
      "Training Step: 314  | total loss: \u001b[1m\u001b[32m0.51335\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 314 | loss: 0.51335 - acc: 0.7810 -- iter: 891/891\n",
      "--\n",
      "Training Step: 315  | total loss: \u001b[1m\u001b[32m0.50668\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 315 | loss: 0.50668 - acc: 0.7842 -- iter: 891/891\n",
      "--\n",
      "Training Step: 316  | total loss: \u001b[1m\u001b[32m0.50096\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 316 | loss: 0.50096 - acc: 0.7866 -- iter: 891/891\n",
      "--\n",
      "Training Step: 317  | total loss: \u001b[1m\u001b[32m0.49607\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 317 | loss: 0.49607 - acc: 0.7883 -- iter: 891/891\n",
      "--\n",
      "Training Step: 318  | total loss: \u001b[1m\u001b[32m0.55346\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 318 | loss: 0.55346 - acc: 0.7647 -- iter: 891/891\n",
      "--\n",
      "Training Step: 319  | total loss: \u001b[1m\u001b[32m0.54284\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 319 | loss: 0.54284 - acc: 0.7694 -- iter: 891/891\n",
      "--\n",
      "Training Step: 320  | total loss: \u001b[1m\u001b[32m0.58660\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 320 | loss: 0.58660 - acc: 0.7463 -- iter: 891/891\n",
      "--\n",
      "Training Step: 321  | total loss: \u001b[1m\u001b[32m0.57366\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 321 | loss: 0.57366 - acc: 0.7524 -- iter: 891/891\n",
      "--\n",
      "Training Step: 322  | total loss: \u001b[1m\u001b[32m0.56315\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 322 | loss: 0.56315 - acc: 0.7575 -- iter: 891/891\n",
      "--\n",
      "Training Step: 323  | total loss: \u001b[1m\u001b[32m0.55403\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 323 | loss: 0.55403 - acc: 0.7616 -- iter: 891/891\n",
      "--\n",
      "Training Step: 324  | total loss: \u001b[1m\u001b[32m0.54500\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 324 | loss: 0.54500 - acc: 0.7656 -- iter: 891/891\n",
      "--\n",
      "Training Step: 325  | total loss: \u001b[1m\u001b[32m0.53588\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 325 | loss: 0.53588 - acc: 0.7695 -- iter: 891/891\n",
      "--\n",
      "Training Step: 326  | total loss: \u001b[1m\u001b[32m0.52741\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 326 | loss: 0.52741 - acc: 0.7734 -- iter: 891/891\n",
      "--\n",
      "Training Step: 327  | total loss: \u001b[1m\u001b[32m0.51999\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 327 | loss: 0.51999 - acc: 0.7775 -- iter: 891/891\n",
      "--\n",
      "Training Step: 328  | total loss: \u001b[1m\u001b[32m0.56014\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 328 | loss: 0.56014 - acc: 0.7552 -- iter: 891/891\n",
      "--\n",
      "Training Step: 329  | total loss: \u001b[1m\u001b[32m0.54984\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 329 | loss: 0.54984 - acc: 0.7606 -- iter: 891/891\n",
      "--\n",
      "Training Step: 330  | total loss: \u001b[1m\u001b[32m0.54065\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 330 | loss: 0.54065 - acc: 0.7649 -- iter: 891/891\n",
      "--\n",
      "Training Step: 331  | total loss: \u001b[1m\u001b[32m0.53235\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 331 | loss: 0.53235 - acc: 0.7689 -- iter: 891/891\n",
      "--\n",
      "Training Step: 332  | total loss: \u001b[1m\u001b[32m0.52472\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 332 | loss: 0.52472 - acc: 0.7726 -- iter: 891/891\n",
      "--\n",
      "Training Step: 333  | total loss: \u001b[1m\u001b[32m0.51769\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 333 | loss: 0.51769 - acc: 0.7761 -- iter: 891/891\n",
      "--\n",
      "Training Step: 334  | total loss: \u001b[1m\u001b[32m0.55266\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 334 | loss: 0.55266 - acc: 0.7543 -- iter: 891/891\n",
      "--\n",
      "Training Step: 335  | total loss: \u001b[1m\u001b[32m0.54309\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 335 | loss: 0.54309 - acc: 0.7597 -- iter: 891/891\n",
      "--\n",
      "Training Step: 336  | total loss: \u001b[1m\u001b[32m0.56840\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 336 | loss: 0.56840 - acc: 0.7378 -- iter: 891/891\n",
      "--\n",
      "Training Step: 337  | total loss: \u001b[1m\u001b[32m0.55987\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 337 | loss: 0.55987 - acc: 0.7446 -- iter: 891/891\n",
      "--\n",
      "Training Step: 338  | total loss: \u001b[1m\u001b[32m0.58123\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 338 | loss: 0.58123 - acc: 0.7240 -- iter: 891/891\n",
      "--\n",
      "Training Step: 339  | total loss: \u001b[1m\u001b[32m0.57485\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 339 | loss: 0.57485 - acc: 0.7311 -- iter: 891/891\n",
      "--\n",
      "Training Step: 340  | total loss: \u001b[1m\u001b[32m0.59059\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 340 | loss: 0.59059 - acc: 0.7131 -- iter: 891/891\n",
      "--\n",
      "Training Step: 341  | total loss: \u001b[1m\u001b[32m0.58471\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 341 | loss: 0.58471 - acc: 0.7212 -- iter: 891/891\n",
      "--\n",
      "Training Step: 342  | total loss: \u001b[1m\u001b[32m0.57913\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 342 | loss: 0.57913 - acc: 0.7285 -- iter: 891/891\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 343  | total loss: \u001b[1m\u001b[32m0.57308\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 343 | loss: 0.57308 - acc: 0.7362 -- iter: 891/891\n",
      "--\n",
      "Training Step: 344  | total loss: \u001b[1m\u001b[32m0.56626\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 344 | loss: 0.56626 - acc: 0.7436 -- iter: 891/891\n",
      "--\n",
      "Training Step: 345  | total loss: \u001b[1m\u001b[32m0.55877\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 345 | loss: 0.55877 - acc: 0.7497 -- iter: 891/891\n",
      "--\n",
      "Training Step: 346  | total loss: \u001b[1m\u001b[32m0.55103\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 346 | loss: 0.55103 - acc: 0.7549 -- iter: 891/891\n",
      "--\n",
      "Training Step: 347  | total loss: \u001b[1m\u001b[32m0.54358\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 347 | loss: 0.54358 - acc: 0.7577 -- iter: 891/891\n",
      "--\n",
      "Training Step: 348  | total loss: \u001b[1m\u001b[32m0.53688\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 348 | loss: 0.53688 - acc: 0.7601 -- iter: 891/891\n",
      "--\n",
      "Training Step: 349  | total loss: \u001b[1m\u001b[32m0.53115\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 349 | loss: 0.53115 - acc: 0.7620 -- iter: 891/891\n",
      "--\n",
      "Training Step: 350  | total loss: \u001b[1m\u001b[32m0.52640\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 350 | loss: 0.52640 - acc: 0.7653 -- iter: 891/891\n",
      "--\n",
      "Training Step: 351  | total loss: \u001b[1m\u001b[32m0.52237\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 351 | loss: 0.52237 - acc: 0.7683 -- iter: 891/891\n",
      "--\n",
      "Training Step: 352  | total loss: \u001b[1m\u001b[32m0.51850\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 352 | loss: 0.51850 - acc: 0.7712 -- iter: 891/891\n",
      "--\n",
      "Training Step: 353  | total loss: \u001b[1m\u001b[32m0.51419\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 353 | loss: 0.51419 - acc: 0.7742 -- iter: 891/891\n",
      "--\n",
      "Training Step: 354  | total loss: \u001b[1m\u001b[32m0.56086\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 354 | loss: 0.56086 - acc: 0.7532 -- iter: 891/891\n",
      "--\n",
      "Training Step: 355  | total loss: \u001b[1m\u001b[32m0.55011\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 355 | loss: 0.55011 - acc: 0.7597 -- iter: 891/891\n",
      "--\n",
      "Training Step: 356  | total loss: \u001b[1m\u001b[32m0.54021\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 356 | loss: 0.54021 - acc: 0.7650 -- iter: 891/891\n",
      "--\n",
      "Training Step: 357  | total loss: \u001b[1m\u001b[32m0.53196\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 357 | loss: 0.53196 - acc: 0.7696 -- iter: 891/891\n",
      "--\n",
      "Training Step: 358  | total loss: \u001b[1m\u001b[32m0.52556\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 358 | loss: 0.52556 - acc: 0.7737 -- iter: 891/891\n",
      "--\n",
      "Training Step: 359  | total loss: \u001b[1m\u001b[32m0.52031\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 359 | loss: 0.52031 - acc: 0.7768 -- iter: 891/891\n",
      "--\n",
      "Training Step: 360  | total loss: \u001b[1m\u001b[32m0.51516\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 360 | loss: 0.51516 - acc: 0.7797 -- iter: 891/891\n",
      "--\n",
      "Training Step: 361  | total loss: \u001b[1m\u001b[32m0.50941\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 361 | loss: 0.50941 - acc: 0.7826 -- iter: 891/891\n",
      "--\n",
      "Training Step: 362  | total loss: \u001b[1m\u001b[32m0.50330\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 362 | loss: 0.50330 - acc: 0.7855 -- iter: 891/891\n",
      "--\n",
      "Training Step: 363  | total loss: \u001b[1m\u001b[32m0.49761\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 363 | loss: 0.49761 - acc: 0.7887 -- iter: 891/891\n",
      "--\n",
      "Training Step: 364  | total loss: \u001b[1m\u001b[32m0.49277\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 364 | loss: 0.49277 - acc: 0.7910 -- iter: 891/891\n",
      "--\n",
      "Training Step: 365  | total loss: \u001b[1m\u001b[32m0.48882\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 365 | loss: 0.48882 - acc: 0.7926 -- iter: 891/891\n",
      "--\n",
      "Training Step: 366  | total loss: \u001b[1m\u001b[32m0.48560\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 366 | loss: 0.48560 - acc: 0.7938 -- iter: 891/891\n",
      "--\n",
      "Training Step: 367  | total loss: \u001b[1m\u001b[32m0.48263\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 367 | loss: 0.48263 - acc: 0.7944 -- iter: 891/891\n",
      "--\n",
      "Training Step: 368  | total loss: \u001b[1m\u001b[32m0.54221\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 368 | loss: 0.54221 - acc: 0.7678 -- iter: 891/891\n",
      "--\n",
      "Training Step: 369  | total loss: \u001b[1m\u001b[32m0.53253\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 369 | loss: 0.53253 - acc: 0.7723 -- iter: 891/891\n",
      "--\n",
      "Training Step: 370  | total loss: \u001b[1m\u001b[32m0.57191\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 370 | loss: 0.57191 - acc: 0.7502 -- iter: 891/891\n",
      "--\n",
      "Training Step: 371  | total loss: \u001b[1m\u001b[32m0.56064\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 371 | loss: 0.56064 - acc: 0.7559 -- iter: 891/891\n",
      "--\n",
      "Training Step: 372  | total loss: \u001b[1m\u001b[32m0.55202\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 372 | loss: 0.55202 - acc: 0.7604 -- iter: 891/891\n",
      "--\n",
      "Training Step: 373  | total loss: \u001b[1m\u001b[32m0.54510\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 373 | loss: 0.54510 - acc: 0.7641 -- iter: 891/891\n",
      "--\n",
      "Training Step: 374  | total loss: \u001b[1m\u001b[32m0.57180\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 374 | loss: 0.57180 - acc: 0.7411 -- iter: 891/891\n",
      "--\n",
      "Training Step: 375  | total loss: \u001b[1m\u001b[32m0.56319\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 375 | loss: 0.56319 - acc: 0.7473 -- iter: 891/891\n",
      "--\n",
      "Training Step: 376  | total loss: \u001b[1m\u001b[32m0.55531\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 376 | loss: 0.55531 - acc: 0.7532 -- iter: 891/891\n",
      "--\n",
      "Training Step: 377  | total loss: \u001b[1m\u001b[32m0.54766\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 377 | loss: 0.54766 - acc: 0.7590 -- iter: 891/891\n",
      "--\n",
      "Training Step: 378  | total loss: \u001b[1m\u001b[32m0.54008\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 378 | loss: 0.54008 - acc: 0.7638 -- iter: 891/891\n",
      "--\n",
      "Training Step: 379  | total loss: \u001b[1m\u001b[32m0.53272\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 379 | loss: 0.53272 - acc: 0.7688 -- iter: 891/891\n",
      "--\n",
      "Training Step: 380  | total loss: \u001b[1m\u001b[32m0.56497\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 380 | loss: 0.56497 - acc: 0.7466 -- iter: 891/891\n",
      "--\n",
      "Training Step: 381  | total loss: \u001b[1m\u001b[32m0.55511\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 381 | loss: 0.55511 - acc: 0.7527 -- iter: 891/891\n",
      "--\n",
      "Training Step: 382  | total loss: \u001b[1m\u001b[32m0.54637\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 382 | loss: 0.54637 - acc: 0.7580 -- iter: 891/891\n",
      "--\n",
      "Training Step: 383  | total loss: \u001b[1m\u001b[32m0.53847\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 383 | loss: 0.53847 - acc: 0.7629 -- iter: 891/891\n",
      "--\n",
      "Training Step: 384  | total loss: \u001b[1m\u001b[32m0.53121\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 384 | loss: 0.53121 - acc: 0.7670 -- iter: 891/891\n",
      "--\n",
      "Training Step: 385  | total loss: \u001b[1m\u001b[32m0.52446\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 385 | loss: 0.52446 - acc: 0.7708 -- iter: 891/891\n",
      "--\n",
      "Training Step: 386  | total loss: \u001b[1m\u001b[32m0.51812\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 386 | loss: 0.51812 - acc: 0.7744 -- iter: 891/891\n",
      "--\n",
      "Training Step: 387  | total loss: \u001b[1m\u001b[32m0.51206\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 387 | loss: 0.51206 - acc: 0.7779 -- iter: 891/891\n",
      "--\n",
      "Training Step: 388  | total loss: \u001b[1m\u001b[32m0.50619\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 388 | loss: 0.50619 - acc: 0.7807 -- iter: 891/891\n",
      "--\n",
      "Training Step: 389  | total loss: \u001b[1m\u001b[32m0.50056\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 389 | loss: 0.50056 - acc: 0.7833 -- iter: 891/891\n",
      "--\n",
      "Training Step: 390  | total loss: \u001b[1m\u001b[32m0.49518\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 390 | loss: 0.49518 - acc: 0.7860 -- iter: 891/891\n",
      "--\n",
      "Training Step: 391  | total loss: \u001b[1m\u001b[32m0.49011\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 391 | loss: 0.49011 - acc: 0.7887 -- iter: 891/891\n",
      "--\n",
      "Training Step: 392  | total loss: \u001b[1m\u001b[32m0.53449\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 392 | loss: 0.53449 - acc: 0.7638 -- iter: 891/891\n",
      "--\n",
      "Training Step: 393  | total loss: \u001b[1m\u001b[32m0.52622\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 393 | loss: 0.52622 - acc: 0.7684 -- iter: 891/891\n",
      "--\n",
      "Training Step: 394  | total loss: \u001b[1m\u001b[32m0.55922\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 394 | loss: 0.55922 - acc: 0.7465 -- iter: 891/891\n",
      "--\n",
      "Training Step: 395  | total loss: \u001b[1m\u001b[32m0.55118\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 395 | loss: 0.55118 - acc: 0.7522 -- iter: 891/891\n",
      "--\n",
      "Training Step: 396  | total loss: \u001b[1m\u001b[32m0.54512\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 396 | loss: 0.54512 - acc: 0.7570 -- iter: 891/891\n",
      "--\n",
      "Training Step: 397  | total loss: \u001b[1m\u001b[32m0.53933\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 397 | loss: 0.53933 - acc: 0.7615 -- iter: 891/891\n",
      "--\n",
      "Training Step: 398  | total loss: \u001b[1m\u001b[32m0.56430\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 398 | loss: 0.56430 - acc: 0.7419 -- iter: 891/891\n",
      "--\n",
      "Training Step: 399  | total loss: \u001b[1m\u001b[32m0.55531\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 399 | loss: 0.55531 - acc: 0.7489 -- iter: 891/891\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 400  | total loss: \u001b[1m\u001b[32m0.57771\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 400 | loss: 0.57771 - acc: 0.7288 -- iter: 891/891\n",
      "--\n",
      "Training Step: 401  | total loss: \u001b[1m\u001b[32m0.56809\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 401 | loss: 0.56809 - acc: 0.7373 -- iter: 891/891\n",
      "--\n",
      "Training Step: 402  | total loss: \u001b[1m\u001b[32m0.58553\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 402 | loss: 0.58553 - acc: 0.7202 -- iter: 891/891\n",
      "--\n",
      "Training Step: 403  | total loss: \u001b[1m\u001b[32m0.57673\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 403 | loss: 0.57673 - acc: 0.7289 -- iter: 891/891\n",
      "--\n",
      "Training Step: 404  | total loss: \u001b[1m\u001b[32m0.56950\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 404 | loss: 0.56950 - acc: 0.7364 -- iter: 891/891\n",
      "--\n",
      "Training Step: 405  | total loss: \u001b[1m\u001b[32m0.56318\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 405 | loss: 0.56318 - acc: 0.7432 -- iter: 891/891\n",
      "--\n",
      "Training Step: 406  | total loss: \u001b[1m\u001b[32m0.58048\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 406 | loss: 0.58048 - acc: 0.7252 -- iter: 891/891\n",
      "--\n",
      "Training Step: 407  | total loss: \u001b[1m\u001b[32m0.57318\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 407 | loss: 0.57318 - acc: 0.7326 -- iter: 891/891\n",
      "--\n",
      "Training Step: 408  | total loss: \u001b[1m\u001b[32m0.56654\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 408 | loss: 0.56654 - acc: 0.7394 -- iter: 891/891\n",
      "--\n",
      "Training Step: 409  | total loss: \u001b[1m\u001b[32m0.56016\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 409 | loss: 0.56016 - acc: 0.7452 -- iter: 891/891\n",
      "--\n",
      "Training Step: 410  | total loss: \u001b[1m\u001b[32m0.58026\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 410 | loss: 0.58026 - acc: 0.7257 -- iter: 891/891\n",
      "--\n",
      "Training Step: 411  | total loss: \u001b[1m\u001b[32m0.57194\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 411 | loss: 0.57194 - acc: 0.7311 -- iter: 891/891\n",
      "--\n",
      "Training Step: 412  | total loss: \u001b[1m\u001b[32m0.56421\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 412 | loss: 0.56421 - acc: 0.7364 -- iter: 891/891\n",
      "--\n",
      "Training Step: 413  | total loss: \u001b[1m\u001b[32m0.55678\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 413 | loss: 0.55678 - acc: 0.7425 -- iter: 891/891\n",
      "--\n",
      "Training Step: 414  | total loss: \u001b[1m\u001b[32m0.54946\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 414 | loss: 0.54946 - acc: 0.7485 -- iter: 891/891\n",
      "--\n",
      "Training Step: 415  | total loss: \u001b[1m\u001b[32m0.54215\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 415 | loss: 0.54215 - acc: 0.7540 -- iter: 891/891\n",
      "--\n",
      "Training Step: 416  | total loss: \u001b[1m\u001b[32m0.53488\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 416 | loss: 0.53488 - acc: 0.7594 -- iter: 891/891\n",
      "--\n",
      "Training Step: 417  | total loss: \u001b[1m\u001b[32m0.52773\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 417 | loss: 0.52773 - acc: 0.7649 -- iter: 891/891\n",
      "--\n",
      "Training Step: 418  | total loss: \u001b[1m\u001b[32m0.56460\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 418 | loss: 0.56460 - acc: 0.7455 -- iter: 891/891\n",
      "--\n",
      "Training Step: 419  | total loss: \u001b[1m\u001b[32m0.55363\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 419 | loss: 0.55363 - acc: 0.7519 -- iter: 891/891\n",
      "--\n",
      "Training Step: 420  | total loss: \u001b[1m\u001b[32m0.58942\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 420 | loss: 0.58942 - acc: 0.7291 -- iter: 891/891\n",
      "--\n",
      "Training Step: 421  | total loss: \u001b[1m\u001b[32m0.57553\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 421 | loss: 0.57553 - acc: 0.7376 -- iter: 891/891\n",
      "--\n",
      "Training Step: 422  | total loss: \u001b[1m\u001b[32m0.56337\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 422 | loss: 0.56337 - acc: 0.7451 -- iter: 891/891\n",
      "--\n",
      "Training Step: 423  | total loss: \u001b[1m\u001b[32m0.55283\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 423 | loss: 0.55283 - acc: 0.7518 -- iter: 891/891\n",
      "--\n",
      "Training Step: 424  | total loss: \u001b[1m\u001b[32m0.58423\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 424 | loss: 0.58423 - acc: 0.7309 -- iter: 891/891\n",
      "--\n",
      "Training Step: 425  | total loss: \u001b[1m\u001b[32m0.57290\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 425 | loss: 0.57290 - acc: 0.7385 -- iter: 891/891\n",
      "--\n",
      "Training Step: 426  | total loss: \u001b[1m\u001b[32m0.59356\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 426 | loss: 0.59356 - acc: 0.7217 -- iter: 891/891\n",
      "--\n",
      "Training Step: 427  | total loss: \u001b[1m\u001b[32m0.58278\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 427 | loss: 0.58278 - acc: 0.7300 -- iter: 891/891\n",
      "--\n",
      "Training Step: 428  | total loss: \u001b[1m\u001b[32m0.60206\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 428 | loss: 0.60206 - acc: 0.7102 -- iter: 891/891\n",
      "--\n",
      "Training Step: 429  | total loss: \u001b[1m\u001b[32m0.59156\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 429 | loss: 0.59156 - acc: 0.7198 -- iter: 891/891\n",
      "--\n",
      "Training Step: 430  | total loss: \u001b[1m\u001b[32m0.60736\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 430 | loss: 0.60736 - acc: 0.7025 -- iter: 891/891\n",
      "--\n",
      "Training Step: 431  | total loss: \u001b[1m\u001b[32m0.59717\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 431 | loss: 0.59717 - acc: 0.7128 -- iter: 891/891\n",
      "--\n",
      "Training Step: 432  | total loss: \u001b[1m\u001b[32m0.58806\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 432 | loss: 0.58806 - acc: 0.7225 -- iter: 891/891\n",
      "--\n",
      "Training Step: 433  | total loss: \u001b[1m\u001b[32m0.57941\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 433 | loss: 0.57941 - acc: 0.7313 -- iter: 891/891\n",
      "--\n",
      "Training Step: 434  | total loss: \u001b[1m\u001b[32m0.57089\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 434 | loss: 0.57089 - acc: 0.7392 -- iter: 891/891\n",
      "--\n",
      "Training Step: 435  | total loss: \u001b[1m\u001b[32m0.56243\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 435 | loss: 0.56243 - acc: 0.7458 -- iter: 891/891\n",
      "--\n",
      "Training Step: 436  | total loss: \u001b[1m\u001b[32m0.55417\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 436 | loss: 0.55417 - acc: 0.7504 -- iter: 891/891\n",
      "--\n",
      "Training Step: 437  | total loss: \u001b[1m\u001b[32m0.54635\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 437 | loss: 0.54635 - acc: 0.7534 -- iter: 891/891\n",
      "--\n",
      "Training Step: 438  | total loss: \u001b[1m\u001b[32m0.58184\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 438 | loss: 0.58184 - acc: 0.7321 -- iter: 891/891\n",
      "--\n",
      "Training Step: 439  | total loss: \u001b[1m\u001b[32m0.57109\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 439 | loss: 0.57109 - acc: 0.7376 -- iter: 891/891\n",
      "--\n",
      "Training Step: 440  | total loss: \u001b[1m\u001b[32m0.60689\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 440 | loss: 0.60689 - acc: 0.7163 -- iter: 891/891\n",
      "--\n",
      "Training Step: 441  | total loss: \u001b[1m\u001b[32m0.59363\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 441 | loss: 0.59363 - acc: 0.7244 -- iter: 891/891\n",
      "--\n",
      "Training Step: 442  | total loss: \u001b[1m\u001b[32m0.61936\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 442 | loss: 0.61936 - acc: 0.7059 -- iter: 891/891\n",
      "--\n",
      "Training Step: 443  | total loss: \u001b[1m\u001b[32m0.60517\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 443 | loss: 0.60517 - acc: 0.7163 -- iter: 891/891\n",
      "--\n",
      "Training Step: 444  | total loss: \u001b[1m\u001b[32m0.62118\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 444 | loss: 0.62118 - acc: 0.7017 -- iter: 891/891\n",
      "--\n",
      "Training Step: 445  | total loss: \u001b[1m\u001b[32m0.60849\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 445 | loss: 0.60849 - acc: 0.7120 -- iter: 891/891\n",
      "--\n",
      "Training Step: 446  | total loss: \u001b[1m\u001b[32m0.62045\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 446 | loss: 0.62045 - acc: 0.6952 -- iter: 891/891\n",
      "--\n",
      "Training Step: 447  | total loss: \u001b[1m\u001b[32m0.61090\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 447 | loss: 0.61090 - acc: 0.7052 -- iter: 891/891\n",
      "--\n",
      "Training Step: 448  | total loss: \u001b[1m\u001b[32m0.62009\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 448 | loss: 0.62009 - acc: 0.6894 -- iter: 891/891\n",
      "--\n",
      "Training Step: 449  | total loss: \u001b[1m\u001b[32m0.61296\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 449 | loss: 0.61296 - acc: 0.6998 -- iter: 891/891\n",
      "--\n",
      "Training Step: 450  | total loss: \u001b[1m\u001b[32m0.60703\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 450 | loss: 0.60703 - acc: 0.7089 -- iter: 891/891\n",
      "--\n",
      "Training Step: 451  | total loss: \u001b[1m\u001b[32m0.60146\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 451 | loss: 0.60146 - acc: 0.7172 -- iter: 891/891\n",
      "--\n",
      "Training Step: 452  | total loss: \u001b[1m\u001b[32m0.61212\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 452 | loss: 0.61212 - acc: 0.6987 -- iter: 891/891\n",
      "--\n",
      "Training Step: 453  | total loss: \u001b[1m\u001b[32m0.60470\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 453 | loss: 0.60470 - acc: 0.7093 -- iter: 891/891\n",
      "--\n",
      "Training Step: 454  | total loss: \u001b[1m\u001b[32m0.59699\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 454 | loss: 0.59699 - acc: 0.7195 -- iter: 891/891\n",
      "--\n",
      "Training Step: 455  | total loss: \u001b[1m\u001b[32m0.58860\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 455 | loss: 0.58860 - acc: 0.7288 -- iter: 891/891\n",
      "--\n",
      "Training Step: 456  | total loss: \u001b[1m\u001b[32m0.57942\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 456 | loss: 0.57942 - acc: 0.7365 -- iter: 891/891\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 457  | total loss: \u001b[1m\u001b[32m0.56970\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 457 | loss: 0.56970 - acc: 0.7437 -- iter: 891/891\n",
      "--\n",
      "Training Step: 458  | total loss: \u001b[1m\u001b[32m0.55991\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 458 | loss: 0.55991 - acc: 0.7483 -- iter: 891/891\n",
      "--\n",
      "Training Step: 459  | total loss: \u001b[1m\u001b[32m0.55062\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 459 | loss: 0.55062 - acc: 0.7529 -- iter: 891/891\n",
      "--\n",
      "Training Step: 460  | total loss: \u001b[1m\u001b[32m0.54231\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 460 | loss: 0.54231 - acc: 0.7582 -- iter: 891/891\n",
      "--\n",
      "Training Step: 461  | total loss: \u001b[1m\u001b[32m0.53526\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 461 | loss: 0.53526 - acc: 0.7629 -- iter: 891/891\n",
      "--\n",
      "Training Step: 462  | total loss: \u001b[1m\u001b[32m0.52936\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 462 | loss: 0.52936 - acc: 0.7662 -- iter: 891/891\n",
      "--\n",
      "Training Step: 463  | total loss: \u001b[1m\u001b[32m0.52408\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 463 | loss: 0.52408 - acc: 0.7695 -- iter: 891/891\n",
      "--\n",
      "Training Step: 464  | total loss: \u001b[1m\u001b[32m0.57628\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 464 | loss: 0.57628 - acc: 0.7475 -- iter: 891/891\n",
      "--\n",
      "Training Step: 465  | total loss: \u001b[1m\u001b[32m0.56453\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 465 | loss: 0.56453 - acc: 0.7537 -- iter: 891/891\n",
      "--\n",
      "Training Step: 466  | total loss: \u001b[1m\u001b[32m0.61139\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 466 | loss: 0.61139 - acc: 0.7312 -- iter: 891/891\n",
      "--\n",
      "Training Step: 467  | total loss: \u001b[1m\u001b[32m0.59521\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 467 | loss: 0.59521 - acc: 0.7392 -- iter: 891/891\n",
      "--\n",
      "Training Step: 468  | total loss: \u001b[1m\u001b[32m0.61821\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 468 | loss: 0.61821 - acc: 0.7213 -- iter: 891/891\n",
      "--\n",
      "Training Step: 469  | total loss: \u001b[1m\u001b[32m0.60585\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 469 | loss: 0.60585 - acc: 0.7294 -- iter: 891/891\n",
      "--\n",
      "Training Step: 470  | total loss: \u001b[1m\u001b[32m0.59744\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 470 | loss: 0.59744 - acc: 0.7354 -- iter: 891/891\n",
      "--\n",
      "Training Step: 471  | total loss: \u001b[1m\u001b[32m0.59117\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 471 | loss: 0.59117 - acc: 0.7401 -- iter: 891/891\n",
      "--\n",
      "Training Step: 472  | total loss: \u001b[1m\u001b[32m0.58529\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 472 | loss: 0.58529 - acc: 0.7453 -- iter: 891/891\n",
      "--\n",
      "Training Step: 473  | total loss: \u001b[1m\u001b[32m0.57857\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 473 | loss: 0.57857 - acc: 0.7508 -- iter: 891/891\n",
      "--\n",
      "Training Step: 474  | total loss: \u001b[1m\u001b[32m0.59622\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 474 | loss: 0.59622 - acc: 0.7278 -- iter: 891/891\n",
      "--\n",
      "Training Step: 475  | total loss: \u001b[1m\u001b[32m0.58599\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 475 | loss: 0.58599 - acc: 0.7365 -- iter: 891/891\n",
      "--\n",
      "Training Step: 476  | total loss: \u001b[1m\u001b[32m0.57614\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 476 | loss: 0.57614 - acc: 0.7442 -- iter: 891/891\n",
      "--\n",
      "Training Step: 477  | total loss: \u001b[1m\u001b[32m0.56658\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 477 | loss: 0.56658 - acc: 0.7500 -- iter: 891/891\n",
      "--\n",
      "Training Step: 478  | total loss: \u001b[1m\u001b[32m0.55740\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 478 | loss: 0.55740 - acc: 0.7557 -- iter: 891/891\n",
      "--\n",
      "Training Step: 479  | total loss: \u001b[1m\u001b[32m0.54879\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 479 | loss: 0.54879 - acc: 0.7605 -- iter: 891/891\n",
      "--\n",
      "Training Step: 480  | total loss: \u001b[1m\u001b[32m0.54098\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 480 | loss: 0.54098 - acc: 0.7650 -- iter: 891/891\n",
      "--\n",
      "Training Step: 481  | total loss: \u001b[1m\u001b[32m0.53413\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 481 | loss: 0.53413 - acc: 0.7688 -- iter: 891/891\n",
      "--\n",
      "Training Step: 482  | total loss: \u001b[1m\u001b[32m0.52820\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 482 | loss: 0.52820 - acc: 0.7716 -- iter: 891/891\n",
      "--\n",
      "Training Step: 483  | total loss: \u001b[1m\u001b[32m0.52295\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 483 | loss: 0.52295 - acc: 0.7741 -- iter: 891/891\n",
      "--\n",
      "Training Step: 484  | total loss: \u001b[1m\u001b[32m0.51807\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 484 | loss: 0.51807 - acc: 0.7770 -- iter: 891/891\n",
      "--\n",
      "Training Step: 485  | total loss: \u001b[1m\u001b[32m0.51329\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 485 | loss: 0.51329 - acc: 0.7802 -- iter: 891/891\n",
      "--\n",
      "Training Step: 486  | total loss: \u001b[1m\u001b[32m0.50833\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 486 | loss: 0.50833 - acc: 0.7829 -- iter: 891/891\n",
      "--\n",
      "Training Step: 487  | total loss: \u001b[1m\u001b[32m0.50308\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 487 | loss: 0.50308 - acc: 0.7855 -- iter: 891/891\n",
      "--\n",
      "Training Step: 488  | total loss: \u001b[1m\u001b[32m0.54953\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 488 | loss: 0.54953 - acc: 0.7598 -- iter: 891/891\n",
      "--\n",
      "Training Step: 489  | total loss: \u001b[1m\u001b[32m0.53954\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 489 | loss: 0.53954 - acc: 0.7650 -- iter: 891/891\n",
      "--\n",
      "Training Step: 490  | total loss: \u001b[1m\u001b[32m0.53167\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 490 | loss: 0.53167 - acc: 0.7695 -- iter: 891/891\n",
      "--\n",
      "Training Step: 491  | total loss: \u001b[1m\u001b[32m0.52571\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 491 | loss: 0.52571 - acc: 0.7731 -- iter: 891/891\n",
      "--\n",
      "Training Step: 492  | total loss: \u001b[1m\u001b[32m0.55501\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 492 | loss: 0.55501 - acc: 0.7491 -- iter: 891/891\n",
      "--\n",
      "Training Step: 493  | total loss: \u001b[1m\u001b[32m0.54793\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 493 | loss: 0.54793 - acc: 0.7549 -- iter: 891/891\n",
      "--\n",
      "Training Step: 494  | total loss: \u001b[1m\u001b[32m0.54142\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 494 | loss: 0.54142 - acc: 0.7600 -- iter: 891/891\n",
      "--\n",
      "Training Step: 495  | total loss: \u001b[1m\u001b[32m0.53457\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 495 | loss: 0.53457 - acc: 0.7648 -- iter: 891/891\n",
      "--\n",
      "Training Step: 496  | total loss: \u001b[1m\u001b[32m0.52736\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 496 | loss: 0.52736 - acc: 0.7696 -- iter: 891/891\n",
      "--\n",
      "Training Step: 497  | total loss: \u001b[1m\u001b[32m0.52026\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 497 | loss: 0.52026 - acc: 0.7741 -- iter: 891/891\n",
      "--\n",
      "Training Step: 498  | total loss: \u001b[1m\u001b[32m0.55579\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 498 | loss: 0.55579 - acc: 0.7501 -- iter: 891/891\n",
      "--\n",
      "Training Step: 499  | total loss: \u001b[1m\u001b[32m0.54595\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 499 | loss: 0.54595 - acc: 0.7566 -- iter: 891/891\n",
      "--\n",
      "Training Step: 500  | total loss: \u001b[1m\u001b[32m0.57806\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 500 | loss: 0.57806 - acc: 0.7359 -- iter: 891/891\n",
      "--\n",
      "Training Step: 501  | total loss: \u001b[1m\u001b[32m0.56642\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 501 | loss: 0.56642 - acc: 0.7434 -- iter: 891/891\n",
      "--\n",
      "Training Step: 502  | total loss: \u001b[1m\u001b[32m0.59139\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 502 | loss: 0.59139 - acc: 0.7239 -- iter: 891/891\n",
      "--\n",
      "Training Step: 503  | total loss: \u001b[1m\u001b[32m0.57983\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 503 | loss: 0.57983 - acc: 0.7322 -- iter: 891/891\n",
      "--\n",
      "Training Step: 504  | total loss: \u001b[1m\u001b[32m0.57032\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 504 | loss: 0.57032 - acc: 0.7399 -- iter: 891/891\n",
      "--\n",
      "Training Step: 505  | total loss: \u001b[1m\u001b[32m0.56227\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 505 | loss: 0.56227 - acc: 0.7470 -- iter: 891/891\n",
      "--\n",
      "Training Step: 506  | total loss: \u001b[1m\u001b[32m0.55518\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 506 | loss: 0.55518 - acc: 0.7539 -- iter: 891/891\n",
      "--\n",
      "Training Step: 507  | total loss: \u001b[1m\u001b[32m0.54859\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 507 | loss: 0.54859 - acc: 0.7600 -- iter: 891/891\n",
      "--\n",
      "Training Step: 508  | total loss: \u001b[1m\u001b[32m0.54207\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 508 | loss: 0.54207 - acc: 0.7654 -- iter: 891/891\n",
      "--\n",
      "Training Step: 509  | total loss: \u001b[1m\u001b[32m0.53531\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 509 | loss: 0.53531 - acc: 0.7704 -- iter: 891/891\n",
      "--\n",
      "Training Step: 510  | total loss: \u001b[1m\u001b[32m0.52827\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 510 | loss: 0.52827 - acc: 0.7750 -- iter: 891/891\n",
      "--\n",
      "Training Step: 511  | total loss: \u001b[1m\u001b[32m0.52123\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 511 | loss: 0.52123 - acc: 0.7783 -- iter: 891/891\n",
      "--\n",
      "Training Step: 512  | total loss: \u001b[1m\u001b[32m0.55909\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 512 | loss: 0.55909 - acc: 0.7543 -- iter: 891/891\n",
      "--\n",
      "Training Step: 513  | total loss: \u001b[1m\u001b[32m0.54847\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 513 | loss: 0.54847 - acc: 0.7592 -- iter: 891/891\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 514  | total loss: \u001b[1m\u001b[32m0.58462\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 514 | loss: 0.58462 - acc: 0.7378 -- iter: 891/891\n",
      "--\n",
      "Training Step: 515  | total loss: \u001b[1m\u001b[32m0.57101\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 515 | loss: 0.57101 - acc: 0.7450 -- iter: 891/891\n",
      "--\n",
      "Training Step: 516  | total loss: \u001b[1m\u001b[32m0.60532\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 516 | loss: 0.60532 - acc: 0.7236 -- iter: 891/891\n",
      "--\n",
      "Training Step: 517  | total loss: \u001b[1m\u001b[32m0.59022\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 517 | loss: 0.59022 - acc: 0.7325 -- iter: 891/891\n",
      "--\n",
      "Training Step: 518  | total loss: \u001b[1m\u001b[32m0.57730\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 518 | loss: 0.57730 - acc: 0.7402 -- iter: 891/891\n",
      "--\n",
      "Training Step: 519  | total loss: \u001b[1m\u001b[32m0.56598\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 519 | loss: 0.56598 - acc: 0.7467 -- iter: 891/891\n",
      "--\n",
      "Training Step: 520  | total loss: \u001b[1m\u001b[32m0.55579\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 520 | loss: 0.55579 - acc: 0.7524 -- iter: 891/891\n",
      "--\n",
      "Training Step: 521  | total loss: \u001b[1m\u001b[32m0.54626\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 521 | loss: 0.54626 - acc: 0.7574 -- iter: 891/891\n",
      "--\n",
      "Training Step: 522  | total loss: \u001b[1m\u001b[32m0.53696\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 522 | loss: 0.53696 - acc: 0.7622 -- iter: 891/891\n",
      "--\n",
      "Training Step: 523  | total loss: \u001b[1m\u001b[32m0.52801\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 523 | loss: 0.52801 - acc: 0.7670 -- iter: 891/891\n",
      "--\n",
      "Training Step: 524  | total loss: \u001b[1m\u001b[32m0.51985\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 524 | loss: 0.51985 - acc: 0.7716 -- iter: 891/891\n",
      "--\n",
      "Training Step: 525  | total loss: \u001b[1m\u001b[32m0.51261\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 525 | loss: 0.51261 - acc: 0.7759 -- iter: 891/891\n",
      "--\n",
      "Training Step: 526  | total loss: \u001b[1m\u001b[32m0.50618\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 526 | loss: 0.50618 - acc: 0.7791 -- iter: 891/891\n",
      "--\n",
      "Training Step: 527  | total loss: \u001b[1m\u001b[32m0.50058\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 527 | loss: 0.50058 - acc: 0.7814 -- iter: 891/891\n",
      "--\n",
      "Training Step: 528  | total loss: \u001b[1m\u001b[32m0.49567\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 528 | loss: 0.49567 - acc: 0.7827 -- iter: 891/891\n",
      "--\n",
      "Training Step: 529  | total loss: \u001b[1m\u001b[32m0.49106\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 529 | loss: 0.49106 - acc: 0.7842 -- iter: 891/891\n",
      "--\n",
      "Training Step: 530  | total loss: \u001b[1m\u001b[32m0.54219\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 530 | loss: 0.54219 - acc: 0.7619 -- iter: 891/891\n",
      "--\n",
      "Training Step: 531  | total loss: \u001b[1m\u001b[32m0.53238\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 531 | loss: 0.53238 - acc: 0.7665 -- iter: 891/891\n",
      "--\n",
      "Training Step: 532  | total loss: \u001b[1m\u001b[32m0.57790\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 532 | loss: 0.57790 - acc: 0.7407 -- iter: 891/891\n",
      "--\n",
      "Training Step: 533  | total loss: \u001b[1m\u001b[32m0.56609\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 533 | loss: 0.56609 - acc: 0.7472 -- iter: 891/891\n",
      "--\n",
      "Training Step: 534  | total loss: \u001b[1m\u001b[32m0.59017\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 534 | loss: 0.59017 - acc: 0.7278 -- iter: 891/891\n",
      "--\n",
      "Training Step: 535  | total loss: \u001b[1m\u001b[32m0.58102\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 535 | loss: 0.58102 - acc: 0.7347 -- iter: 891/891\n",
      "--\n",
      "Training Step: 536  | total loss: \u001b[1m\u001b[32m0.59773\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 536 | loss: 0.59773 - acc: 0.7152 -- iter: 891/891\n",
      "--\n",
      "Training Step: 537  | total loss: \u001b[1m\u001b[32m0.59085\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 537 | loss: 0.59085 - acc: 0.7224 -- iter: 891/891\n",
      "--\n",
      "Training Step: 538  | total loss: \u001b[1m\u001b[32m0.60477\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 538 | loss: 0.60477 - acc: 0.7044 -- iter: 891/891\n",
      "--\n",
      "Training Step: 539  | total loss: \u001b[1m\u001b[32m0.59858\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 539 | loss: 0.59858 - acc: 0.7125 -- iter: 891/891\n",
      "--\n",
      "Training Step: 540  | total loss: \u001b[1m\u001b[32m0.60985\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 540 | loss: 0.60985 - acc: 0.6953 -- iter: 891/891\n",
      "--\n",
      "Training Step: 541  | total loss: \u001b[1m\u001b[32m0.60370\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 541 | loss: 0.60370 - acc: 0.7041 -- iter: 891/891\n",
      "--\n",
      "Training Step: 542  | total loss: \u001b[1m\u001b[32m0.59804\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 542 | loss: 0.59804 - acc: 0.7117 -- iter: 891/891\n",
      "--\n",
      "Training Step: 543  | total loss: \u001b[1m\u001b[32m0.59238\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 543 | loss: 0.59238 - acc: 0.7188 -- iter: 891/891\n",
      "--\n",
      "Training Step: 544  | total loss: \u001b[1m\u001b[32m0.58642\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 544 | loss: 0.58642 - acc: 0.7257 -- iter: 891/891\n",
      "--\n",
      "Training Step: 545  | total loss: \u001b[1m\u001b[32m0.57999\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 545 | loss: 0.57999 - acc: 0.7302 -- iter: 891/891\n",
      "--\n",
      "Training Step: 546  | total loss: \u001b[1m\u001b[32m0.57311\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 546 | loss: 0.57311 - acc: 0.7341 -- iter: 891/891\n",
      "--\n",
      "Training Step: 547  | total loss: \u001b[1m\u001b[32m0.56591\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 547 | loss: 0.56591 - acc: 0.7378 -- iter: 891/891\n",
      "--\n",
      "Training Step: 548  | total loss: \u001b[1m\u001b[32m0.58724\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 548 | loss: 0.58724 - acc: 0.7212 -- iter: 891/891\n",
      "--\n",
      "Training Step: 549  | total loss: \u001b[1m\u001b[32m0.57769\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 549 | loss: 0.57769 - acc: 0.7269 -- iter: 891/891\n",
      "--\n",
      "Training Step: 550  | total loss: \u001b[1m\u001b[32m0.60132\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 550 | loss: 0.60132 - acc: 0.7117 -- iter: 891/891\n",
      "--\n",
      "Training Step: 551  | total loss: \u001b[1m\u001b[32m0.59031\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 551 | loss: 0.59031 - acc: 0.7185 -- iter: 891/891\n",
      "--\n",
      "Training Step: 552  | total loss: \u001b[1m\u001b[32m0.61454\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 552 | loss: 0.61454 - acc: 0.7005 -- iter: 891/891\n",
      "--\n",
      "Training Step: 553  | total loss: \u001b[1m\u001b[32m0.60264\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 553 | loss: 0.60264 - acc: 0.7098 -- iter: 891/891\n",
      "--\n",
      "Training Step: 554  | total loss: \u001b[1m\u001b[32m0.61595\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 554 | loss: 0.61595 - acc: 0.6966 -- iter: 891/891\n",
      "--\n",
      "Training Step: 555  | total loss: \u001b[1m\u001b[32m0.60468\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 555 | loss: 0.60468 - acc: 0.7071 -- iter: 891/891\n",
      "--\n",
      "Training Step: 556  | total loss: \u001b[1m\u001b[32m0.61981\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 556 | loss: 0.61981 - acc: 0.6915 -- iter: 891/891\n",
      "--\n",
      "Training Step: 557  | total loss: \u001b[1m\u001b[32m0.60952\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 557 | loss: 0.60952 - acc: 0.7007 -- iter: 891/891\n",
      "--\n",
      "Training Step: 558  | total loss: \u001b[1m\u001b[32m0.62039\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 558 | loss: 0.62039 - acc: 0.6864 -- iter: 891/891\n",
      "--\n",
      "Training Step: 559  | total loss: \u001b[1m\u001b[32m0.61181\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 559 | loss: 0.61181 - acc: 0.6954 -- iter: 891/891\n",
      "--\n",
      "Training Step: 560  | total loss: \u001b[1m\u001b[32m0.60474\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 560 | loss: 0.60474 - acc: 0.7027 -- iter: 891/891\n",
      "--\n",
      "Training Step: 561  | total loss: \u001b[1m\u001b[32m0.59857\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 561 | loss: 0.59857 - acc: 0.7084 -- iter: 891/891\n",
      "--\n",
      "Training Step: 562  | total loss: \u001b[1m\u001b[32m0.60922\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 562 | loss: 0.60922 - acc: 0.6948 -- iter: 891/891\n",
      "--\n",
      "Training Step: 563  | total loss: \u001b[1m\u001b[32m0.60236\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 563 | loss: 0.60236 - acc: 0.7021 -- iter: 891/891\n",
      "--\n",
      "Training Step: 564  | total loss: \u001b[1m\u001b[32m0.59577\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 564 | loss: 0.59577 - acc: 0.7106 -- iter: 891/891\n",
      "--\n",
      "Training Step: 565  | total loss: \u001b[1m\u001b[32m0.58899\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 565 | loss: 0.58899 - acc: 0.7194 -- iter: 891/891\n",
      "--\n",
      "Training Step: 566  | total loss: \u001b[1m\u001b[32m0.60182\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 566 | loss: 0.60182 - acc: 0.7012 -- iter: 891/891\n",
      "--\n",
      "Training Step: 567  | total loss: \u001b[1m\u001b[32m0.59271\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 567 | loss: 0.59271 - acc: 0.7117 -- iter: 891/891\n",
      "--\n",
      "Training Step: 568  | total loss: \u001b[1m\u001b[32m0.60835\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 568 | loss: 0.60835 - acc: 0.6952 -- iter: 891/891\n",
      "--\n",
      "Training Step: 569  | total loss: \u001b[1m\u001b[32m0.59757\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 569 | loss: 0.59757 - acc: 0.7071 -- iter: 891/891\n",
      "--\n",
      "Training Step: 570  | total loss: \u001b[1m\u001b[32m0.58735\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 570 | loss: 0.58735 - acc: 0.7177 -- iter: 891/891\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 571  | total loss: \u001b[1m\u001b[32m0.57735\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 571 | loss: 0.57735 - acc: 0.7276 -- iter: 891/891\n",
      "--\n",
      "Training Step: 572  | total loss: \u001b[1m\u001b[32m0.59467\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 572 | loss: 0.59467 - acc: 0.7113 -- iter: 891/891\n",
      "--\n",
      "Training Step: 573  | total loss: \u001b[1m\u001b[32m0.58280\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 573 | loss: 0.58280 - acc: 0.7218 -- iter: 891/891\n",
      "--\n",
      "Training Step: 574  | total loss: \u001b[1m\u001b[32m0.57173\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 574 | loss: 0.57173 - acc: 0.7311 -- iter: 891/891\n",
      "--\n",
      "Training Step: 575  | total loss: \u001b[1m\u001b[32m0.56125\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 575 | loss: 0.56125 - acc: 0.7396 -- iter: 891/891\n",
      "--\n",
      "Training Step: 576  | total loss: \u001b[1m\u001b[32m0.58976\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 576 | loss: 0.58976 - acc: 0.7201 -- iter: 891/891\n",
      "--\n",
      "Training Step: 577  | total loss: \u001b[1m\u001b[32m0.57687\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 577 | loss: 0.57687 - acc: 0.7297 -- iter: 891/891\n",
      "--\n",
      "Training Step: 578  | total loss: \u001b[1m\u001b[32m0.60174\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 578 | loss: 0.60174 - acc: 0.7111 -- iter: 891/891\n",
      "--\n",
      "Training Step: 579  | total loss: \u001b[1m\u001b[32m0.58798\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 579 | loss: 0.58798 - acc: 0.7210 -- iter: 891/891\n",
      "--\n",
      "Training Step: 580  | total loss: \u001b[1m\u001b[32m0.57594\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 580 | loss: 0.57594 - acc: 0.7299 -- iter: 891/891\n",
      "--\n",
      "Training Step: 581  | total loss: \u001b[1m\u001b[32m0.56520\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 581 | loss: 0.56520 - acc: 0.7381 -- iter: 891/891\n",
      "--\n",
      "Training Step: 582  | total loss: \u001b[1m\u001b[32m0.59079\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 582 | loss: 0.59079 - acc: 0.7164 -- iter: 891/891\n",
      "--\n",
      "Training Step: 583  | total loss: \u001b[1m\u001b[32m0.57901\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 583 | loss: 0.57901 - acc: 0.7262 -- iter: 891/891\n",
      "--\n",
      "Training Step: 584  | total loss: \u001b[1m\u001b[32m0.56864\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 584 | loss: 0.56864 - acc: 0.7349 -- iter: 891/891\n",
      "--\n",
      "Training Step: 585  | total loss: \u001b[1m\u001b[32m0.55901\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 585 | loss: 0.55901 - acc: 0.7427 -- iter: 891/891\n",
      "--\n",
      "Training Step: 586  | total loss: \u001b[1m\u001b[32m0.54968\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 586 | loss: 0.54968 - acc: 0.7496 -- iter: 891/891\n",
      "--\n",
      "Training Step: 587  | total loss: \u001b[1m\u001b[32m0.54053\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 587 | loss: 0.54053 - acc: 0.7555 -- iter: 891/891\n",
      "--\n",
      "Training Step: 588  | total loss: \u001b[1m\u001b[32m0.53172\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 588 | loss: 0.53172 - acc: 0.7612 -- iter: 891/891\n",
      "--\n",
      "Training Step: 589  | total loss: \u001b[1m\u001b[32m0.52353\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 589 | loss: 0.52353 - acc: 0.7664 -- iter: 891/891\n",
      "--\n",
      "Training Step: 590  | total loss: \u001b[1m\u001b[32m0.56305\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 590 | loss: 0.56305 - acc: 0.7448 -- iter: 891/891\n",
      "--\n",
      "Training Step: 591  | total loss: \u001b[1m\u001b[32m0.55175\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 591 | loss: 0.55175 - acc: 0.7515 -- iter: 891/891\n",
      "--\n",
      "Training Step: 592  | total loss: \u001b[1m\u001b[32m0.54151\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 592 | loss: 0.54151 - acc: 0.7573 -- iter: 891/891\n",
      "--\n",
      "Training Step: 593  | total loss: \u001b[1m\u001b[32m0.53220\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 593 | loss: 0.53220 - acc: 0.7621 -- iter: 891/891\n",
      "--\n",
      "Training Step: 594  | total loss: \u001b[1m\u001b[32m0.52371\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 594 | loss: 0.52371 - acc: 0.7664 -- iter: 891/891\n",
      "--\n",
      "Training Step: 595  | total loss: \u001b[1m\u001b[32m0.51593\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 595 | loss: 0.51593 - acc: 0.7704 -- iter: 891/891\n",
      "--\n",
      "Training Step: 596  | total loss: \u001b[1m\u001b[32m0.50886\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 596 | loss: 0.50886 - acc: 0.7742 -- iter: 891/891\n",
      "--\n",
      "Training Step: 597  | total loss: \u001b[1m\u001b[32m0.50247\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 597 | loss: 0.50247 - acc: 0.7777 -- iter: 891/891\n",
      "--\n",
      "Training Step: 598  | total loss: \u001b[1m\u001b[32m0.53993\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 598 | loss: 0.53993 - acc: 0.7574 -- iter: 891/891\n",
      "--\n",
      "Training Step: 599  | total loss: \u001b[1m\u001b[32m0.53081\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 599 | loss: 0.53081 - acc: 0.7620 -- iter: 891/891\n",
      "--\n",
      "Training Step: 600  | total loss: \u001b[1m\u001b[32m0.52306\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 600 | loss: 0.52306 - acc: 0.7662 -- iter: 891/891\n",
      "--\n",
      "Training Step: 601  | total loss: \u001b[1m\u001b[32m0.51627\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 601 | loss: 0.51627 - acc: 0.7699 -- iter: 891/891\n",
      "--\n",
      "Training Step: 602  | total loss: \u001b[1m\u001b[32m0.55518\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 602 | loss: 0.55518 - acc: 0.7452 -- iter: 891/891\n",
      "--\n",
      "Training Step: 603  | total loss: \u001b[1m\u001b[32m0.54558\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 603 | loss: 0.54558 - acc: 0.7511 -- iter: 891/891\n",
      "--\n",
      "Training Step: 604  | total loss: \u001b[1m\u001b[32m0.53712\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 604 | loss: 0.53712 - acc: 0.7562 -- iter: 891/891\n",
      "--\n",
      "Training Step: 605  | total loss: \u001b[1m\u001b[32m0.52931\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 605 | loss: 0.52931 - acc: 0.7611 -- iter: 891/891\n",
      "--\n",
      "Training Step: 606  | total loss: \u001b[1m\u001b[32m0.52191\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 606 | loss: 0.52191 - acc: 0.7654 -- iter: 891/891\n",
      "--\n",
      "Training Step: 607  | total loss: \u001b[1m\u001b[32m0.51489\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 607 | loss: 0.51489 - acc: 0.7700 -- iter: 891/891\n",
      "--\n",
      "Training Step: 608  | total loss: \u001b[1m\u001b[32m0.50838\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 608 | loss: 0.50838 - acc: 0.7741 -- iter: 891/891\n",
      "--\n",
      "Training Step: 609  | total loss: \u001b[1m\u001b[32m0.50248\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 609 | loss: 0.50248 - acc: 0.7771 -- iter: 891/891\n",
      "--\n",
      "Training Step: 610  | total loss: \u001b[1m\u001b[32m0.54618\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 610 | loss: 0.54618 - acc: 0.7543 -- iter: 891/891\n",
      "--\n",
      "Training Step: 611  | total loss: \u001b[1m\u001b[32m0.53659\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 611 | loss: 0.53659 - acc: 0.7593 -- iter: 891/891\n",
      "--\n",
      "Training Step: 612  | total loss: \u001b[1m\u001b[32m0.57367\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 612 | loss: 0.57367 - acc: 0.7387 -- iter: 891/891\n",
      "--\n",
      "Training Step: 613  | total loss: \u001b[1m\u001b[32m0.56140\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 613 | loss: 0.56140 - acc: 0.7456 -- iter: 891/891\n",
      "--\n",
      "Training Step: 614  | total loss: \u001b[1m\u001b[32m0.59193\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 614 | loss: 0.59193 - acc: 0.7250 -- iter: 891/891\n",
      "--\n",
      "Training Step: 615  | total loss: \u001b[1m\u001b[32m0.57904\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 615 | loss: 0.57904 - acc: 0.7333 -- iter: 891/891\n",
      "--\n",
      "Training Step: 616  | total loss: \u001b[1m\u001b[32m0.59848\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 616 | loss: 0.59848 - acc: 0.7144 -- iter: 891/891\n",
      "--\n",
      "Training Step: 617  | total loss: \u001b[1m\u001b[32m0.58781\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 617 | loss: 0.58781 - acc: 0.7237 -- iter: 891/891\n",
      "--\n",
      "Training Step: 618  | total loss: \u001b[1m\u001b[32m0.57939\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 618 | loss: 0.57939 - acc: 0.7315 -- iter: 891/891\n",
      "--\n",
      "Training Step: 619  | total loss: \u001b[1m\u001b[32m0.57218\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 619 | loss: 0.57218 - acc: 0.7386 -- iter: 891/891\n",
      "--\n",
      "Training Step: 620  | total loss: \u001b[1m\u001b[32m0.56535\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 620 | loss: 0.56535 - acc: 0.7453 -- iter: 891/891\n",
      "--\n",
      "Training Step: 621  | total loss: \u001b[1m\u001b[32m0.55830\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 621 | loss: 0.55830 - acc: 0.7518 -- iter: 891/891\n",
      "--\n",
      "Training Step: 622  | total loss: \u001b[1m\u001b[32m0.55078\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 622 | loss: 0.55078 - acc: 0.7577 -- iter: 891/891\n",
      "--\n",
      "Training Step: 623  | total loss: \u001b[1m\u001b[32m0.54288\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 623 | loss: 0.54288 - acc: 0.7633 -- iter: 891/891\n",
      "--\n",
      "Training Step: 624  | total loss: \u001b[1m\u001b[32m0.53493\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 624 | loss: 0.53493 - acc: 0.7686 -- iter: 891/891\n",
      "--\n",
      "Training Step: 625  | total loss: \u001b[1m\u001b[32m0.52734\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 625 | loss: 0.52734 - acc: 0.7731 -- iter: 891/891\n",
      "--\n",
      "Training Step: 626  | total loss: \u001b[1m\u001b[32m0.52042\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 626 | loss: 0.52042 - acc: 0.7769 -- iter: 891/891\n",
      "--\n",
      "Training Step: 627  | total loss: \u001b[1m\u001b[32m0.51438\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 627 | loss: 0.51438 - acc: 0.7800 -- iter: 891/891\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 628  | total loss: \u001b[1m\u001b[32m0.56304\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 628 | loss: 0.56304 - acc: 0.7563 -- iter: 891/891\n",
      "--\n",
      "Training Step: 629  | total loss: \u001b[1m\u001b[32m0.55303\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 629 | loss: 0.55303 - acc: 0.7597 -- iter: 891/891\n",
      "--\n",
      "Training Step: 630  | total loss: \u001b[1m\u001b[32m0.59606\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 630 | loss: 0.59606 - acc: 0.7378 -- iter: 891/891\n",
      "--\n",
      "Training Step: 631  | total loss: \u001b[1m\u001b[32m0.58177\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 631 | loss: 0.58177 - acc: 0.7446 -- iter: 891/891\n",
      "--\n",
      "Training Step: 632  | total loss: \u001b[1m\u001b[32m0.61961\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 632 | loss: 0.61961 - acc: 0.7220 -- iter: 891/891\n",
      "--\n",
      "Training Step: 633  | total loss: \u001b[1m\u001b[32m0.60287\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 633 | loss: 0.60287 - acc: 0.7311 -- iter: 891/891\n",
      "--\n",
      "Training Step: 634  | total loss: \u001b[1m\u001b[32m0.58886\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 634 | loss: 0.58886 - acc: 0.7390 -- iter: 891/891\n",
      "--\n",
      "Training Step: 635  | total loss: \u001b[1m\u001b[32m0.57721\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 635 | loss: 0.57721 - acc: 0.7464 -- iter: 891/891\n",
      "--\n",
      "Training Step: 636  | total loss: \u001b[1m\u001b[32m0.56719\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 636 | loss: 0.56719 - acc: 0.7530 -- iter: 891/891\n",
      "--\n",
      "Training Step: 637  | total loss: \u001b[1m\u001b[32m0.55814\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 637 | loss: 0.55814 - acc: 0.7588 -- iter: 891/891\n",
      "--\n",
      "Training Step: 638  | total loss: \u001b[1m\u001b[32m0.54953\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 638 | loss: 0.54953 - acc: 0.7633 -- iter: 891/891\n",
      "--\n",
      "Training Step: 639  | total loss: \u001b[1m\u001b[32m0.54094\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 639 | loss: 0.54094 - acc: 0.7676 -- iter: 891/891\n",
      "--\n",
      "Training Step: 640  | total loss: \u001b[1m\u001b[32m0.53233\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 640 | loss: 0.53233 - acc: 0.7721 -- iter: 891/891\n",
      "--\n",
      "Training Step: 641  | total loss: \u001b[1m\u001b[32m0.52411\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 641 | loss: 0.52411 - acc: 0.7763 -- iter: 891/891\n",
      "--\n",
      "Training Step: 642  | total loss: \u001b[1m\u001b[32m0.51665\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 642 | loss: 0.51665 - acc: 0.7800 -- iter: 891/891\n",
      "--\n",
      "Training Step: 643  | total loss: \u001b[1m\u001b[32m0.51001\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 643 | loss: 0.51001 - acc: 0.7831 -- iter: 891/891\n",
      "--\n",
      "Training Step: 644  | total loss: \u001b[1m\u001b[32m0.50417\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 644 | loss: 0.50417 - acc: 0.7858 -- iter: 891/891\n",
      "--\n",
      "Training Step: 645  | total loss: \u001b[1m\u001b[32m0.49914\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 645 | loss: 0.49914 - acc: 0.7869 -- iter: 891/891\n",
      "--\n",
      "Training Step: 646  | total loss: \u001b[1m\u001b[32m0.49470\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 646 | loss: 0.49470 - acc: 0.7876 -- iter: 891/891\n",
      "--\n",
      "Training Step: 647  | total loss: \u001b[1m\u001b[32m0.49045\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 647 | loss: 0.49045 - acc: 0.7886 -- iter: 891/891\n",
      "--\n",
      "Training Step: 648  | total loss: \u001b[1m\u001b[32m0.48623\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 648 | loss: 0.48623 - acc: 0.7905 -- iter: 891/891\n",
      "--\n",
      "Training Step: 649  | total loss: \u001b[1m\u001b[32m0.48217\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 649 | loss: 0.48217 - acc: 0.7924 -- iter: 891/891\n",
      "--\n",
      "Training Step: 650  | total loss: \u001b[1m\u001b[32m0.47832\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 650 | loss: 0.47832 - acc: 0.7943 -- iter: 891/891\n",
      "--\n",
      "Training Step: 651  | total loss: \u001b[1m\u001b[32m0.47482\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 651 | loss: 0.47482 - acc: 0.7956 -- iter: 891/891\n",
      "--\n",
      "Training Step: 652  | total loss: \u001b[1m\u001b[32m0.47193\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 652 | loss: 0.47193 - acc: 0.7962 -- iter: 891/891\n",
      "--\n",
      "Training Step: 653  | total loss: \u001b[1m\u001b[32m0.46956\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 653 | loss: 0.46956 - acc: 0.7966 -- iter: 891/891\n",
      "--\n",
      "Training Step: 654  | total loss: \u001b[1m\u001b[32m0.51936\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 654 | loss: 0.51936 - acc: 0.7691 -- iter: 891/891\n",
      "--\n",
      "Training Step: 655  | total loss: \u001b[1m\u001b[32m0.51271\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 655 | loss: 0.51271 - acc: 0.7726 -- iter: 891/891\n",
      "--\n",
      "Training Step: 656  | total loss: \u001b[1m\u001b[32m0.50690\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 656 | loss: 0.50690 - acc: 0.7756 -- iter: 891/891\n",
      "--\n",
      "Training Step: 657  | total loss: \u001b[1m\u001b[32m0.50143\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 657 | loss: 0.50143 - acc: 0.7782 -- iter: 891/891\n",
      "--\n",
      "Training Step: 658  | total loss: \u001b[1m\u001b[32m0.49620\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 658 | loss: 0.49620 - acc: 0.7808 -- iter: 891/891\n",
      "--\n",
      "Training Step: 659  | total loss: \u001b[1m\u001b[32m0.49127\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 659 | loss: 0.49127 - acc: 0.7832 -- iter: 891/891\n",
      "--\n",
      "Training Step: 660  | total loss: \u001b[1m\u001b[32m0.48674\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 660 | loss: 0.48674 - acc: 0.7851 -- iter: 891/891\n",
      "--\n",
      "Training Step: 661  | total loss: \u001b[1m\u001b[32m0.48277\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 661 | loss: 0.48277 - acc: 0.7871 -- iter: 891/891\n",
      "--\n",
      "Training Step: 662  | total loss: \u001b[1m\u001b[32m0.47936\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 662 | loss: 0.47936 - acc: 0.7889 -- iter: 891/891\n",
      "--\n",
      "Training Step: 663  | total loss: \u001b[1m\u001b[32m0.47641\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 663 | loss: 0.47641 - acc: 0.7907 -- iter: 891/891\n",
      "--\n",
      "Training Step: 664  | total loss: \u001b[1m\u001b[32m0.47377\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 664 | loss: 0.47377 - acc: 0.7921 -- iter: 891/891\n",
      "--\n",
      "Training Step: 665  | total loss: \u001b[1m\u001b[32m0.47137\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 665 | loss: 0.47137 - acc: 0.7932 -- iter: 891/891\n",
      "--\n",
      "Training Step: 666  | total loss: \u001b[1m\u001b[32m0.46910\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 666 | loss: 0.46910 - acc: 0.7941 -- iter: 891/891\n",
      "--\n",
      "Training Step: 667  | total loss: \u001b[1m\u001b[32m0.46685\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 667 | loss: 0.46685 - acc: 0.7949 -- iter: 891/891\n",
      "--\n",
      "Training Step: 668  | total loss: \u001b[1m\u001b[32m0.46460\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 668 | loss: 0.46460 - acc: 0.7963 -- iter: 891/891\n",
      "--\n",
      "Training Step: 669  | total loss: \u001b[1m\u001b[32m0.46244\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 669 | loss: 0.46244 - acc: 0.7977 -- iter: 891/891\n",
      "--\n",
      "Training Step: 670  | total loss: \u001b[1m\u001b[32m0.51199\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 670 | loss: 0.51199 - acc: 0.7712 -- iter: 891/891\n",
      "--\n",
      "Training Step: 671  | total loss: \u001b[1m\u001b[32m0.50540\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 671 | loss: 0.50540 - acc: 0.7748 -- iter: 891/891\n",
      "--\n",
      "Training Step: 672  | total loss: \u001b[1m\u001b[32m0.50009\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 672 | loss: 0.50009 - acc: 0.7775 -- iter: 891/891\n",
      "--\n",
      "Training Step: 673  | total loss: \u001b[1m\u001b[32m0.49574\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 673 | loss: 0.49574 - acc: 0.7799 -- iter: 891/891\n",
      "--\n",
      "Training Step: 674  | total loss: \u001b[1m\u001b[32m0.49178\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 674 | loss: 0.49178 - acc: 0.7820 -- iter: 891/891\n",
      "--\n",
      "Training Step: 675  | total loss: \u001b[1m\u001b[32m0.48776\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 675 | loss: 0.48776 - acc: 0.7840 -- iter: 891/891\n",
      "--\n",
      "Training Step: 676  | total loss: \u001b[1m\u001b[32m0.48360\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 676 | loss: 0.48360 - acc: 0.7864 -- iter: 891/891\n",
      "--\n",
      "Training Step: 677  | total loss: \u001b[1m\u001b[32m0.47957\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 677 | loss: 0.47957 - acc: 0.7888 -- iter: 891/891\n",
      "--\n",
      "Training Step: 678  | total loss: \u001b[1m\u001b[32m0.47593\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 678 | loss: 0.47593 - acc: 0.7910 -- iter: 891/891\n",
      "--\n",
      "Training Step: 679  | total loss: \u001b[1m\u001b[32m0.47279\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 679 | loss: 0.47279 - acc: 0.7927 -- iter: 891/891\n",
      "--\n",
      "Training Step: 680  | total loss: \u001b[1m\u001b[32m0.52812\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 680 | loss: 0.52812 - acc: 0.7655 -- iter: 891/891\n",
      "--\n",
      "Training Step: 681  | total loss: \u001b[1m\u001b[32m0.51990\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 681 | loss: 0.51990 - acc: 0.7688 -- iter: 891/891\n",
      "--\n",
      "Training Step: 682  | total loss: \u001b[1m\u001b[32m0.51239\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 682 | loss: 0.51239 - acc: 0.7723 -- iter: 891/891\n",
      "--\n",
      "Training Step: 683  | total loss: \u001b[1m\u001b[32m0.50551\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 683 | loss: 0.50551 - acc: 0.7759 -- iter: 891/891\n",
      "--\n",
      "Training Step: 684  | total loss: \u001b[1m\u001b[32m0.49929\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 684 | loss: 0.49929 - acc: 0.7796 -- iter: 891/891\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 685  | total loss: \u001b[1m\u001b[32m0.49375\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 685 | loss: 0.49375 - acc: 0.7829 -- iter: 891/891\n",
      "--\n",
      "Training Step: 686  | total loss: \u001b[1m\u001b[32m0.53162\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 686 | loss: 0.53162 - acc: 0.7607 -- iter: 891/891\n",
      "--\n",
      "Training Step: 687  | total loss: \u001b[1m\u001b[32m0.52349\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 687 | loss: 0.52349 - acc: 0.7653 -- iter: 891/891\n",
      "--\n",
      "Training Step: 688  | total loss: \u001b[1m\u001b[32m0.55492\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 688 | loss: 0.55492 - acc: 0.7423 -- iter: 891/891\n",
      "--\n",
      "Training Step: 689  | total loss: \u001b[1m\u001b[32m0.54697\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 689 | loss: 0.54697 - acc: 0.7480 -- iter: 891/891\n",
      "--\n",
      "Training Step: 690  | total loss: \u001b[1m\u001b[32m0.56747\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 690 | loss: 0.56747 - acc: 0.7298 -- iter: 891/891\n",
      "--\n",
      "Training Step: 691  | total loss: \u001b[1m\u001b[32m0.56103\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 691 | loss: 0.56103 - acc: 0.7365 -- iter: 891/891\n",
      "--\n",
      "Training Step: 692  | total loss: \u001b[1m\u001b[32m0.57923\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 692 | loss: 0.57923 - acc: 0.7169 -- iter: 891/891\n",
      "--\n",
      "Training Step: 693  | total loss: \u001b[1m\u001b[32m0.57334\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 693 | loss: 0.57334 - acc: 0.7245 -- iter: 891/891\n",
      "--\n",
      "Training Step: 694  | total loss: \u001b[1m\u001b[32m0.59048\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 694 | loss: 0.59048 - acc: 0.7052 -- iter: 891/891\n",
      "--\n",
      "Training Step: 695  | total loss: \u001b[1m\u001b[32m0.58436\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 695 | loss: 0.58436 - acc: 0.7147 -- iter: 891/891\n",
      "--\n",
      "Training Step: 696  | total loss: \u001b[1m\u001b[32m0.59425\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 696 | loss: 0.59425 - acc: 0.7021 -- iter: 891/891\n",
      "--\n",
      "Training Step: 697  | total loss: \u001b[1m\u001b[32m0.58817\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 697 | loss: 0.58817 - acc: 0.7121 -- iter: 891/891\n",
      "--\n",
      "Training Step: 698  | total loss: \u001b[1m\u001b[32m0.60088\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 698 | loss: 0.60088 - acc: 0.6947 -- iter: 891/891\n",
      "--\n",
      "Training Step: 699  | total loss: \u001b[1m\u001b[32m0.59413\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 699 | loss: 0.59413 - acc: 0.7053 -- iter: 891/891\n",
      "--\n",
      "Training Step: 700  | total loss: \u001b[1m\u001b[32m0.58779\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 700 | loss: 0.58779 - acc: 0.7145 -- iter: 891/891\n",
      "--\n",
      "Training Step: 701  | total loss: \u001b[1m\u001b[32m0.58148\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 701 | loss: 0.58148 - acc: 0.7220 -- iter: 891/891\n",
      "--\n",
      "Training Step: 702  | total loss: \u001b[1m\u001b[32m0.59513\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 702 | loss: 0.59513 - acc: 0.7062 -- iter: 891/891\n",
      "--\n",
      "Training Step: 703  | total loss: \u001b[1m\u001b[32m0.58698\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 703 | loss: 0.58698 - acc: 0.7123 -- iter: 891/891\n",
      "--\n",
      "Training Step: 704  | total loss: \u001b[1m\u001b[32m0.57913\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 704 | loss: 0.57913 - acc: 0.7182 -- iter: 891/891\n",
      "--\n",
      "Training Step: 705  | total loss: \u001b[1m\u001b[32m0.57141\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 705 | loss: 0.57141 - acc: 0.7237 -- iter: 891/891\n",
      "--\n",
      "Training Step: 706  | total loss: \u001b[1m\u001b[32m0.58943\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 706 | loss: 0.58943 - acc: 0.7090 -- iter: 891/891\n",
      "--\n",
      "Training Step: 707  | total loss: \u001b[1m\u001b[32m0.57978\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 707 | loss: 0.57978 - acc: 0.7157 -- iter: 891/891\n",
      "--\n",
      "Training Step: 708  | total loss: \u001b[1m\u001b[32m0.57077\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 708 | loss: 0.57077 - acc: 0.7221 -- iter: 891/891\n",
      "--\n",
      "Training Step: 709  | total loss: \u001b[1m\u001b[32m0.56226\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 709 | loss: 0.56226 - acc: 0.7287 -- iter: 891/891\n",
      "--\n",
      "Training Step: 710  | total loss: \u001b[1m\u001b[32m0.55412\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 710 | loss: 0.55412 - acc: 0.7351 -- iter: 891/891\n",
      "--\n",
      "Training Step: 711  | total loss: \u001b[1m\u001b[32m0.54629\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 711 | loss: 0.54629 - acc: 0.7406 -- iter: 891/891\n",
      "--\n",
      "Training Step: 712  | total loss: \u001b[1m\u001b[32m0.53874\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 712 | loss: 0.53874 - acc: 0.7470 -- iter: 891/891\n",
      "--\n",
      "Training Step: 713  | total loss: \u001b[1m\u001b[32m0.53152\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 713 | loss: 0.53152 - acc: 0.7526 -- iter: 891/891\n",
      "--\n",
      "Training Step: 714  | total loss: \u001b[1m\u001b[32m0.52462\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 714 | loss: 0.52462 - acc: 0.7578 -- iter: 891/891\n",
      "--\n",
      "Training Step: 715  | total loss: \u001b[1m\u001b[32m0.51806\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 715 | loss: 0.51806 - acc: 0.7631 -- iter: 891/891\n",
      "--\n",
      "Training Step: 716  | total loss: \u001b[1m\u001b[32m0.51179\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 716 | loss: 0.51179 - acc: 0.7682 -- iter: 891/891\n",
      "--\n",
      "Training Step: 717  | total loss: \u001b[1m\u001b[32m0.50582\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 717 | loss: 0.50582 - acc: 0.7721 -- iter: 891/891\n",
      "--\n",
      "Training Step: 718  | total loss: \u001b[1m\u001b[32m0.50013\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 718 | loss: 0.50013 - acc: 0.7755 -- iter: 891/891\n",
      "--\n",
      "Training Step: 719  | total loss: \u001b[1m\u001b[32m0.49477\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 719 | loss: 0.49477 - acc: 0.7789 -- iter: 891/891\n",
      "--\n",
      "Training Step: 720  | total loss: \u001b[1m\u001b[32m0.48989\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 720 | loss: 0.48989 - acc: 0.7819 -- iter: 891/891\n",
      "--\n",
      "Training Step: 721  | total loss: \u001b[1m\u001b[32m0.48568\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 721 | loss: 0.48568 - acc: 0.7846 -- iter: 891/891\n",
      "--\n",
      "Training Step: 722  | total loss: \u001b[1m\u001b[32m0.53521\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 722 | loss: 0.53521 - acc: 0.7606 -- iter: 891/891\n",
      "--\n",
      "Training Step: 723  | total loss: \u001b[1m\u001b[32m0.52758\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 723 | loss: 0.52758 - acc: 0.7654 -- iter: 891/891\n",
      "--\n",
      "Training Step: 724  | total loss: \u001b[1m\u001b[32m0.52111\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 724 | loss: 0.52111 - acc: 0.7691 -- iter: 891/891\n",
      "--\n",
      "Training Step: 725  | total loss: \u001b[1m\u001b[32m0.51496\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 725 | loss: 0.51496 - acc: 0.7726 -- iter: 891/891\n",
      "--\n",
      "Training Step: 726  | total loss: \u001b[1m\u001b[32m0.50861\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 726 | loss: 0.50861 - acc: 0.7758 -- iter: 891/891\n",
      "--\n",
      "Training Step: 727  | total loss: \u001b[1m\u001b[32m0.50221\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 727 | loss: 0.50221 - acc: 0.7783 -- iter: 891/891\n",
      "--\n",
      "Training Step: 728  | total loss: \u001b[1m\u001b[32m0.49627\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 728 | loss: 0.49627 - acc: 0.7817 -- iter: 891/891\n",
      "--\n",
      "Training Step: 729  | total loss: \u001b[1m\u001b[32m0.49112\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 729 | loss: 0.49112 - acc: 0.7840 -- iter: 891/891\n",
      "--\n",
      "Training Step: 730  | total loss: \u001b[1m\u001b[32m0.54150\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 730 | loss: 0.54150 - acc: 0.7591 -- iter: 891/891\n",
      "--\n",
      "Training Step: 731  | total loss: \u001b[1m\u001b[32m0.53210\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 731 | loss: 0.53210 - acc: 0.7637 -- iter: 891/891\n",
      "--\n",
      "Training Step: 732  | total loss: \u001b[1m\u001b[32m0.52373\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 732 | loss: 0.52373 - acc: 0.7675 -- iter: 891/891\n",
      "--\n",
      "Training Step: 733  | total loss: \u001b[1m\u001b[32m0.51626\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 733 | loss: 0.51626 - acc: 0.7709 -- iter: 891/891\n",
      "--\n",
      "Training Step: 734  | total loss: \u001b[1m\u001b[32m0.55743\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 734 | loss: 0.55743 - acc: 0.7452 -- iter: 891/891\n",
      "--\n",
      "Training Step: 735  | total loss: \u001b[1m\u001b[32m0.54708\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 735 | loss: 0.54708 - acc: 0.7512 -- iter: 891/891\n",
      "--\n",
      "Training Step: 736  | total loss: \u001b[1m\u001b[32m0.57555\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 736 | loss: 0.57555 - acc: 0.7292 -- iter: 891/891\n",
      "--\n",
      "Training Step: 737  | total loss: \u001b[1m\u001b[32m0.56564\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 737 | loss: 0.56564 - acc: 0.7370 -- iter: 891/891\n",
      "--\n",
      "Training Step: 738  | total loss: \u001b[1m\u001b[32m0.58736\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 738 | loss: 0.58736 - acc: 0.7156 -- iter: 891/891\n",
      "--\n",
      "Training Step: 739  | total loss: \u001b[1m\u001b[32m0.57971\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 739 | loss: 0.57971 - acc: 0.7240 -- iter: 891/891\n",
      "--\n",
      "Training Step: 740  | total loss: \u001b[1m\u001b[32m0.59275\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 740 | loss: 0.59275 - acc: 0.7080 -- iter: 891/891\n",
      "--\n",
      "Training Step: 741  | total loss: \u001b[1m\u001b[32m0.58713\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 741 | loss: 0.58713 - acc: 0.7164 -- iter: 891/891\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 742  | total loss: \u001b[1m\u001b[32m0.58247\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 742 | loss: 0.58247 - acc: 0.7237 -- iter: 891/891\n",
      "--\n",
      "Training Step: 743  | total loss: \u001b[1m\u001b[32m0.57792\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 743 | loss: 0.57792 - acc: 0.7305 -- iter: 891/891\n",
      "--\n",
      "Training Step: 744  | total loss: \u001b[1m\u001b[32m0.59030\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 744 | loss: 0.59030 - acc: 0.7142 -- iter: 891/891\n",
      "--\n",
      "Training Step: 745  | total loss: \u001b[1m\u001b[32m0.58358\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 745 | loss: 0.58358 - acc: 0.7222 -- iter: 891/891\n",
      "--\n",
      "Training Step: 746  | total loss: \u001b[1m\u001b[32m0.57664\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 746 | loss: 0.57664 - acc: 0.7297 -- iter: 891/891\n",
      "--\n",
      "Training Step: 747  | total loss: \u001b[1m\u001b[32m0.56926\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 747 | loss: 0.56926 - acc: 0.7366 -- iter: 891/891\n",
      "--\n",
      "Training Step: 748  | total loss: \u001b[1m\u001b[32m0.56144\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 748 | loss: 0.56144 - acc: 0.7426 -- iter: 891/891\n",
      "--\n",
      "Training Step: 749  | total loss: \u001b[1m\u001b[32m0.55339\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 749 | loss: 0.55339 - acc: 0.7481 -- iter: 891/891\n",
      "--\n",
      "Training Step: 750  | total loss: \u001b[1m\u001b[32m0.54540\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 750 | loss: 0.54540 - acc: 0.7536 -- iter: 891/891\n",
      "--\n",
      "Training Step: 751  | total loss: \u001b[1m\u001b[32m0.53783\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 751 | loss: 0.53783 - acc: 0.7586 -- iter: 891/891\n",
      "--\n",
      "Training Step: 752  | total loss: \u001b[1m\u001b[32m0.53094\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 752 | loss: 0.53094 - acc: 0.7630 -- iter: 891/891\n",
      "--\n",
      "Training Step: 753  | total loss: \u001b[1m\u001b[32m0.52491\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 753 | loss: 0.52491 - acc: 0.7662 -- iter: 891/891\n",
      "--\n",
      "Training Step: 754  | total loss: \u001b[1m\u001b[32m0.51968\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 754 | loss: 0.51968 - acc: 0.7695 -- iter: 891/891\n",
      "--\n",
      "Training Step: 755  | total loss: \u001b[1m\u001b[32m0.51503\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 755 | loss: 0.51503 - acc: 0.7724 -- iter: 891/891\n",
      "--\n",
      "Training Step: 756  | total loss: \u001b[1m\u001b[32m0.51059\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 756 | loss: 0.51059 - acc: 0.7757 -- iter: 891/891\n",
      "--\n",
      "Training Step: 757  | total loss: \u001b[1m\u001b[32m0.50602\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 757 | loss: 0.50602 - acc: 0.7786 -- iter: 891/891\n",
      "--\n",
      "Training Step: 758  | total loss: \u001b[1m\u001b[32m0.50113\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 758 | loss: 0.50113 - acc: 0.7812 -- iter: 891/891\n",
      "--\n",
      "Training Step: 759  | total loss: \u001b[1m\u001b[32m0.49597\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 759 | loss: 0.49597 - acc: 0.7842 -- iter: 891/891\n",
      "--\n",
      "Training Step: 760  | total loss: \u001b[1m\u001b[32m0.54452\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 760 | loss: 0.54452 - acc: 0.7572 -- iter: 891/891\n",
      "--\n",
      "Training Step: 761  | total loss: \u001b[1m\u001b[32m0.53495\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 761 | loss: 0.53495 - acc: 0.7625 -- iter: 891/891\n",
      "--\n",
      "Training Step: 762  | total loss: \u001b[1m\u001b[32m0.52784\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 762 | loss: 0.52784 - acc: 0.7667 -- iter: 891/891\n",
      "--\n",
      "Training Step: 763  | total loss: \u001b[1m\u001b[32m0.52253\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 763 | loss: 0.52253 - acc: 0.7705 -- iter: 891/891\n",
      "--\n",
      "Training Step: 764  | total loss: \u001b[1m\u001b[32m0.51773\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 764 | loss: 0.51773 - acc: 0.7738 -- iter: 891/891\n",
      "--\n",
      "Training Step: 765  | total loss: \u001b[1m\u001b[32m0.51241\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 765 | loss: 0.51241 - acc: 0.7771 -- iter: 891/891\n",
      "--\n",
      "Training Step: 766  | total loss: \u001b[1m\u001b[32m0.50643\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 766 | loss: 0.50643 - acc: 0.7800 -- iter: 891/891\n",
      "--\n",
      "Training Step: 767  | total loss: \u001b[1m\u001b[32m0.50035\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 767 | loss: 0.50035 - acc: 0.7834 -- iter: 891/891\n",
      "--\n",
      "Training Step: 768  | total loss: \u001b[1m\u001b[32m0.49481\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 768 | loss: 0.49481 - acc: 0.7862 -- iter: 891/891\n",
      "--\n",
      "Training Step: 769  | total loss: \u001b[1m\u001b[32m0.49010\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 769 | loss: 0.49010 - acc: 0.7888 -- iter: 891/891\n",
      "--\n",
      "Training Step: 770  | total loss: \u001b[1m\u001b[32m0.54142\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 770 | loss: 0.54142 - acc: 0.7636 -- iter: 891/891\n",
      "--\n",
      "Training Step: 771  | total loss: \u001b[1m\u001b[32m0.53237\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 771 | loss: 0.53237 - acc: 0.7673 -- iter: 891/891\n",
      "--\n",
      "Training Step: 772  | total loss: \u001b[1m\u001b[32m0.52414\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 772 | loss: 0.52414 - acc: 0.7703 -- iter: 891/891\n",
      "--\n",
      "Training Step: 773  | total loss: \u001b[1m\u001b[32m0.51657\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 773 | loss: 0.51657 - acc: 0.7738 -- iter: 891/891\n",
      "--\n",
      "Training Step: 774  | total loss: \u001b[1m\u001b[32m0.55190\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 774 | loss: 0.55190 - acc: 0.7522 -- iter: 891/891\n",
      "--\n",
      "Training Step: 775  | total loss: \u001b[1m\u001b[32m0.54162\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 775 | loss: 0.54162 - acc: 0.7582 -- iter: 891/891\n",
      "--\n",
      "Training Step: 776  | total loss: \u001b[1m\u001b[32m0.53294\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 776 | loss: 0.53294 - acc: 0.7633 -- iter: 891/891\n",
      "--\n",
      "Training Step: 777  | total loss: \u001b[1m\u001b[32m0.52572\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 777 | loss: 0.52572 - acc: 0.7680 -- iter: 891/891\n",
      "--\n",
      "Training Step: 778  | total loss: \u001b[1m\u001b[32m0.51952\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 778 | loss: 0.51952 - acc: 0.7721 -- iter: 891/891\n",
      "--\n",
      "Training Step: 779  | total loss: \u001b[1m\u001b[32m0.51381\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 779 | loss: 0.51381 - acc: 0.7757 -- iter: 891/891\n",
      "--\n",
      "Training Step: 780  | total loss: \u001b[1m\u001b[32m0.54706\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 780 | loss: 0.54706 - acc: 0.7516 -- iter: 891/891\n",
      "--\n",
      "Training Step: 781  | total loss: \u001b[1m\u001b[32m0.53851\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 781 | loss: 0.53851 - acc: 0.7573 -- iter: 891/891\n",
      "--\n",
      "Training Step: 782  | total loss: \u001b[1m\u001b[32m0.53082\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 782 | loss: 0.53082 - acc: 0.7626 -- iter: 891/891\n",
      "--\n",
      "Training Step: 783  | total loss: \u001b[1m\u001b[32m0.52357\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 783 | loss: 0.52357 - acc: 0.7676 -- iter: 891/891\n",
      "--\n",
      "Training Step: 784  | total loss: \u001b[1m\u001b[32m0.55295\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 784 | loss: 0.55295 - acc: 0.7461 -- iter: 891/891\n",
      "--\n",
      "Training Step: 785  | total loss: \u001b[1m\u001b[32m0.54332\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 785 | loss: 0.54332 - acc: 0.7525 -- iter: 891/891\n",
      "--\n",
      "Training Step: 786  | total loss: \u001b[1m\u001b[32m0.56902\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 786 | loss: 0.56902 - acc: 0.7325 -- iter: 891/891\n",
      "--\n",
      "Training Step: 787  | total loss: \u001b[1m\u001b[32m0.55867\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 787 | loss: 0.55867 - acc: 0.7401 -- iter: 891/891\n",
      "--\n",
      "Training Step: 788  | total loss: \u001b[1m\u001b[32m0.54983\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 788 | loss: 0.54983 - acc: 0.7468 -- iter: 891/891\n",
      "--\n",
      "Training Step: 789  | total loss: \u001b[1m\u001b[32m0.54191\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 789 | loss: 0.54191 - acc: 0.7530 -- iter: 891/891\n",
      "--\n",
      "Training Step: 790  | total loss: \u001b[1m\u001b[32m0.56763\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 790 | loss: 0.56763 - acc: 0.7304 -- iter: 891/891\n",
      "--\n",
      "Training Step: 791  | total loss: \u001b[1m\u001b[32m0.55813\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 791 | loss: 0.55813 - acc: 0.7384 -- iter: 891/891\n",
      "--\n",
      "Training Step: 792  | total loss: \u001b[1m\u001b[32m0.54967\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 792 | loss: 0.54967 - acc: 0.7456 -- iter: 891/891\n",
      "--\n",
      "Training Step: 793  | total loss: \u001b[1m\u001b[32m0.54177\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 793 | loss: 0.54177 - acc: 0.7522 -- iter: 891/891\n",
      "--\n",
      "Training Step: 794  | total loss: \u001b[1m\u001b[32m0.53411\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 794 | loss: 0.53411 - acc: 0.7580 -- iter: 891/891\n",
      "--\n",
      "Training Step: 795  | total loss: \u001b[1m\u001b[32m0.52661\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 795 | loss: 0.52661 - acc: 0.7633 -- iter: 891/891\n",
      "--\n",
      "Training Step: 796  | total loss: \u001b[1m\u001b[32m0.51934\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 796 | loss: 0.51934 - acc: 0.7678 -- iter: 891/891\n",
      "--\n",
      "Training Step: 797  | total loss: \u001b[1m\u001b[32m0.51246\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 797 | loss: 0.51246 - acc: 0.7718 -- iter: 891/891\n",
      "--\n",
      "Training Step: 798  | total loss: \u001b[1m\u001b[32m0.55988\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 798 | loss: 0.55988 - acc: 0.7449 -- iter: 891/891\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 799  | total loss: \u001b[1m\u001b[32m0.54874\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 799 | loss: 0.54874 - acc: 0.7515 -- iter: 891/891\n",
      "--\n",
      "Training Step: 800  | total loss: \u001b[1m\u001b[32m0.53861\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 800 | loss: 0.53861 - acc: 0.7572 -- iter: 891/891\n",
      "--\n",
      "Training Step: 801  | total loss: \u001b[1m\u001b[32m0.52936\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 801 | loss: 0.52936 - acc: 0.7624 -- iter: 891/891\n",
      "--\n",
      "Training Step: 802  | total loss: \u001b[1m\u001b[32m0.52092\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 802 | loss: 0.52092 - acc: 0.7670 -- iter: 891/891\n",
      "--\n",
      "Training Step: 803  | total loss: \u001b[1m\u001b[32m0.51321\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 803 | loss: 0.51321 - acc: 0.7715 -- iter: 891/891\n",
      "--\n",
      "Training Step: 804  | total loss: \u001b[1m\u001b[32m0.50622\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 804 | loss: 0.50622 - acc: 0.7755 -- iter: 891/891\n",
      "--\n",
      "Training Step: 805  | total loss: \u001b[1m\u001b[32m0.49991\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 805 | loss: 0.49991 - acc: 0.7791 -- iter: 891/891\n",
      "--\n",
      "Training Step: 806  | total loss: \u001b[1m\u001b[32m0.54794\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 806 | loss: 0.54794 - acc: 0.7524 -- iter: 891/891\n",
      "--\n",
      "Training Step: 807  | total loss: \u001b[1m\u001b[32m0.53785\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 807 | loss: 0.53785 - acc: 0.7573 -- iter: 891/891\n",
      "--\n",
      "Training Step: 808  | total loss: \u001b[1m\u001b[32m0.52921\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 808 | loss: 0.52921 - acc: 0.7617 -- iter: 891/891\n",
      "--\n",
      "Training Step: 809  | total loss: \u001b[1m\u001b[32m0.52159\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 809 | loss: 0.52159 - acc: 0.7658 -- iter: 891/891\n",
      "--\n",
      "Training Step: 810  | total loss: \u001b[1m\u001b[32m0.51457\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 810 | loss: 0.51457 - acc: 0.7694 -- iter: 891/891\n",
      "--\n",
      "Training Step: 811  | total loss: \u001b[1m\u001b[32m0.50789\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 811 | loss: 0.50789 - acc: 0.7730 -- iter: 891/891\n",
      "--\n",
      "Training Step: 812  | total loss: \u001b[1m\u001b[32m0.50158\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 812 | loss: 0.50158 - acc: 0.7763 -- iter: 891/891\n",
      "--\n",
      "Training Step: 813  | total loss: \u001b[1m\u001b[32m0.49577\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 813 | loss: 0.49577 - acc: 0.7799 -- iter: 891/891\n",
      "--\n",
      "Training Step: 814  | total loss: \u001b[1m\u001b[32m0.54050\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 814 | loss: 0.54050 - acc: 0.7565 -- iter: 891/891\n",
      "--\n",
      "Training Step: 815  | total loss: \u001b[1m\u001b[32m0.53089\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 815 | loss: 0.53089 - acc: 0.7615 -- iter: 891/891\n",
      "--\n",
      "Training Step: 816  | total loss: \u001b[1m\u001b[32m0.57041\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 816 | loss: 0.57041 - acc: 0.7388 -- iter: 891/891\n",
      "--\n",
      "Training Step: 817  | total loss: \u001b[1m\u001b[32m0.55810\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 817 | loss: 0.55810 - acc: 0.7455 -- iter: 891/891\n",
      "--\n",
      "Training Step: 818  | total loss: \u001b[1m\u001b[32m0.58613\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 818 | loss: 0.58613 - acc: 0.7266 -- iter: 891/891\n",
      "--\n",
      "Training Step: 819  | total loss: \u001b[1m\u001b[32m0.57374\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 819 | loss: 0.57374 - acc: 0.7346 -- iter: 891/891\n",
      "--\n",
      "Training Step: 820  | total loss: \u001b[1m\u001b[32m0.56361\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 820 | loss: 0.56361 - acc: 0.7419 -- iter: 891/891\n",
      "--\n",
      "Training Step: 821  | total loss: \u001b[1m\u001b[32m0.55504\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 821 | loss: 0.55504 - acc: 0.7484 -- iter: 891/891\n",
      "--\n",
      "Training Step: 822  | total loss: \u001b[1m\u001b[32m0.54732\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 822 | loss: 0.54732 - acc: 0.7541 -- iter: 891/891\n",
      "--\n",
      "Training Step: 823  | total loss: \u001b[1m\u001b[32m0.53992\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 823 | loss: 0.53992 - acc: 0.7592 -- iter: 891/891\n",
      "--\n",
      "Training Step: 824  | total loss: \u001b[1m\u001b[32m0.53256\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 824 | loss: 0.53256 - acc: 0.7642 -- iter: 891/891\n",
      "--\n",
      "Training Step: 825  | total loss: \u001b[1m\u001b[32m0.52523\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 825 | loss: 0.52523 - acc: 0.7690 -- iter: 891/891\n",
      "--\n",
      "Training Step: 826  | total loss: \u001b[1m\u001b[32m0.55836\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 826 | loss: 0.55836 - acc: 0.7449 -- iter: 891/891\n",
      "--\n",
      "Training Step: 827  | total loss: \u001b[1m\u001b[32m0.54787\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 827 | loss: 0.54787 - acc: 0.7511 -- iter: 891/891\n",
      "--\n",
      "Training Step: 828  | total loss: \u001b[1m\u001b[32m0.53834\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 828 | loss: 0.53834 - acc: 0.7570 -- iter: 891/891\n",
      "--\n",
      "Training Step: 829  | total loss: \u001b[1m\u001b[32m0.52963\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 829 | loss: 0.52963 - acc: 0.7620 -- iter: 891/891\n",
      "--\n",
      "Training Step: 830  | total loss: \u001b[1m\u001b[32m0.52165\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 830 | loss: 0.52165 - acc: 0.7667 -- iter: 891/891\n",
      "--\n",
      "Training Step: 831  | total loss: \u001b[1m\u001b[32m0.51431\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 831 | loss: 0.51431 - acc: 0.7707 -- iter: 891/891\n",
      "--\n",
      "Training Step: 832  | total loss: \u001b[1m\u001b[32m0.50757\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 832 | loss: 0.50757 - acc: 0.7743 -- iter: 891/891\n",
      "--\n",
      "Training Step: 833  | total loss: \u001b[1m\u001b[32m0.50136\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 833 | loss: 0.50136 - acc: 0.7775 -- iter: 891/891\n",
      "--\n",
      "Training Step: 834  | total loss: \u001b[1m\u001b[32m0.49562\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 834 | loss: 0.49562 - acc: 0.7806 -- iter: 891/891\n",
      "--\n",
      "Training Step: 835  | total loss: \u001b[1m\u001b[32m0.49035\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 835 | loss: 0.49035 - acc: 0.7832 -- iter: 891/891\n",
      "--\n",
      "Training Step: 836  | total loss: \u001b[1m\u001b[32m0.48552\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 836 | loss: 0.48552 - acc: 0.7859 -- iter: 891/891\n",
      "--\n",
      "Training Step: 837  | total loss: \u001b[1m\u001b[32m0.48115\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 837 | loss: 0.48115 - acc: 0.7881 -- iter: 891/891\n",
      "--\n",
      "Training Step: 838  | total loss: \u001b[1m\u001b[32m0.53188\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 838 | loss: 0.53188 - acc: 0.7624 -- iter: 891/891\n",
      "--\n",
      "Training Step: 839  | total loss: \u001b[1m\u001b[32m0.52318\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 839 | loss: 0.52318 - acc: 0.7665 -- iter: 891/891\n",
      "--\n",
      "Training Step: 840  | total loss: \u001b[1m\u001b[32m0.51577\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 840 | loss: 0.51577 - acc: 0.7701 -- iter: 891/891\n",
      "--\n",
      "Training Step: 841  | total loss: \u001b[1m\u001b[32m0.50926\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 841 | loss: 0.50926 - acc: 0.7735 -- iter: 891/891\n",
      "--\n",
      "Training Step: 842  | total loss: \u001b[1m\u001b[32m0.54877\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 842 | loss: 0.54877 - acc: 0.7500 -- iter: 891/891\n",
      "--\n",
      "Training Step: 843  | total loss: \u001b[1m\u001b[32m0.53921\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 843 | loss: 0.53921 - acc: 0.7552 -- iter: 891/891\n",
      "--\n",
      "Training Step: 844  | total loss: \u001b[1m\u001b[32m0.53075\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 844 | loss: 0.53075 - acc: 0.7597 -- iter: 891/891\n",
      "--\n",
      "Training Step: 845  | total loss: \u001b[1m\u001b[32m0.52300\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 845 | loss: 0.52300 - acc: 0.7645 -- iter: 891/891\n",
      "--\n",
      "Training Step: 846  | total loss: \u001b[1m\u001b[32m0.51580\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 846 | loss: 0.51580 - acc: 0.7688 -- iter: 891/891\n",
      "--\n",
      "Training Step: 847  | total loss: \u001b[1m\u001b[32m0.50914\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 847 | loss: 0.50914 - acc: 0.7731 -- iter: 891/891\n",
      "--\n",
      "Training Step: 848  | total loss: \u001b[1m\u001b[32m0.50308\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 848 | loss: 0.50308 - acc: 0.7766 -- iter: 891/891\n",
      "--\n",
      "Training Step: 849  | total loss: \u001b[1m\u001b[32m0.49767\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 849 | loss: 0.49767 - acc: 0.7798 -- iter: 891/891\n",
      "--\n",
      "Training Step: 850  | total loss: \u001b[1m\u001b[32m0.49289\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 850 | loss: 0.49289 - acc: 0.7829 -- iter: 891/891\n",
      "--\n",
      "Training Step: 851  | total loss: \u001b[1m\u001b[32m0.48867\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 851 | loss: 0.48867 - acc: 0.7854 -- iter: 891/891\n",
      "--\n",
      "Training Step: 852  | total loss: \u001b[1m\u001b[32m0.48491\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 852 | loss: 0.48491 - acc: 0.7877 -- iter: 891/891\n",
      "--\n",
      "Training Step: 853  | total loss: \u001b[1m\u001b[32m0.48151\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 853 | loss: 0.48151 - acc: 0.7894 -- iter: 891/891\n",
      "--\n",
      "Training Step: 854  | total loss: \u001b[1m\u001b[32m0.47832\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 854 | loss: 0.47832 - acc: 0.7907 -- iter: 891/891\n",
      "--\n",
      "Training Step: 855  | total loss: \u001b[1m\u001b[32m0.47526\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 855 | loss: 0.47526 - acc: 0.7919 -- iter: 891/891\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 856  | total loss: \u001b[1m\u001b[32m0.47227\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 856 | loss: 0.47227 - acc: 0.7934 -- iter: 891/891\n",
      "--\n",
      "Training Step: 857  | total loss: \u001b[1m\u001b[32m0.46937\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 857 | loss: 0.46937 - acc: 0.7947 -- iter: 891/891\n",
      "--\n",
      "Training Step: 858  | total loss: \u001b[1m\u001b[32m0.51957\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 858 | loss: 0.51957 - acc: 0.7671 -- iter: 891/891\n",
      "--\n",
      "Training Step: 859  | total loss: \u001b[1m\u001b[32m0.51201\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 859 | loss: 0.51201 - acc: 0.7709 -- iter: 891/891\n",
      "--\n",
      "Training Step: 860  | total loss: \u001b[1m\u001b[32m0.50579\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 860 | loss: 0.50579 - acc: 0.7741 -- iter: 891/891\n",
      "--\n",
      "Training Step: 861  | total loss: \u001b[1m\u001b[32m0.50070\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 861 | loss: 0.50070 - acc: 0.7769 -- iter: 891/891\n",
      "--\n",
      "Training Step: 862  | total loss: \u001b[1m\u001b[32m0.53690\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 862 | loss: 0.53690 - acc: 0.7531 -- iter: 891/891\n",
      "--\n",
      "Training Step: 863  | total loss: \u001b[1m\u001b[32m0.52978\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 863 | loss: 0.52978 - acc: 0.7579 -- iter: 891/891\n",
      "--\n",
      "Training Step: 864  | total loss: \u001b[1m\u001b[32m0.52374\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 864 | loss: 0.52374 - acc: 0.7620 -- iter: 891/891\n",
      "--\n",
      "Training Step: 865  | total loss: \u001b[1m\u001b[32m0.51802\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 865 | loss: 0.51802 - acc: 0.7659 -- iter: 891/891\n",
      "--\n",
      "Training Step: 866  | total loss: \u001b[1m\u001b[32m0.51224\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 866 | loss: 0.51224 - acc: 0.7705 -- iter: 891/891\n",
      "--\n",
      "Training Step: 867  | total loss: \u001b[1m\u001b[32m0.50640\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 867 | loss: 0.50640 - acc: 0.7747 -- iter: 891/891\n",
      "--\n",
      "Training Step: 868  | total loss: \u001b[1m\u001b[32m0.50075\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 868 | loss: 0.50075 - acc: 0.7785 -- iter: 891/891\n",
      "--\n",
      "Training Step: 869  | total loss: \u001b[1m\u001b[32m0.49556\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 869 | loss: 0.49556 - acc: 0.7814 -- iter: 891/891\n",
      "--\n",
      "Training Step: 870  | total loss: \u001b[1m\u001b[32m0.54333\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 870 | loss: 0.54333 - acc: 0.7550 -- iter: 891/891\n",
      "--\n",
      "Training Step: 871  | total loss: \u001b[1m\u001b[32m0.53411\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 871 | loss: 0.53411 - acc: 0.7603 -- iter: 891/891\n",
      "--\n",
      "Training Step: 872  | total loss: \u001b[1m\u001b[32m0.56828\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 872 | loss: 0.56828 - acc: 0.7411 -- iter: 891/891\n",
      "--\n",
      "Training Step: 873  | total loss: \u001b[1m\u001b[32m0.55667\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 873 | loss: 0.55667 - acc: 0.7475 -- iter: 891/891\n",
      "--\n",
      "Training Step: 874  | total loss: \u001b[1m\u001b[32m0.58936\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 874 | loss: 0.58936 - acc: 0.7260 -- iter: 891/891\n",
      "--\n",
      "Training Step: 875  | total loss: \u001b[1m\u001b[32m0.57621\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 875 | loss: 0.57621 - acc: 0.7342 -- iter: 891/891\n",
      "--\n",
      "Training Step: 876  | total loss: \u001b[1m\u001b[32m0.59716\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 876 | loss: 0.59716 - acc: 0.7157 -- iter: 891/891\n",
      "--\n",
      "Training Step: 877  | total loss: \u001b[1m\u001b[32m0.58538\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 877 | loss: 0.58538 - acc: 0.7249 -- iter: 891/891\n",
      "--\n",
      "Training Step: 878  | total loss: \u001b[1m\u001b[32m0.57603\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 878 | loss: 0.57603 - acc: 0.7332 -- iter: 891/891\n",
      "--\n",
      "Training Step: 879  | total loss: \u001b[1m\u001b[32m0.56828\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 879 | loss: 0.56828 - acc: 0.7403 -- iter: 891/891\n",
      "--\n",
      "Training Step: 880  | total loss: \u001b[1m\u001b[32m0.58473\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 880 | loss: 0.58473 - acc: 0.7228 -- iter: 891/891\n",
      "--\n",
      "Training Step: 881  | total loss: \u001b[1m\u001b[32m0.57665\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 881 | loss: 0.57665 - acc: 0.7312 -- iter: 891/891\n",
      "--\n",
      "Training Step: 882  | total loss: \u001b[1m\u001b[32m0.59182\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 882 | loss: 0.59182 - acc: 0.7133 -- iter: 891/891\n",
      "--\n",
      "Training Step: 883  | total loss: \u001b[1m\u001b[32m0.58329\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 883 | loss: 0.58329 - acc: 0.7229 -- iter: 891/891\n",
      "--\n",
      "Training Step: 884  | total loss: \u001b[1m\u001b[32m0.57540\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 884 | loss: 0.57540 - acc: 0.7315 -- iter: 891/891\n",
      "--\n",
      "Training Step: 885  | total loss: \u001b[1m\u001b[32m0.56766\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 885 | loss: 0.56766 - acc: 0.7394 -- iter: 891/891\n",
      "--\n",
      "Training Step: 886  | total loss: \u001b[1m\u001b[32m0.55979\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 886 | loss: 0.55979 - acc: 0.7465 -- iter: 891/891\n",
      "--\n",
      "Training Step: 887  | total loss: \u001b[1m\u001b[32m0.55172\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 887 | loss: 0.55172 - acc: 0.7529 -- iter: 891/891\n",
      "--\n",
      "Training Step: 888  | total loss: \u001b[1m\u001b[32m0.54358\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 888 | loss: 0.54358 - acc: 0.7583 -- iter: 891/891\n",
      "--\n",
      "Training Step: 889  | total loss: \u001b[1m\u001b[32m0.53558\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 889 | loss: 0.53558 - acc: 0.7628 -- iter: 891/891\n",
      "--\n",
      "Training Step: 890  | total loss: \u001b[1m\u001b[32m0.52801\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 890 | loss: 0.52801 - acc: 0.7677 -- iter: 891/891\n",
      "--\n",
      "Training Step: 891  | total loss: \u001b[1m\u001b[32m0.52114\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 891 | loss: 0.52114 - acc: 0.7715 -- iter: 891/891\n",
      "--\n",
      "Training Step: 892  | total loss: \u001b[1m\u001b[32m0.56666\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 892 | loss: 0.56666 - acc: 0.7489 -- iter: 891/891\n",
      "--\n",
      "Training Step: 893  | total loss: \u001b[1m\u001b[32m0.55605\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 893 | loss: 0.55605 - acc: 0.7535 -- iter: 891/891\n",
      "--\n",
      "Training Step: 894  | total loss: \u001b[1m\u001b[32m0.60405\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 894 | loss: 0.60405 - acc: 0.7299 -- iter: 891/891\n",
      "--\n",
      "Training Step: 895  | total loss: \u001b[1m\u001b[32m0.58902\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 895 | loss: 0.58902 - acc: 0.7370 -- iter: 891/891\n",
      "--\n",
      "Training Step: 896  | total loss: \u001b[1m\u001b[32m0.62593\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 896 | loss: 0.62593 - acc: 0.7145 -- iter: 891/891\n",
      "--\n",
      "Training Step: 897  | total loss: \u001b[1m\u001b[32m0.60847\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 897 | loss: 0.60847 - acc: 0.7246 -- iter: 891/891\n",
      "--\n",
      "Training Step: 898  | total loss: \u001b[1m\u001b[32m0.59336\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 898 | loss: 0.59336 - acc: 0.7338 -- iter: 891/891\n",
      "--\n",
      "Training Step: 899  | total loss: \u001b[1m\u001b[32m0.58051\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 899 | loss: 0.58051 - acc: 0.7414 -- iter: 891/891\n",
      "--\n",
      "Training Step: 900  | total loss: \u001b[1m\u001b[32m0.56939\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 900 | loss: 0.56939 - acc: 0.7486 -- iter: 891/891\n",
      "--\n",
      "Training Step: 901  | total loss: \u001b[1m\u001b[32m0.55944\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 901 | loss: 0.55944 - acc: 0.7549 -- iter: 891/891\n",
      "--\n",
      "Training Step: 902  | total loss: \u001b[1m\u001b[32m0.55017\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 902 | loss: 0.55017 - acc: 0.7602 -- iter: 891/891\n",
      "--\n",
      "Training Step: 903  | total loss: \u001b[1m\u001b[32m0.54122\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 903 | loss: 0.54122 - acc: 0.7648 -- iter: 891/891\n",
      "--\n",
      "Training Step: 904  | total loss: \u001b[1m\u001b[32m0.57050\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 904 | loss: 0.57050 - acc: 0.7430 -- iter: 891/891\n",
      "--\n",
      "Training Step: 905  | total loss: \u001b[1m\u001b[32m0.55879\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 905 | loss: 0.55879 - acc: 0.7499 -- iter: 891/891\n",
      "--\n",
      "Training Step: 906  | total loss: \u001b[1m\u001b[32m0.54816\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 906 | loss: 0.54816 - acc: 0.7559 -- iter: 891/891\n",
      "--\n",
      "Training Step: 907  | total loss: \u001b[1m\u001b[32m0.53844\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 907 | loss: 0.53844 - acc: 0.7615 -- iter: 891/891\n",
      "--\n",
      "Training Step: 908  | total loss: \u001b[1m\u001b[32m0.52950\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 908 | loss: 0.52950 - acc: 0.7670 -- iter: 891/891\n",
      "--\n",
      "Training Step: 909  | total loss: \u001b[1m\u001b[32m0.52128\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 909 | loss: 0.52128 - acc: 0.7715 -- iter: 891/891\n",
      "--\n",
      "Training Step: 910  | total loss: \u001b[1m\u001b[32m0.55972\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 910 | loss: 0.55972 - acc: 0.7495 -- iter: 891/891\n",
      "--\n",
      "Training Step: 911  | total loss: \u001b[1m\u001b[32m0.54849\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 911 | loss: 0.54849 - acc: 0.7550 -- iter: 891/891\n",
      "--\n",
      "Training Step: 912  | total loss: \u001b[1m\u001b[32m0.58477\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 912 | loss: 0.58477 - acc: 0.7323 -- iter: 891/891\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 913  | total loss: \u001b[1m\u001b[32m0.57147\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 913 | loss: 0.57147 - acc: 0.7396 -- iter: 891/891\n",
      "--\n",
      "Training Step: 914  | total loss: \u001b[1m\u001b[32m0.55991\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 914 | loss: 0.55991 - acc: 0.7470 -- iter: 891/891\n",
      "--\n",
      "Training Step: 915  | total loss: \u001b[1m\u001b[32m0.54987\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 915 | loss: 0.54987 - acc: 0.7531 -- iter: 891/891\n",
      "--\n",
      "Training Step: 916  | total loss: \u001b[1m\u001b[32m0.54095\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 916 | loss: 0.54095 - acc: 0.7586 -- iter: 891/891\n",
      "--\n",
      "Training Step: 917  | total loss: \u001b[1m\u001b[32m0.53272\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 917 | loss: 0.53272 - acc: 0.7637 -- iter: 891/891\n",
      "--\n",
      "Training Step: 918  | total loss: \u001b[1m\u001b[32m0.52494\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 918 | loss: 0.52494 - acc: 0.7685 -- iter: 891/891\n",
      "--\n",
      "Training Step: 919  | total loss: \u001b[1m\u001b[32m0.51757\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 919 | loss: 0.51757 - acc: 0.7726 -- iter: 891/891\n",
      "--\n",
      "Training Step: 920  | total loss: \u001b[1m\u001b[32m0.55555\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 920 | loss: 0.55555 - acc: 0.7487 -- iter: 891/891\n",
      "--\n",
      "Training Step: 921  | total loss: \u001b[1m\u001b[32m0.54484\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 921 | loss: 0.54484 - acc: 0.7543 -- iter: 891/891\n",
      "--\n",
      "Training Step: 922  | total loss: \u001b[1m\u001b[32m0.53513\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 922 | loss: 0.53513 - acc: 0.7594 -- iter: 891/891\n",
      "--\n",
      "Training Step: 923  | total loss: \u001b[1m\u001b[32m0.52630\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 923 | loss: 0.52630 - acc: 0.7646 -- iter: 891/891\n",
      "--\n",
      "Training Step: 924  | total loss: \u001b[1m\u001b[32m0.51828\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 924 | loss: 0.51828 - acc: 0.7694 -- iter: 891/891\n",
      "--\n",
      "Training Step: 925  | total loss: \u001b[1m\u001b[32m0.51094\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 925 | loss: 0.51094 - acc: 0.7738 -- iter: 891/891\n",
      "--\n",
      "Training Step: 926  | total loss: \u001b[1m\u001b[32m0.50421\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 926 | loss: 0.50421 - acc: 0.7770 -- iter: 891/891\n",
      "--\n",
      "Training Step: 927  | total loss: \u001b[1m\u001b[32m0.49809\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 927 | loss: 0.49809 - acc: 0.7794 -- iter: 891/891\n",
      "--\n",
      "Training Step: 928  | total loss: \u001b[1m\u001b[32m0.49253\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 928 | loss: 0.49253 - acc: 0.7814 -- iter: 891/891\n",
      "--\n",
      "Training Step: 929  | total loss: \u001b[1m\u001b[32m0.48749\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 929 | loss: 0.48749 - acc: 0.7833 -- iter: 891/891\n",
      "--\n",
      "Training Step: 930  | total loss: \u001b[1m\u001b[32m0.53640\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 930 | loss: 0.53640 - acc: 0.7568 -- iter: 891/891\n",
      "--\n",
      "Training Step: 931  | total loss: \u001b[1m\u001b[32m0.52697\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 931 | loss: 0.52697 - acc: 0.7618 -- iter: 891/891\n",
      "--\n",
      "Training Step: 932  | total loss: \u001b[1m\u001b[32m0.51863\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 932 | loss: 0.51863 - acc: 0.7661 -- iter: 891/891\n",
      "--\n",
      "Training Step: 933  | total loss: \u001b[1m\u001b[32m0.51124\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 933 | loss: 0.51124 - acc: 0.7694 -- iter: 891/891\n",
      "--\n",
      "Training Step: 934  | total loss: \u001b[1m\u001b[32m0.55099\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 934 | loss: 0.55099 - acc: 0.7473 -- iter: 891/891\n",
      "--\n",
      "Training Step: 935  | total loss: \u001b[1m\u001b[32m0.54096\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 935 | loss: 0.54096 - acc: 0.7530 -- iter: 891/891\n",
      "--\n",
      "Training Step: 936  | total loss: \u001b[1m\u001b[32m0.53222\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 936 | loss: 0.53222 - acc: 0.7578 -- iter: 891/891\n",
      "--\n",
      "Training Step: 937  | total loss: \u001b[1m\u001b[32m0.52429\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 937 | loss: 0.52429 - acc: 0.7623 -- iter: 891/891\n",
      "--\n",
      "Training Step: 938  | total loss: \u001b[1m\u001b[32m0.55496\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 938 | loss: 0.55496 - acc: 0.7420 -- iter: 891/891\n",
      "--\n",
      "Training Step: 939  | total loss: \u001b[1m\u001b[32m0.54496\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 939 | loss: 0.54496 - acc: 0.7486 -- iter: 891/891\n",
      "--\n",
      "Training Step: 940  | total loss: \u001b[1m\u001b[32m0.53621\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 940 | loss: 0.53621 - acc: 0.7544 -- iter: 891/891\n",
      "--\n",
      "Training Step: 941  | total loss: \u001b[1m\u001b[32m0.52839\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 941 | loss: 0.52839 - acc: 0.7596 -- iter: 891/891\n",
      "--\n",
      "Training Step: 942  | total loss: \u001b[1m\u001b[32m0.52122\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 942 | loss: 0.52122 - acc: 0.7649 -- iter: 891/891\n",
      "--\n",
      "Training Step: 943  | total loss: \u001b[1m\u001b[32m0.51458\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 943 | loss: 0.51458 - acc: 0.7696 -- iter: 891/891\n",
      "--\n",
      "Training Step: 944  | total loss: \u001b[1m\u001b[32m0.54598\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 944 | loss: 0.54598 - acc: 0.7494 -- iter: 891/891\n",
      "--\n",
      "Training Step: 945  | total loss: \u001b[1m\u001b[32m0.53696\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 945 | loss: 0.53696 - acc: 0.7555 -- iter: 891/891\n",
      "--\n",
      "Training Step: 946  | total loss: \u001b[1m\u001b[32m0.52895\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 946 | loss: 0.52895 - acc: 0.7608 -- iter: 891/891\n",
      "--\n",
      "Training Step: 947  | total loss: \u001b[1m\u001b[32m0.52175\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 947 | loss: 0.52175 - acc: 0.7656 -- iter: 891/891\n",
      "--\n",
      "Training Step: 948  | total loss: \u001b[1m\u001b[32m0.51518\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 948 | loss: 0.51518 - acc: 0.7700 -- iter: 891/891\n",
      "--\n",
      "Training Step: 949  | total loss: \u001b[1m\u001b[32m0.50912\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 949 | loss: 0.50912 - acc: 0.7737 -- iter: 891/891\n",
      "--\n",
      "Training Step: 950  | total loss: \u001b[1m\u001b[32m0.50346\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 950 | loss: 0.50346 - acc: 0.7770 -- iter: 891/891\n",
      "--\n",
      "Training Step: 951  | total loss: \u001b[1m\u001b[32m0.49814\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 951 | loss: 0.49814 - acc: 0.7800 -- iter: 891/891\n",
      "--\n",
      "Training Step: 952  | total loss: \u001b[1m\u001b[32m0.53936\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 952 | loss: 0.53936 - acc: 0.7538 -- iter: 891/891\n",
      "--\n",
      "Training Step: 953  | total loss: \u001b[1m\u001b[32m0.53019\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 953 | loss: 0.53019 - acc: 0.7594 -- iter: 891/891\n",
      "--\n",
      "Training Step: 954  | total loss: \u001b[1m\u001b[32m0.52191\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 954 | loss: 0.52191 - acc: 0.7645 -- iter: 891/891\n",
      "--\n",
      "Training Step: 955  | total loss: \u001b[1m\u001b[32m0.51445\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 955 | loss: 0.51445 - acc: 0.7693 -- iter: 891/891\n",
      "--\n",
      "Training Step: 956  | total loss: \u001b[1m\u001b[32m0.54923\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 956 | loss: 0.54923 - acc: 0.7464 -- iter: 891/891\n",
      "--\n",
      "Training Step: 957  | total loss: \u001b[1m\u001b[32m0.53951\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 957 | loss: 0.53951 - acc: 0.7522 -- iter: 891/891\n",
      "--\n",
      "Training Step: 958  | total loss: \u001b[1m\u001b[32m0.53119\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 958 | loss: 0.53119 - acc: 0.7572 -- iter: 891/891\n",
      "--\n",
      "Training Step: 959  | total loss: \u001b[1m\u001b[32m0.52381\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 959 | loss: 0.52381 - acc: 0.7616 -- iter: 891/891\n",
      "--\n",
      "Training Step: 960  | total loss: \u001b[1m\u001b[32m0.55496\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 960 | loss: 0.55496 - acc: 0.7397 -- iter: 891/891\n",
      "--\n",
      "Training Step: 961  | total loss: \u001b[1m\u001b[32m0.54546\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 961 | loss: 0.54546 - acc: 0.7457 -- iter: 891/891\n",
      "--\n",
      "Training Step: 962  | total loss: \u001b[1m\u001b[32m0.56900\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 962 | loss: 0.56900 - acc: 0.7283 -- iter: 891/891\n",
      "--\n",
      "Training Step: 963  | total loss: \u001b[1m\u001b[32m0.55906\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 963 | loss: 0.55906 - acc: 0.7357 -- iter: 891/891\n",
      "--\n",
      "Training Step: 964  | total loss: \u001b[1m\u001b[32m0.55047\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 964 | loss: 0.55047 - acc: 0.7423 -- iter: 891/891\n",
      "--\n",
      "Training Step: 965  | total loss: \u001b[1m\u001b[32m0.54259\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 965 | loss: 0.54259 - acc: 0.7486 -- iter: 891/891\n",
      "--\n",
      "Training Step: 966  | total loss: \u001b[1m\u001b[32m0.53501\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 966 | loss: 0.53501 - acc: 0.7549 -- iter: 891/891\n",
      "--\n",
      "Training Step: 967  | total loss: \u001b[1m\u001b[32m0.52758\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 967 | loss: 0.52758 - acc: 0.7607 -- iter: 891/891\n",
      "--\n",
      "Training Step: 968  | total loss: \u001b[1m\u001b[32m0.52040\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 968 | loss: 0.52040 - acc: 0.7657 -- iter: 891/891\n",
      "--\n",
      "Training Step: 969  | total loss: \u001b[1m\u001b[32m0.51365\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 969 | loss: 0.51365 - acc: 0.7701 -- iter: 891/891\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 970  | total loss: \u001b[1m\u001b[32m0.50748\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 970 | loss: 0.50748 - acc: 0.7746 -- iter: 891/891\n",
      "--\n",
      "Training Step: 971  | total loss: \u001b[1m\u001b[32m0.50195\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 971 | loss: 0.50195 - acc: 0.7784 -- iter: 891/891\n",
      "--\n",
      "Training Step: 972  | total loss: \u001b[1m\u001b[32m0.49709\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 972 | loss: 0.49709 - acc: 0.7813 -- iter: 891/891\n",
      "--\n",
      "Training Step: 973  | total loss: \u001b[1m\u001b[32m0.49281\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 973 | loss: 0.49281 - acc: 0.7828 -- iter: 891/891\n",
      "--\n",
      "Training Step: 974  | total loss: \u001b[1m\u001b[32m0.48893\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 974 | loss: 0.48893 - acc: 0.7843 -- iter: 891/891\n",
      "--\n",
      "Training Step: 975  | total loss: \u001b[1m\u001b[32m0.48525\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 975 | loss: 0.48525 - acc: 0.7857 -- iter: 891/891\n",
      "--\n",
      "Training Step: 976  | total loss: \u001b[1m\u001b[32m0.48163\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 976 | loss: 0.48163 - acc: 0.7877 -- iter: 891/891\n",
      "--\n",
      "Training Step: 977  | total loss: \u001b[1m\u001b[32m0.47803\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 977 | loss: 0.47803 - acc: 0.7898 -- iter: 891/891\n",
      "--\n",
      "Training Step: 978  | total loss: \u001b[1m\u001b[32m0.47451\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 978 | loss: 0.47451 - acc: 0.7915 -- iter: 891/891\n",
      "--\n",
      "Training Step: 979  | total loss: \u001b[1m\u001b[32m0.47124\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 979 | loss: 0.47124 - acc: 0.7933 -- iter: 891/891\n",
      "--\n",
      "Training Step: 980  | total loss: \u001b[1m\u001b[32m0.46841\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 980 | loss: 0.46841 - acc: 0.7943 -- iter: 891/891\n",
      "--\n",
      "Training Step: 981  | total loss: \u001b[1m\u001b[32m0.46612\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 981 | loss: 0.46612 - acc: 0.7950 -- iter: 891/891\n",
      "--\n",
      "Training Step: 982  | total loss: \u001b[1m\u001b[32m0.51586\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 982 | loss: 0.51586 - acc: 0.7675 -- iter: 891/891\n",
      "--\n",
      "Training Step: 983  | total loss: \u001b[1m\u001b[32m0.50969\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 983 | loss: 0.50969 - acc: 0.7712 -- iter: 891/891\n",
      "--\n",
      "Training Step: 984  | total loss: \u001b[1m\u001b[32m0.50446\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 984 | loss: 0.50446 - acc: 0.7743 -- iter: 891/891\n",
      "--\n",
      "Training Step: 985  | total loss: \u001b[1m\u001b[32m0.49955\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 985 | loss: 0.49955 - acc: 0.7774 -- iter: 891/891\n",
      "--\n",
      "Training Step: 986  | total loss: \u001b[1m\u001b[32m0.49463\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 986 | loss: 0.49463 - acc: 0.7801 -- iter: 891/891\n",
      "--\n",
      "Training Step: 987  | total loss: \u001b[1m\u001b[32m0.48975\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 987 | loss: 0.48975 - acc: 0.7825 -- iter: 891/891\n",
      "--\n",
      "Training Step: 988  | total loss: \u001b[1m\u001b[32m0.48516\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 988 | loss: 0.48516 - acc: 0.7850 -- iter: 891/891\n",
      "--\n",
      "Training Step: 989  | total loss: \u001b[1m\u001b[32m0.48108\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 989 | loss: 0.48108 - acc: 0.7872 -- iter: 891/891\n",
      "--\n",
      "Training Step: 990  | total loss: \u001b[1m\u001b[32m0.47759\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 990 | loss: 0.47759 - acc: 0.7893 -- iter: 891/891\n",
      "--\n",
      "Training Step: 991  | total loss: \u001b[1m\u001b[32m0.47464\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 991 | loss: 0.47464 - acc: 0.7911 -- iter: 891/891\n",
      "--\n",
      "Training Step: 992  | total loss: \u001b[1m\u001b[32m0.47211\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 992 | loss: 0.47211 - acc: 0.7924 -- iter: 891/891\n",
      "--\n",
      "Training Step: 993  | total loss: \u001b[1m\u001b[32m0.46987\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 993 | loss: 0.46987 - acc: 0.7932 -- iter: 891/891\n",
      "--\n",
      "Training Step: 994  | total loss: \u001b[1m\u001b[32m0.52006\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 994 | loss: 0.52006 - acc: 0.7690 -- iter: 891/891\n",
      "--\n",
      "Training Step: 995  | total loss: \u001b[1m\u001b[32m0.51271\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 995 | loss: 0.51271 - acc: 0.7721 -- iter: 891/891\n",
      "--\n",
      "Training Step: 996  | total loss: \u001b[1m\u001b[32m0.50596\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 996 | loss: 0.50596 - acc: 0.7754 -- iter: 891/891\n",
      "--\n",
      "Training Step: 997  | total loss: \u001b[1m\u001b[32m0.49992\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 997 | loss: 0.49992 - acc: 0.7787 -- iter: 891/891\n",
      "--\n",
      "Training Step: 998  | total loss: \u001b[1m\u001b[32m0.54174\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 998 | loss: 0.54174 - acc: 0.7515 -- iter: 891/891\n",
      "--\n",
      "Training Step: 999  | total loss: \u001b[1m\u001b[32m0.53321\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 999 | loss: 0.53321 - acc: 0.7576 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1000  | total loss: \u001b[1m\u001b[32m0.52631\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1000 | loss: 0.52631 - acc: 0.7628 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1001  | total loss: \u001b[1m\u001b[32m0.52051\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1001 | loss: 0.52051 - acc: 0.7669 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1002  | total loss: \u001b[1m\u001b[32m0.51521\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1002 | loss: 0.51521 - acc: 0.7704 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1003  | total loss: \u001b[1m\u001b[32m0.50989\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1003 | loss: 0.50989 - acc: 0.7736 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1004  | total loss: \u001b[1m\u001b[32m0.50434\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1004 | loss: 0.50434 - acc: 0.7765 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1005  | total loss: \u001b[1m\u001b[32m0.49875\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1005 | loss: 0.49875 - acc: 0.7802 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1006  | total loss: \u001b[1m\u001b[32m0.49348\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1006 | loss: 0.49348 - acc: 0.7832 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1007  | total loss: \u001b[1m\u001b[32m0.48872\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1007 | loss: 0.48872 - acc: 0.7858 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1008  | total loss: \u001b[1m\u001b[32m0.53627\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1008 | loss: 0.53627 - acc: 0.7607 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1009  | total loss: \u001b[1m\u001b[32m0.52738\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1009 | loss: 0.52738 - acc: 0.7644 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1010  | total loss: \u001b[1m\u001b[32m0.57418\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1010 | loss: 0.57418 - acc: 0.7394 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1011  | total loss: \u001b[1m\u001b[32m0.56148\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1011 | loss: 0.56148 - acc: 0.7449 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1012  | total loss: \u001b[1m\u001b[32m0.59540\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1012 | loss: 0.59540 - acc: 0.7229 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1013  | total loss: \u001b[1m\u001b[32m0.58106\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1013 | loss: 0.58106 - acc: 0.7318 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1014  | total loss: \u001b[1m\u001b[32m0.56909\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1014 | loss: 0.56909 - acc: 0.7397 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1015  | total loss: \u001b[1m\u001b[32m0.55898\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1015 | loss: 0.55898 - acc: 0.7467 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1016  | total loss: \u001b[1m\u001b[32m0.58390\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1016 | loss: 0.58390 - acc: 0.7254 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1017  | total loss: \u001b[1m\u001b[32m0.57334\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1017 | loss: 0.57334 - acc: 0.7331 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1018  | total loss: \u001b[1m\u001b[32m0.59288\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1018 | loss: 0.59288 - acc: 0.7134 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1019  | total loss: \u001b[1m\u001b[32m0.58280\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1019 | loss: 0.58280 - acc: 0.7224 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1020  | total loss: \u001b[1m\u001b[32m0.57397\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1020 | loss: 0.57397 - acc: 0.7308 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1021  | total loss: \u001b[1m\u001b[32m0.56574\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1021 | loss: 0.56574 - acc: 0.7387 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1022  | total loss: \u001b[1m\u001b[32m0.55775\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1022 | loss: 0.55775 - acc: 0.7460 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1023  | total loss: \u001b[1m\u001b[32m0.54980\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1023 | loss: 0.54980 - acc: 0.7527 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1024  | total loss: \u001b[1m\u001b[32m0.54187\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1024 | loss: 0.54187 - acc: 0.7587 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1025  | total loss: \u001b[1m\u001b[32m0.53409\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1025 | loss: 0.53409 - acc: 0.7634 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1026  | total loss: \u001b[1m\u001b[32m0.52668\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1026 | loss: 0.52668 - acc: 0.7682 -- iter: 891/891\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1027  | total loss: \u001b[1m\u001b[32m0.51989\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1027 | loss: 0.51989 - acc: 0.7724 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1028  | total loss: \u001b[1m\u001b[32m0.56128\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1028 | loss: 0.56128 - acc: 0.7490 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1029  | total loss: \u001b[1m\u001b[32m0.55116\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1029 | loss: 0.55116 - acc: 0.7540 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1030  | total loss: \u001b[1m\u001b[32m0.54193\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1030 | loss: 0.54193 - acc: 0.7585 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1031  | total loss: \u001b[1m\u001b[32m0.53337\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1031 | loss: 0.53337 - acc: 0.7632 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1032  | total loss: \u001b[1m\u001b[32m0.57638\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1032 | loss: 0.57638 - acc: 0.7409 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1033  | total loss: \u001b[1m\u001b[32m0.56382\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1033 | loss: 0.56382 - acc: 0.7482 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1034  | total loss: \u001b[1m\u001b[32m0.55243\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1034 | loss: 0.55243 - acc: 0.7545 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1035  | total loss: \u001b[1m\u001b[32m0.54223\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1035 | loss: 0.54223 - acc: 0.7607 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1036  | total loss: \u001b[1m\u001b[32m0.57462\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1036 | loss: 0.57462 - acc: 0.7382 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1037  | total loss: \u001b[1m\u001b[32m0.56334\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1037 | loss: 0.56334 - acc: 0.7452 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1038  | total loss: \u001b[1m\u001b[32m0.55409\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1038 | loss: 0.55409 - acc: 0.7510 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1039  | total loss: \u001b[1m\u001b[32m0.54617\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1039 | loss: 0.54617 - acc: 0.7563 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1040  | total loss: \u001b[1m\u001b[32m0.57039\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1040 | loss: 0.57039 - acc: 0.7367 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1041  | total loss: \u001b[1m\u001b[32m0.56110\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1041 | loss: 0.56110 - acc: 0.7430 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1042  | total loss: \u001b[1m\u001b[32m0.58336\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1042 | loss: 0.58336 - acc: 0.7230 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1043  | total loss: \u001b[1m\u001b[32m0.57314\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1043 | loss: 0.57314 - acc: 0.7314 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1044  | total loss: \u001b[1m\u001b[32m0.56393\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1044 | loss: 0.56393 - acc: 0.7394 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1045  | total loss: \u001b[1m\u001b[32m0.55524\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1045 | loss: 0.55524 - acc: 0.7463 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1046  | total loss: \u001b[1m\u001b[32m0.57714\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1046 | loss: 0.57714 - acc: 0.7259 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1047  | total loss: \u001b[1m\u001b[32m0.56663\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1047 | loss: 0.56663 - acc: 0.7344 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1048  | total loss: \u001b[1m\u001b[32m0.55709\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1048 | loss: 0.55709 - acc: 0.7421 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1049  | total loss: \u001b[1m\u001b[32m0.54829\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1049 | loss: 0.54829 - acc: 0.7488 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1050  | total loss: \u001b[1m\u001b[32m0.57438\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1050 | loss: 0.57438 - acc: 0.7265 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1051  | total loss: \u001b[1m\u001b[32m0.56384\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1051 | loss: 0.56384 - acc: 0.7350 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1052  | total loss: \u001b[1m\u001b[32m0.58778\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1052 | loss: 0.58778 - acc: 0.7147 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1053  | total loss: \u001b[1m\u001b[32m0.57655\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1053 | loss: 0.57655 - acc: 0.7245 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1054  | total loss: \u001b[1m\u001b[32m0.59838\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1054 | loss: 0.59838 - acc: 0.7054 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1055  | total loss: \u001b[1m\u001b[32m0.58734\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1055 | loss: 0.58734 - acc: 0.7163 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1056  | total loss: \u001b[1m\u001b[32m0.57804\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1056 | loss: 0.57804 - acc: 0.7259 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1057  | total loss: \u001b[1m\u001b[32m0.56995\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1057 | loss: 0.56995 - acc: 0.7343 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1058  | total loss: \u001b[1m\u001b[32m0.56262\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1058 | loss: 0.56262 - acc: 0.7419 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1059  | total loss: \u001b[1m\u001b[32m0.55565\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1059 | loss: 0.55565 - acc: 0.7483 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1060  | total loss: \u001b[1m\u001b[32m0.54870\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1060 | loss: 0.54870 - acc: 0.7542 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1061  | total loss: \u001b[1m\u001b[32m0.54157\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1061 | loss: 0.54157 - acc: 0.7599 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1062  | total loss: \u001b[1m\u001b[32m0.56789\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1062 | loss: 0.56789 - acc: 0.7380 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1063  | total loss: \u001b[1m\u001b[32m0.55784\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1063 | loss: 0.55784 - acc: 0.7455 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1064  | total loss: \u001b[1m\u001b[32m0.54858\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1064 | loss: 0.54858 - acc: 0.7520 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1065  | total loss: \u001b[1m\u001b[32m0.53986\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1065 | loss: 0.53986 - acc: 0.7577 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1066  | total loss: \u001b[1m\u001b[32m0.53153\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1066 | loss: 0.53153 - acc: 0.7632 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1067  | total loss: \u001b[1m\u001b[32m0.52365\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1067 | loss: 0.52365 - acc: 0.7685 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1068  | total loss: \u001b[1m\u001b[32m0.56109\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1068 | loss: 0.56109 - acc: 0.7463 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1069  | total loss: \u001b[1m\u001b[32m0.54992\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1069 | loss: 0.54992 - acc: 0.7535 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1070  | total loss: \u001b[1m\u001b[32m0.53971\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1070 | loss: 0.53971 - acc: 0.7597 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1071  | total loss: \u001b[1m\u001b[32m0.53041\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1071 | loss: 0.53041 - acc: 0.7650 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1072  | total loss: \u001b[1m\u001b[32m0.56698\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1072 | loss: 0.56698 - acc: 0.7419 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1073  | total loss: \u001b[1m\u001b[32m0.55535\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1073 | loss: 0.55535 - acc: 0.7482 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1074  | total loss: \u001b[1m\u001b[32m0.54520\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1074 | loss: 0.54520 - acc: 0.7537 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1075  | total loss: \u001b[1m\u001b[32m0.53620\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1075 | loss: 0.53620 - acc: 0.7591 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1076  | total loss: \u001b[1m\u001b[32m0.52803\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1076 | loss: 0.52803 - acc: 0.7644 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1077  | total loss: \u001b[1m\u001b[32m0.52036\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1077 | loss: 0.52036 - acc: 0.7691 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1078  | total loss: \u001b[1m\u001b[32m0.51300\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1078 | loss: 0.51300 - acc: 0.7732 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1079  | total loss: \u001b[1m\u001b[32m0.50613\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1079 | loss: 0.50613 - acc: 0.7764 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1080  | total loss: \u001b[1m\u001b[32m0.54928\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1080 | loss: 0.54928 - acc: 0.7519 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1081  | total loss: \u001b[1m\u001b[32m0.53875\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1081 | loss: 0.53875 - acc: 0.7572 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1082  | total loss: \u001b[1m\u001b[32m0.52925\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1082 | loss: 0.52925 - acc: 0.7622 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1083  | total loss: \u001b[1m\u001b[32m0.52072\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1083 | loss: 0.52072 - acc: 0.7668 -- iter: 891/891\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1084  | total loss: \u001b[1m\u001b[32m0.51304\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1084 | loss: 0.51304 - acc: 0.7712 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1085  | total loss: \u001b[1m\u001b[32m0.50610\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1085 | loss: 0.50610 - acc: 0.7752 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1086  | total loss: \u001b[1m\u001b[32m0.49981\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1086 | loss: 0.49981 - acc: 0.7784 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1087  | total loss: \u001b[1m\u001b[32m0.49413\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1087 | loss: 0.49413 - acc: 0.7806 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1088  | total loss: \u001b[1m\u001b[32m0.54106\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1088 | loss: 0.54106 - acc: 0.7533 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1089  | total loss: \u001b[1m\u001b[32m0.53141\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1089 | loss: 0.53141 - acc: 0.7583 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1090  | total loss: \u001b[1m\u001b[32m0.52299\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1090 | loss: 0.52299 - acc: 0.7627 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1091  | total loss: \u001b[1m\u001b[32m0.51563\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1091 | loss: 0.51563 - acc: 0.7668 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1092  | total loss: \u001b[1m\u001b[32m0.55145\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1092 | loss: 0.55145 - acc: 0.7435 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1093  | total loss: \u001b[1m\u001b[32m0.54201\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1093 | loss: 0.54201 - acc: 0.7498 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1094  | total loss: \u001b[1m\u001b[32m0.57162\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1094 | loss: 0.57162 - acc: 0.7288 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1095  | total loss: \u001b[1m\u001b[32m0.56186\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1095 | loss: 0.56186 - acc: 0.7361 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1096  | total loss: \u001b[1m\u001b[32m0.58558\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1096 | loss: 0.58558 - acc: 0.7143 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1097  | total loss: \u001b[1m\u001b[32m0.57673\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1097 | loss: 0.57673 - acc: 0.7226 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1098  | total loss: \u001b[1m\u001b[32m0.56948\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1098 | loss: 0.56948 - acc: 0.7301 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1099  | total loss: \u001b[1m\u001b[32m0.56298\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1099 | loss: 0.56298 - acc: 0.7372 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1100  | total loss: \u001b[1m\u001b[32m0.58067\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1100 | loss: 0.58067 - acc: 0.7169 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1101  | total loss: \u001b[1m\u001b[32m0.57283\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1101 | loss: 0.57283 - acc: 0.7258 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1102  | total loss: \u001b[1m\u001b[32m0.59000\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1102 | loss: 0.59000 - acc: 0.7074 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1103  | total loss: \u001b[1m\u001b[32m0.58126\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1103 | loss: 0.58126 - acc: 0.7174 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1104  | total loss: \u001b[1m\u001b[32m0.59492\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1104 | loss: 0.59492 - acc: 0.7030 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1105  | total loss: \u001b[1m\u001b[32m0.58591\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1105 | loss: 0.58591 - acc: 0.7136 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1106  | total loss: \u001b[1m\u001b[32m0.57774\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1106 | loss: 0.57774 - acc: 0.7228 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1107  | total loss: \u001b[1m\u001b[32m0.56999\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1107 | loss: 0.56999 - acc: 0.7309 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1108  | total loss: \u001b[1m\u001b[32m0.58907\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1108 | loss: 0.58907 - acc: 0.7111 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1109  | total loss: \u001b[1m\u001b[32m0.57961\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1109 | loss: 0.57961 - acc: 0.7200 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1110  | total loss: \u001b[1m\u001b[32m0.57087\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1110 | loss: 0.57087 - acc: 0.7280 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1111  | total loss: \u001b[1m\u001b[32m0.56261\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1111 | loss: 0.56261 - acc: 0.7338 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1112  | total loss: \u001b[1m\u001b[32m0.58455\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1112 | loss: 0.58455 - acc: 0.7151 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1113  | total loss: \u001b[1m\u001b[32m0.57437\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1113 | loss: 0.57437 - acc: 0.7243 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1114  | total loss: \u001b[1m\u001b[32m0.56504\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1114 | loss: 0.56504 - acc: 0.7324 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1115  | total loss: \u001b[1m\u001b[32m0.55632\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1115 | loss: 0.55632 - acc: 0.7395 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1116  | total loss: \u001b[1m\u001b[32m0.54802\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1116 | loss: 0.54802 - acc: 0.7470 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1117  | total loss: \u001b[1m\u001b[32m0.54002\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1117 | loss: 0.54002 - acc: 0.7539 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1118  | total loss: \u001b[1m\u001b[32m0.53227\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1118 | loss: 0.53227 - acc: 0.7602 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1119  | total loss: \u001b[1m\u001b[32m0.52480\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1119 | loss: 0.52480 - acc: 0.7659 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1120  | total loss: \u001b[1m\u001b[32m0.51766\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1120 | loss: 0.51766 - acc: 0.7707 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1121  | total loss: \u001b[1m\u001b[32m0.51092\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1121 | loss: 0.51092 - acc: 0.7753 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1122  | total loss: \u001b[1m\u001b[32m0.50464\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1122 | loss: 0.50464 - acc: 0.7790 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1123  | total loss: \u001b[1m\u001b[32m0.49886\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1123 | loss: 0.49886 - acc: 0.7820 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1124  | total loss: \u001b[1m\u001b[32m0.55149\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1124 | loss: 0.55149 - acc: 0.7559 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1125  | total loss: \u001b[1m\u001b[32m0.54077\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1125 | loss: 0.54077 - acc: 0.7610 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1126  | total loss: \u001b[1m\u001b[32m0.58521\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1126 | loss: 0.58521 - acc: 0.7374 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1127  | total loss: \u001b[1m\u001b[32m0.57174\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1127 | loss: 0.57174 - acc: 0.7447 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1128  | total loss: \u001b[1m\u001b[32m0.56039\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1128 | loss: 0.56039 - acc: 0.7511 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1129  | total loss: \u001b[1m\u001b[32m0.55041\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1129 | loss: 0.55041 - acc: 0.7568 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1130  | total loss: \u001b[1m\u001b[32m0.54124\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1130 | loss: 0.54124 - acc: 0.7616 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1131  | total loss: \u001b[1m\u001b[32m0.53248\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1131 | loss: 0.53248 - acc: 0.7658 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1132  | total loss: \u001b[1m\u001b[32m0.52396\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1132 | loss: 0.52396 - acc: 0.7696 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1133  | total loss: \u001b[1m\u001b[32m0.51595\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1133 | loss: 0.51595 - acc: 0.7737 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1134  | total loss: \u001b[1m\u001b[32m0.50883\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1134 | loss: 0.50883 - acc: 0.7777 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1135  | total loss: \u001b[1m\u001b[32m0.50259\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1135 | loss: 0.50259 - acc: 0.7809 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1136  | total loss: \u001b[1m\u001b[32m0.55106\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1136 | loss: 0.55106 - acc: 0.7562 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1137  | total loss: \u001b[1m\u001b[32m0.54070\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1137 | loss: 0.54070 - acc: 0.7610 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1138  | total loss: \u001b[1m\u001b[32m0.53139\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1138 | loss: 0.53139 - acc: 0.7648 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1139  | total loss: \u001b[1m\u001b[32m0.52297\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1139 | loss: 0.52297 - acc: 0.7684 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1140  | total loss: \u001b[1m\u001b[32m0.55658\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1140 | loss: 0.55658 - acc: 0.7460 -- iter: 891/891\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1141  | total loss: \u001b[1m\u001b[32m0.54582\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1141 | loss: 0.54582 - acc: 0.7523 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1142  | total loss: \u001b[1m\u001b[32m0.53662\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1142 | loss: 0.53662 - acc: 0.7581 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1143  | total loss: \u001b[1m\u001b[32m0.52878\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1143 | loss: 0.52878 - acc: 0.7636 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1144  | total loss: \u001b[1m\u001b[32m0.55800\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1144 | loss: 0.55800 - acc: 0.7400 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1145  | total loss: \u001b[1m\u001b[32m0.54925\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1145 | loss: 0.54925 - acc: 0.7467 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1146  | total loss: \u001b[1m\u001b[32m0.57304\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1146 | loss: 0.57304 - acc: 0.7253 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1147  | total loss: \u001b[1m\u001b[32m0.56471\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1147 | loss: 0.56471 - acc: 0.7327 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1148  | total loss: \u001b[1m\u001b[32m0.55781\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1148 | loss: 0.55781 - acc: 0.7397 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1149  | total loss: \u001b[1m\u001b[32m0.55159\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1149 | loss: 0.55159 - acc: 0.7461 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1150  | total loss: \u001b[1m\u001b[32m0.54549\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1150 | loss: 0.54549 - acc: 0.7523 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1151  | total loss: \u001b[1m\u001b[32m0.53918\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1151 | loss: 0.53918 - acc: 0.7580 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1152  | total loss: \u001b[1m\u001b[32m0.53257\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1152 | loss: 0.53257 - acc: 0.7633 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1153  | total loss: \u001b[1m\u001b[32m0.52582\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1153 | loss: 0.52582 - acc: 0.7683 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1154  | total loss: \u001b[1m\u001b[32m0.56218\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1154 | loss: 0.56218 - acc: 0.7418 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1155  | total loss: \u001b[1m\u001b[32m0.55190\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1155 | loss: 0.55190 - acc: 0.7488 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1156  | total loss: \u001b[1m\u001b[32m0.58484\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1156 | loss: 0.58484 - acc: 0.7266 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1157  | total loss: \u001b[1m\u001b[32m0.57241\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1157 | loss: 0.57241 - acc: 0.7348 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1158  | total loss: \u001b[1m\u001b[32m0.59737\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1158 | loss: 0.59737 - acc: 0.7160 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1159  | total loss: \u001b[1m\u001b[32m0.58409\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1159 | loss: 0.58409 - acc: 0.7255 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1160  | total loss: \u001b[1m\u001b[32m0.60639\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1160 | loss: 0.60639 - acc: 0.7078 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1161  | total loss: \u001b[1m\u001b[32m0.59353\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1161 | loss: 0.59353 - acc: 0.7185 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1162  | total loss: \u001b[1m\u001b[32m0.58284\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1162 | loss: 0.58284 - acc: 0.7282 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1163  | total loss: \u001b[1m\u001b[32m0.57375\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1163 | loss: 0.57375 - acc: 0.7367 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1164  | total loss: \u001b[1m\u001b[32m0.56564\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1164 | loss: 0.56564 - acc: 0.7442 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1165  | total loss: \u001b[1m\u001b[32m0.55799\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1165 | loss: 0.55799 - acc: 0.7511 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1166  | total loss: \u001b[1m\u001b[32m0.57439\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1166 | loss: 0.57439 - acc: 0.7344 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1167  | total loss: \u001b[1m\u001b[32m0.56508\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1167 | loss: 0.56508 - acc: 0.7422 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1168  | total loss: \u001b[1m\u001b[32m0.55630\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1168 | loss: 0.55630 - acc: 0.7491 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1169  | total loss: \u001b[1m\u001b[32m0.54774\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1169 | loss: 0.54774 - acc: 0.7557 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1170  | total loss: \u001b[1m\u001b[32m0.53931\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1170 | loss: 0.53931 - acc: 0.7617 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1171  | total loss: \u001b[1m\u001b[32m0.53111\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1171 | loss: 0.53111 - acc: 0.7674 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1172  | total loss: \u001b[1m\u001b[32m0.56122\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1172 | loss: 0.56122 - acc: 0.7472 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1173  | total loss: \u001b[1m\u001b[32m0.55033\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1173 | loss: 0.55033 - acc: 0.7535 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1174  | total loss: \u001b[1m\u001b[32m0.58079\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1174 | loss: 0.58079 - acc: 0.7330 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1175  | total loss: \u001b[1m\u001b[32m0.56810\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1175 | loss: 0.56810 - acc: 0.7404 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1176  | total loss: \u001b[1m\u001b[32m0.55679\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1176 | loss: 0.55679 - acc: 0.7471 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1177  | total loss: \u001b[1m\u001b[32m0.54662\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1177 | loss: 0.54662 - acc: 0.7539 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1178  | total loss: \u001b[1m\u001b[32m0.57189\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1178 | loss: 0.57189 - acc: 0.7360 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1179  | total loss: \u001b[1m\u001b[32m0.56087\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1179 | loss: 0.56087 - acc: 0.7438 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1180  | total loss: \u001b[1m\u001b[32m0.55137\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1180 | loss: 0.55137 - acc: 0.7509 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1181  | total loss: \u001b[1m\u001b[32m0.54291\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1181 | loss: 0.54291 - acc: 0.7571 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1182  | total loss: \u001b[1m\u001b[32m0.53509\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1182 | loss: 0.53509 - acc: 0.7622 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1183  | total loss: \u001b[1m\u001b[32m0.52763\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1183 | loss: 0.52763 - acc: 0.7665 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1184  | total loss: \u001b[1m\u001b[32m0.52035\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1184 | loss: 0.52035 - acc: 0.7708 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1185  | total loss: \u001b[1m\u001b[32m0.51330\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1185 | loss: 0.51330 - acc: 0.7750 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1186  | total loss: \u001b[1m\u001b[32m0.50670\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1186 | loss: 0.50670 - acc: 0.7791 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1187  | total loss: \u001b[1m\u001b[32m0.50071\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1187 | loss: 0.50071 - acc: 0.7822 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1188  | total loss: \u001b[1m\u001b[32m0.49536\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1188 | loss: 0.49536 - acc: 0.7851 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1189  | total loss: \u001b[1m\u001b[32m0.49062\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1189 | loss: 0.49062 - acc: 0.7876 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1190  | total loss: \u001b[1m\u001b[32m0.53980\u001b[0m\u001b[0m | time: 0.012s\n",
      "| Adam | epoch: 1190 | loss: 0.53980 - acc: 0.7631 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1191  | total loss: \u001b[1m\u001b[32m0.53056\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1191 | loss: 0.53056 - acc: 0.7664 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1192  | total loss: \u001b[1m\u001b[32m0.52197\u001b[0m\u001b[0m | time: 0.011s\n",
      "| Adam | epoch: 1192 | loss: 0.52197 - acc: 0.7696 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1193  | total loss: \u001b[1m\u001b[32m0.51403\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1193 | loss: 0.51403 - acc: 0.7731 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1194  | total loss: \u001b[1m\u001b[32m0.55236\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1194 | loss: 0.55236 - acc: 0.7518 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1195  | total loss: \u001b[1m\u001b[32m0.54207\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1195 | loss: 0.54207 - acc: 0.7576 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1196  | total loss: \u001b[1m\u001b[32m0.53342\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1196 | loss: 0.53342 - acc: 0.7625 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1197  | total loss: \u001b[1m\u001b[32m0.52604\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1197 | loss: 0.52604 - acc: 0.7664 -- iter: 891/891\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1198  | total loss: \u001b[1m\u001b[32m0.51956\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1198 | loss: 0.51956 - acc: 0.7700 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1199  | total loss: \u001b[1m\u001b[32m0.51340\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1199 | loss: 0.51340 - acc: 0.7731 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1200  | total loss: \u001b[1m\u001b[32m0.54342\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1200 | loss: 0.54342 - acc: 0.7519 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1201  | total loss: \u001b[1m\u001b[32m0.53448\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1201 | loss: 0.53448 - acc: 0.7578 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1202  | total loss: \u001b[1m\u001b[32m0.56451\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1202 | loss: 0.56451 - acc: 0.7371 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1203  | total loss: \u001b[1m\u001b[32m0.55431\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1203 | loss: 0.55431 - acc: 0.7448 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1204  | total loss: \u001b[1m\u001b[32m0.54561\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1204 | loss: 0.54561 - acc: 0.7513 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1205  | total loss: \u001b[1m\u001b[32m0.53798\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1205 | loss: 0.53798 - acc: 0.7571 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1206  | total loss: \u001b[1m\u001b[32m0.56650\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1206 | loss: 0.56650 - acc: 0.7328 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1207  | total loss: \u001b[1m\u001b[32m0.55731\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1207 | loss: 0.55731 - acc: 0.7408 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1208  | total loss: \u001b[1m\u001b[32m0.57871\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1208 | loss: 0.57871 - acc: 0.7209 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1209  | total loss: \u001b[1m\u001b[32m0.56935\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1209 | loss: 0.56935 - acc: 0.7296 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1210  | total loss: \u001b[1m\u001b[32m0.56134\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1210 | loss: 0.56134 - acc: 0.7374 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1211  | total loss: \u001b[1m\u001b[32m0.55419\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1211 | loss: 0.55419 - acc: 0.7445 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1212  | total loss: \u001b[1m\u001b[32m0.54747\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1212 | loss: 0.54747 - acc: 0.7510 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1213  | total loss: \u001b[1m\u001b[32m0.54090\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1213 | loss: 0.54090 - acc: 0.7571 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1214  | total loss: \u001b[1m\u001b[32m0.53435\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1214 | loss: 0.53435 - acc: 0.7625 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1215  | total loss: \u001b[1m\u001b[32m0.52781\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1215 | loss: 0.52781 - acc: 0.7673 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1216  | total loss: \u001b[1m\u001b[32m0.52136\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1216 | loss: 0.52136 - acc: 0.7717 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1217  | total loss: \u001b[1m\u001b[32m0.51515\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1217 | loss: 0.51515 - acc: 0.7757 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1218  | total loss: \u001b[1m\u001b[32m0.55584\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1218 | loss: 0.55584 - acc: 0.7509 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1219  | total loss: \u001b[1m\u001b[32m0.54578\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1219 | loss: 0.54578 - acc: 0.7567 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1220  | total loss: \u001b[1m\u001b[32m0.53654\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1220 | loss: 0.53654 - acc: 0.7617 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1221  | total loss: \u001b[1m\u001b[32m0.52797\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1221 | loss: 0.52797 - acc: 0.7665 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1222  | total loss: \u001b[1m\u001b[32m0.52002\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1222 | loss: 0.52002 - acc: 0.7710 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1223  | total loss: \u001b[1m\u001b[32m0.51264\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1223 | loss: 0.51264 - acc: 0.7751 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1224  | total loss: \u001b[1m\u001b[32m0.55536\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1224 | loss: 0.55536 - acc: 0.7499 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1225  | total loss: \u001b[1m\u001b[32m0.54424\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1225 | loss: 0.54424 - acc: 0.7561 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1226  | total loss: \u001b[1m\u001b[32m0.53452\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1226 | loss: 0.53452 - acc: 0.7610 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1227  | total loss: \u001b[1m\u001b[32m0.52614\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1227 | loss: 0.52614 - acc: 0.7655 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1228  | total loss: \u001b[1m\u001b[32m0.55680\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1228 | loss: 0.55680 - acc: 0.7461 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1229  | total loss: \u001b[1m\u001b[32m0.54728\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1229 | loss: 0.54728 - acc: 0.7521 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1230  | total loss: \u001b[1m\u001b[32m0.53911\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1230 | loss: 0.53911 - acc: 0.7571 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1231  | total loss: \u001b[1m\u001b[32m0.53157\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1231 | loss: 0.53157 - acc: 0.7619 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1232  | total loss: \u001b[1m\u001b[32m0.56058\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1232 | loss: 0.56058 - acc: 0.7415 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1233  | total loss: \u001b[1m\u001b[32m0.55053\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1233 | loss: 0.55053 - acc: 0.7477 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1234  | total loss: \u001b[1m\u001b[32m0.54140\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1234 | loss: 0.54140 - acc: 0.7535 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1235  | total loss: \u001b[1m\u001b[32m0.53293\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1235 | loss: 0.53293 - acc: 0.7591 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1236  | total loss: \u001b[1m\u001b[32m0.56467\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1236 | loss: 0.56467 - acc: 0.7358 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1237  | total loss: \u001b[1m\u001b[32m0.55388\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1237 | loss: 0.55388 - acc: 0.7436 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1238  | total loss: \u001b[1m\u001b[32m0.58250\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1238 | loss: 0.58250 - acc: 0.7201 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1239  | total loss: \u001b[1m\u001b[32m0.57090\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1239 | loss: 0.57090 - acc: 0.7292 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1240  | total loss: \u001b[1m\u001b[32m0.58980\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1240 | loss: 0.58980 - acc: 0.7114 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1241  | total loss: \u001b[1m\u001b[32m0.57923\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1241 | loss: 0.57923 - acc: 0.7212 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1242  | total loss: \u001b[1m\u001b[32m0.59589\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1242 | loss: 0.59589 - acc: 0.7039 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1243  | total loss: \u001b[1m\u001b[32m0.58683\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1243 | loss: 0.58683 - acc: 0.7142 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1244  | total loss: \u001b[1m\u001b[32m0.57947\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1244 | loss: 0.57947 - acc: 0.7236 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1245  | total loss: \u001b[1m\u001b[32m0.57310\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1245 | loss: 0.57310 - acc: 0.7316 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1246  | total loss: \u001b[1m\u001b[32m0.58776\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1246 | loss: 0.58776 - acc: 0.7151 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1247  | total loss: \u001b[1m\u001b[32m0.58048\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1247 | loss: 0.58048 - acc: 0.7235 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1248  | total loss: \u001b[1m\u001b[32m0.57364\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1248 | loss: 0.57364 - acc: 0.7314 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1249  | total loss: \u001b[1m\u001b[32m0.56686\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1249 | loss: 0.56686 - acc: 0.7386 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1250  | total loss: \u001b[1m\u001b[32m0.58433\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1250 | loss: 0.58433 - acc: 0.7199 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1251  | total loss: \u001b[1m\u001b[32m0.57536\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1251 | loss: 0.57536 - acc: 0.7284 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1252  | total loss: \u001b[1m\u001b[32m0.59364\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1252 | loss: 0.59364 - acc: 0.7092 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1253  | total loss: \u001b[1m\u001b[32m0.58333\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1253 | loss: 0.58333 - acc: 0.7196 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1254  | total loss: \u001b[1m\u001b[32m0.59875\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1254 | loss: 0.59875 - acc: 0.7025 -- iter: 891/891\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1255  | total loss: \u001b[1m\u001b[32m0.58789\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1255 | loss: 0.58789 - acc: 0.7136 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1256  | total loss: \u001b[1m\u001b[32m0.60402\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1256 | loss: 0.60402 - acc: 0.6966 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1257  | total loss: \u001b[1m\u001b[32m0.59291\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1257 | loss: 0.59291 - acc: 0.7078 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1258  | total loss: \u001b[1m\u001b[32m0.60660\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1258 | loss: 0.60660 - acc: 0.6931 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1259  | total loss: \u001b[1m\u001b[32m0.59583\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1259 | loss: 0.59583 - acc: 0.7047 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1260  | total loss: \u001b[1m\u001b[32m0.61153\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1260 | loss: 0.61153 - acc: 0.6876 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1261  | total loss: \u001b[1m\u001b[32m0.60103\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1261 | loss: 0.60103 - acc: 0.6997 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1262  | total loss: \u001b[1m\u001b[32m0.61475\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1262 | loss: 0.61475 - acc: 0.6837 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1263  | total loss: \u001b[1m\u001b[32m0.60482\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1263 | loss: 0.60482 - acc: 0.6966 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1264  | total loss: \u001b[1m\u001b[32m0.61410\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1264 | loss: 0.61410 - acc: 0.6834 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1265  | total loss: \u001b[1m\u001b[32m0.60487\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1265 | loss: 0.60487 - acc: 0.6963 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1266  | total loss: \u001b[1m\u001b[32m0.61579\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1266 | loss: 0.61579 - acc: 0.6819 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1267  | total loss: \u001b[1m\u001b[32m0.60664\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1267 | loss: 0.60664 - acc: 0.6939 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1268  | total loss: \u001b[1m\u001b[32m0.61688\u001b[0m\u001b[0m | time: 0.011s\n",
      "| Adam | epoch: 1268 | loss: 0.61688 - acc: 0.6800 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1269  | total loss: \u001b[1m\u001b[32m0.60760\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1269 | loss: 0.60760 - acc: 0.6926 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1270  | total loss: \u001b[1m\u001b[32m0.59898\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1270 | loss: 0.59898 - acc: 0.7040 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1271  | total loss: \u001b[1m\u001b[32m0.59058\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1271 | loss: 0.59058 - acc: 0.7144 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1272  | total loss: \u001b[1m\u001b[32m0.60387\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1272 | loss: 0.60387 - acc: 0.6989 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1273  | total loss: \u001b[1m\u001b[32m0.59363\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1273 | loss: 0.59363 - acc: 0.7100 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1274  | total loss: \u001b[1m\u001b[32m0.60895\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1274 | loss: 0.60895 - acc: 0.6934 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1275  | total loss: \u001b[1m\u001b[32m0.59748\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1275 | loss: 0.59748 - acc: 0.7057 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1276  | total loss: \u001b[1m\u001b[32m0.61077\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1276 | loss: 0.61077 - acc: 0.6908 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1277  | total loss: \u001b[1m\u001b[32m0.59885\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1277 | loss: 0.59885 - acc: 0.7034 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1278  | total loss: \u001b[1m\u001b[32m0.58788\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1278 | loss: 0.58788 - acc: 0.7145 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1279  | total loss: \u001b[1m\u001b[32m0.57758\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1279 | loss: 0.57758 - acc: 0.7245 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1280  | total loss: \u001b[1m\u001b[32m0.56779\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1280 | loss: 0.56779 - acc: 0.7334 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1281  | total loss: \u001b[1m\u001b[32m0.55841\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1281 | loss: 0.55841 - acc: 0.7412 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1282  | total loss: \u001b[1m\u001b[32m0.54939\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1282 | loss: 0.54939 - acc: 0.7485 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1283  | total loss: \u001b[1m\u001b[32m0.54071\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1283 | loss: 0.54071 - acc: 0.7548 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1284  | total loss: \u001b[1m\u001b[32m0.56991\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1284 | loss: 0.56991 - acc: 0.7354 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1285  | total loss: \u001b[1m\u001b[32m0.55862\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1285 | loss: 0.55862 - acc: 0.7429 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1286  | total loss: \u001b[1m\u001b[32m0.59429\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1286 | loss: 0.59429 - acc: 0.7204 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1287  | total loss: \u001b[1m\u001b[32m0.58051\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1287 | loss: 0.58051 - acc: 0.7286 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1288  | total loss: \u001b[1m\u001b[32m0.56801\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1288 | loss: 0.56801 - acc: 0.7364 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1289  | total loss: \u001b[1m\u001b[32m0.55668\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1289 | loss: 0.55668 - acc: 0.7444 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1290  | total loss: \u001b[1m\u001b[32m0.58437\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1290 | loss: 0.58437 - acc: 0.7241 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1291  | total loss: \u001b[1m\u001b[32m0.57230\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1291 | loss: 0.57230 - acc: 0.7332 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1292  | total loss: \u001b[1m\u001b[32m0.59430\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1292 | loss: 0.59430 - acc: 0.7163 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1293  | total loss: \u001b[1m\u001b[32m0.58278\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1293 | loss: 0.58278 - acc: 0.7260 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1294  | total loss: \u001b[1m\u001b[32m0.60413\u001b[0m\u001b[0m | time: 0.016s\n",
      "| Adam | epoch: 1294 | loss: 0.60413 - acc: 0.7051 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1295  | total loss: \u001b[1m\u001b[32m0.59399\u001b[0m\u001b[0m | time: 0.018s\n",
      "| Adam | epoch: 1295 | loss: 0.59399 - acc: 0.7149 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1296  | total loss: \u001b[1m\u001b[32m0.58561\u001b[0m\u001b[0m | time: 0.014s\n",
      "| Adam | epoch: 1296 | loss: 0.58561 - acc: 0.7238 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1297  | total loss: \u001b[1m\u001b[32m0.57809\u001b[0m\u001b[0m | time: 0.012s\n",
      "| Adam | epoch: 1297 | loss: 0.57809 - acc: 0.7319 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1298  | total loss: \u001b[1m\u001b[32m0.59482\u001b[0m\u001b[0m | time: 0.016s\n",
      "| Adam | epoch: 1298 | loss: 0.59482 - acc: 0.7125 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1299  | total loss: \u001b[1m\u001b[32m0.58588\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1299 | loss: 0.58588 - acc: 0.7218 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1300  | total loss: \u001b[1m\u001b[32m0.57744\u001b[0m\u001b[0m | time: 0.014s\n",
      "| Adam | epoch: 1300 | loss: 0.57744 - acc: 0.7308 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1301  | total loss: \u001b[1m\u001b[32m0.56916\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1301 | loss: 0.56916 - acc: 0.7393 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1302  | total loss: \u001b[1m\u001b[32m0.56089\u001b[0m\u001b[0m | time: 0.011s\n",
      "| Adam | epoch: 1302 | loss: 0.56089 - acc: 0.7465 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1303  | total loss: \u001b[1m\u001b[32m0.55264\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1303 | loss: 0.55264 - acc: 0.7526 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1304  | total loss: \u001b[1m\u001b[32m0.57919\u001b[0m\u001b[0m | time: 0.011s\n",
      "| Adam | epoch: 1304 | loss: 0.57919 - acc: 0.7311 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1305  | total loss: \u001b[1m\u001b[32m0.56846\u001b[0m\u001b[0m | time: 0.011s\n",
      "| Adam | epoch: 1305 | loss: 0.56846 - acc: 0.7385 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1306  | total loss: \u001b[1m\u001b[32m0.59460\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1306 | loss: 0.59460 - acc: 0.7169 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1307  | total loss: \u001b[1m\u001b[32m0.58253\u001b[0m\u001b[0m | time: 0.011s\n",
      "| Adam | epoch: 1307 | loss: 0.58253 - acc: 0.7255 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1308  | total loss: \u001b[1m\u001b[32m0.57177\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1308 | loss: 0.57177 - acc: 0.7332 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1309  | total loss: \u001b[1m\u001b[32m0.56201\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1309 | loss: 0.56201 - acc: 0.7402 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1310  | total loss: \u001b[1m\u001b[32m0.55304\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1310 | loss: 0.55304 - acc: 0.7467 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1311  | total loss: \u001b[1m\u001b[32m0.54473\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1311 | loss: 0.54473 - acc: 0.7525 -- iter: 891/891\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1312  | total loss: \u001b[1m\u001b[32m0.53696\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1312 | loss: 0.53696 - acc: 0.7576 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1313  | total loss: \u001b[1m\u001b[32m0.52961\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1313 | loss: 0.52961 - acc: 0.7625 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1314  | total loss: \u001b[1m\u001b[32m0.55919\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1314 | loss: 0.55919 - acc: 0.7414 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1315  | total loss: \u001b[1m\u001b[32m0.54910\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1315 | loss: 0.54910 - acc: 0.7485 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1316  | total loss: \u001b[1m\u001b[32m0.57817\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1316 | loss: 0.57817 - acc: 0.7258 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1317  | total loss: \u001b[1m\u001b[32m0.56662\u001b[0m\u001b[0m | time: 0.011s\n",
      "| Adam | epoch: 1317 | loss: 0.56662 - acc: 0.7335 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1318  | total loss: \u001b[1m\u001b[32m0.59078\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1318 | loss: 0.59078 - acc: 0.7126 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1319  | total loss: \u001b[1m\u001b[32m0.57949\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1319 | loss: 0.57949 - acc: 0.7223 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1320  | total loss: \u001b[1m\u001b[32m0.59744\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1320 | loss: 0.59744 - acc: 0.7036 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1321  | total loss: \u001b[1m\u001b[32m0.58801\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1321 | loss: 0.58801 - acc: 0.7138 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1322  | total loss: \u001b[1m\u001b[32m0.60217\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1322 | loss: 0.60217 - acc: 0.6988 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1323  | total loss: \u001b[1m\u001b[32m0.59419\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1323 | loss: 0.59419 - acc: 0.7086 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1324  | total loss: \u001b[1m\u001b[32m0.60967\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1324 | loss: 0.60967 - acc: 0.6902 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1325  | total loss: \u001b[1m\u001b[32m0.60179\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1325 | loss: 0.60179 - acc: 0.7008 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1326  | total loss: \u001b[1m\u001b[32m0.61228\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1326 | loss: 0.61228 - acc: 0.6878 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1327  | total loss: \u001b[1m\u001b[32m0.60427\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1327 | loss: 0.60427 - acc: 0.6997 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1328  | total loss: \u001b[1m\u001b[32m0.59674\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1328 | loss: 0.59674 - acc: 0.7104 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1329  | total loss: \u001b[1m\u001b[32m0.58924\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1329 | loss: 0.58924 - acc: 0.7207 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1330  | total loss: \u001b[1m\u001b[32m0.60402\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1330 | loss: 0.60402 - acc: 0.7024 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1331  | total loss: \u001b[1m\u001b[32m0.59444\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1331 | loss: 0.59444 - acc: 0.7134 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1332  | total loss: \u001b[1m\u001b[32m0.60711\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1332 | loss: 0.60711 - acc: 0.6983 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1333  | total loss: \u001b[1m\u001b[32m0.59653\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1333 | loss: 0.59653 - acc: 0.7076 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1334  | total loss: \u001b[1m\u001b[32m0.59947\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1334 | loss: 0.59947 - acc: 0.7048 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1335  | total loss: \u001b[1m\u001b[32m0.58914\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1335 | loss: 0.58914 - acc: 0.7131 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1336  | total loss: \u001b[1m\u001b[32m0.60576\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1336 | loss: 0.60576 - acc: 0.6977 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1337  | total loss: \u001b[1m\u001b[32m0.59457\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1337 | loss: 0.59457 - acc: 0.7072 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1338  | total loss: \u001b[1m\u001b[32m0.61026\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1338 | loss: 0.61026 - acc: 0.6915 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1339  | total loss: \u001b[1m\u001b[32m0.59886\u001b[0m\u001b[0m | time: 0.011s\n",
      "| Adam | epoch: 1339 | loss: 0.59886 - acc: 0.7031 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1340  | total loss: \u001b[1m\u001b[32m0.58869\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1340 | loss: 0.58869 - acc: 0.7138 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1341  | total loss: \u001b[1m\u001b[32m0.57937\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1341 | loss: 0.57937 - acc: 0.7234 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1342  | total loss: \u001b[1m\u001b[32m0.59681\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1342 | loss: 0.59681 - acc: 0.7047 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1343  | total loss: \u001b[1m\u001b[32m0.58639\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1343 | loss: 0.58639 - acc: 0.7151 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1344  | total loss: \u001b[1m\u001b[32m0.60145\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1344 | loss: 0.60145 - acc: 0.6994 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1345  | total loss: \u001b[1m\u001b[32m0.59072\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1345 | loss: 0.59072 - acc: 0.7104 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1346  | total loss: \u001b[1m\u001b[32m0.58110\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1346 | loss: 0.58110 - acc: 0.7202 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1347  | total loss: \u001b[1m\u001b[32m0.57219\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1347 | loss: 0.57219 - acc: 0.7292 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1348  | total loss: \u001b[1m\u001b[32m0.56368\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1348 | loss: 0.56368 - acc: 0.7374 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1349  | total loss: \u001b[1m\u001b[32m0.55534\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1349 | loss: 0.55534 - acc: 0.7451 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1350  | total loss: \u001b[1m\u001b[32m0.57939\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1350 | loss: 0.57939 - acc: 0.7245 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1351  | total loss: \u001b[1m\u001b[32m0.56854\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1351 | loss: 0.56854 - acc: 0.7341 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1352  | total loss: \u001b[1m\u001b[32m0.58997\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1352 | loss: 0.58997 - acc: 0.7164 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1353  | total loss: \u001b[1m\u001b[32m0.57802\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1353 | loss: 0.57802 - acc: 0.7261 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1354  | total loss: \u001b[1m\u001b[32m0.56729\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1354 | loss: 0.56729 - acc: 0.7350 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1355  | total loss: \u001b[1m\u001b[32m0.55744\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1355 | loss: 0.55744 - acc: 0.7427 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1356  | total loss: \u001b[1m\u001b[32m0.54823\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1356 | loss: 0.54823 - acc: 0.7499 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1357  | total loss: \u001b[1m\u001b[32m0.53951\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1357 | loss: 0.53951 - acc: 0.7564 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1358  | total loss: \u001b[1m\u001b[32m0.53121\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1358 | loss: 0.53121 - acc: 0.7624 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1359  | total loss: \u001b[1m\u001b[32m0.52331\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1359 | loss: 0.52331 - acc: 0.7676 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1360  | total loss: \u001b[1m\u001b[32m0.55994\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1360 | loss: 0.55994 - acc: 0.7438 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1361  | total loss: \u001b[1m\u001b[32m0.54882\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1361 | loss: 0.54882 - acc: 0.7504 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1362  | total loss: \u001b[1m\u001b[32m0.58156\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1362 | loss: 0.58156 - acc: 0.7279 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1363  | total loss: \u001b[1m\u001b[32m0.56844\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1363 | loss: 0.56844 - acc: 0.7353 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1364  | total loss: \u001b[1m\u001b[32m0.59810\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1364 | loss: 0.59810 - acc: 0.7151 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1365  | total loss: \u001b[1m\u001b[32m0.58452\u001b[0m\u001b[0m | time: 0.011s\n",
      "| Adam | epoch: 1365 | loss: 0.58452 - acc: 0.7243 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1366  | total loss: \u001b[1m\u001b[32m0.57319\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1366 | loss: 0.57319 - acc: 0.7324 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1367  | total loss: \u001b[1m\u001b[32m0.56342\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1367 | loss: 0.56342 - acc: 0.7397 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1368  | total loss: \u001b[1m\u001b[32m0.55452\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1368 | loss: 0.55452 - acc: 0.7463 -- iter: 891/891\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1369  | total loss: \u001b[1m\u001b[32m0.54603\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1369 | loss: 0.54603 - acc: 0.7523 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1370  | total loss: \u001b[1m\u001b[32m0.53772\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1370 | loss: 0.53772 - acc: 0.7577 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1371  | total loss: \u001b[1m\u001b[32m0.52962\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1371 | loss: 0.52962 - acc: 0.7630 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1372  | total loss: \u001b[1m\u001b[32m0.52189\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1372 | loss: 0.52189 - acc: 0.7677 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1373  | total loss: \u001b[1m\u001b[32m0.51474\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1373 | loss: 0.51474 - acc: 0.7722 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1374  | total loss: \u001b[1m\u001b[32m0.50833\u001b[0m\u001b[0m | time: 0.011s\n",
      "| Adam | epoch: 1374 | loss: 0.50833 - acc: 0.7761 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1375  | total loss: \u001b[1m\u001b[32m0.50266\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1375 | loss: 0.50266 - acc: 0.7800 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1376  | total loss: \u001b[1m\u001b[32m0.49766\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1376 | loss: 0.49766 - acc: 0.7828 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1377  | total loss: \u001b[1m\u001b[32m0.49323\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1377 | loss: 0.49323 - acc: 0.7844 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1378  | total loss: \u001b[1m\u001b[32m0.54119\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1378 | loss: 0.54119 - acc: 0.7607 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1379  | total loss: \u001b[1m\u001b[32m0.53201\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1379 | loss: 0.53201 - acc: 0.7644 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1380  | total loss: \u001b[1m\u001b[32m0.52337\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1380 | loss: 0.52337 - acc: 0.7679 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1381  | total loss: \u001b[1m\u001b[32m0.51538\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1381 | loss: 0.51538 - acc: 0.7718 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1382  | total loss: \u001b[1m\u001b[32m0.54835\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1382 | loss: 0.54835 - acc: 0.7531 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1383  | total loss: \u001b[1m\u001b[32m0.53871\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1383 | loss: 0.53871 - acc: 0.7590 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1384  | total loss: \u001b[1m\u001b[32m0.53097\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1384 | loss: 0.53097 - acc: 0.7637 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1385  | total loss: \u001b[1m\u001b[32m0.52470\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1385 | loss: 0.52470 - acc: 0.7673 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1386  | total loss: \u001b[1m\u001b[32m0.51924\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1386 | loss: 0.51924 - acc: 0.7707 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1387  | total loss: \u001b[1m\u001b[32m0.51397\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1387 | loss: 0.51397 - acc: 0.7738 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1388  | total loss: \u001b[1m\u001b[32m0.50855\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1388 | loss: 0.50855 - acc: 0.7766 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1389  | total loss: \u001b[1m\u001b[32m0.50302\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1389 | loss: 0.50302 - acc: 0.7804 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1390  | total loss: \u001b[1m\u001b[32m0.54344\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1390 | loss: 0.54344 - acc: 0.7538 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1391  | total loss: \u001b[1m\u001b[32m0.53418\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1391 | loss: 0.53418 - acc: 0.7596 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1392  | total loss: \u001b[1m\u001b[32m0.52581\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1392 | loss: 0.52581 - acc: 0.7646 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1393  | total loss: \u001b[1m\u001b[32m0.51817\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1393 | loss: 0.51817 - acc: 0.7689 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1394  | total loss: \u001b[1m\u001b[32m0.55248\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1394 | loss: 0.55248 - acc: 0.7472 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1395  | total loss: \u001b[1m\u001b[32m0.54247\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1395 | loss: 0.54247 - acc: 0.7523 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1396  | total loss: \u001b[1m\u001b[32m0.53359\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1396 | loss: 0.53359 - acc: 0.7570 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1397  | total loss: \u001b[1m\u001b[32m0.52548\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1397 | loss: 0.52548 - acc: 0.7622 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1398  | total loss: \u001b[1m\u001b[32m0.51811\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1398 | loss: 0.51811 - acc: 0.7670 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1399  | total loss: \u001b[1m\u001b[32m0.51152\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1399 | loss: 0.51152 - acc: 0.7715 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1400  | total loss: \u001b[1m\u001b[32m0.50549\u001b[0m\u001b[0m | time: 0.011s\n",
      "| Adam | epoch: 1400 | loss: 0.50549 - acc: 0.7754 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1401  | total loss: \u001b[1m\u001b[32m0.49977\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1401 | loss: 0.49977 - acc: 0.7791 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1402  | total loss: \u001b[1m\u001b[32m0.53827\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1402 | loss: 0.53827 - acc: 0.7562 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1403  | total loss: \u001b[1m\u001b[32m0.52930\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1403 | loss: 0.52930 - acc: 0.7609 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1404  | total loss: \u001b[1m\u001b[32m0.52135\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1404 | loss: 0.52135 - acc: 0.7649 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1405  | total loss: \u001b[1m\u001b[32m0.51406\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1405 | loss: 0.51406 - acc: 0.7689 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1406  | total loss: \u001b[1m\u001b[32m0.55219\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1406 | loss: 0.55219 - acc: 0.7457 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1407  | total loss: \u001b[1m\u001b[32m0.54233\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1407 | loss: 0.54233 - acc: 0.7520 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1408  | total loss: \u001b[1m\u001b[32m0.53375\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1408 | loss: 0.53375 - acc: 0.7580 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1409  | total loss: \u001b[1m\u001b[32m0.52595\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1409 | loss: 0.52595 - acc: 0.7631 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1410  | total loss: \u001b[1m\u001b[32m0.55406\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1410 | loss: 0.55406 - acc: 0.7430 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1411  | total loss: \u001b[1m\u001b[32m0.54456\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1411 | loss: 0.54456 - acc: 0.7491 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1412  | total loss: \u001b[1m\u001b[32m0.57255\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1412 | loss: 0.57255 - acc: 0.7266 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1413  | total loss: \u001b[1m\u001b[32m0.56201\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1413 | loss: 0.56201 - acc: 0.7343 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1414  | total loss: \u001b[1m\u001b[32m0.55291\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1414 | loss: 0.55291 - acc: 0.7418 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1415  | total loss: \u001b[1m\u001b[32m0.54477\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1415 | loss: 0.54477 - acc: 0.7484 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1416  | total loss: \u001b[1m\u001b[32m0.57021\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1416 | loss: 0.57021 - acc: 0.7268 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1417  | total loss: \u001b[1m\u001b[32m0.56042\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1417 | loss: 0.56042 - acc: 0.7349 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1418  | total loss: \u001b[1m\u001b[32m0.55167\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1418 | loss: 0.55167 - acc: 0.7428 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1419  | total loss: \u001b[1m\u001b[32m0.54363\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1419 | loss: 0.54363 - acc: 0.7496 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1420  | total loss: \u001b[1m\u001b[32m0.57221\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1420 | loss: 0.57221 - acc: 0.7260 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1421  | total loss: \u001b[1m\u001b[32m0.56213\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1421 | loss: 0.56213 - acc: 0.7344 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1422  | total loss: \u001b[1m\u001b[32m0.55315\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1422 | loss: 0.55315 - acc: 0.7422 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1423  | total loss: \u001b[1m\u001b[32m0.54497\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1423 | loss: 0.54497 - acc: 0.7496 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1424  | total loss: \u001b[1m\u001b[32m0.57019\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1424 | loss: 0.57019 - acc: 0.7284 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1425  | total loss: \u001b[1m\u001b[32m0.56037\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1425 | loss: 0.56037 - acc: 0.7368 -- iter: 891/891\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1426  | total loss: \u001b[1m\u001b[32m0.55163\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1426 | loss: 0.55163 - acc: 0.7444 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1427  | total loss: \u001b[1m\u001b[32m0.54368\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1427 | loss: 0.54368 - acc: 0.7515 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1428  | total loss: \u001b[1m\u001b[32m0.57207\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1428 | loss: 0.57207 - acc: 0.7287 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1429  | total loss: \u001b[1m\u001b[32m0.56211\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1429 | loss: 0.56211 - acc: 0.7373 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1430  | total loss: \u001b[1m\u001b[32m0.55324\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1430 | loss: 0.55324 - acc: 0.7448 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1431  | total loss: \u001b[1m\u001b[32m0.54513\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1431 | loss: 0.54513 - acc: 0.7516 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1432  | total loss: \u001b[1m\u001b[32m0.53751\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1432 | loss: 0.53751 - acc: 0.7578 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1433  | total loss: \u001b[1m\u001b[32m0.53022\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1433 | loss: 0.53022 - acc: 0.7636 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1434  | total loss: \u001b[1m\u001b[32m0.56105\u001b[0m\u001b[0m | time: 0.011s\n",
      "| Adam | epoch: 1434 | loss: 0.56105 - acc: 0.7400 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1435  | total loss: \u001b[1m\u001b[32m0.55092\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1435 | loss: 0.55092 - acc: 0.7474 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1436  | total loss: \u001b[1m\u001b[32m0.54167\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1436 | loss: 0.54167 - acc: 0.7537 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1437  | total loss: \u001b[1m\u001b[32m0.53313\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1437 | loss: 0.53313 - acc: 0.7595 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1438  | total loss: \u001b[1m\u001b[32m0.56574\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1438 | loss: 0.56574 - acc: 0.7363 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1439  | total loss: \u001b[1m\u001b[32m0.55471\u001b[0m\u001b[0m | time: 0.011s\n",
      "| Adam | epoch: 1439 | loss: 0.55471 - acc: 0.7437 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1440  | total loss: \u001b[1m\u001b[32m0.58403\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1440 | loss: 0.58403 - acc: 0.7222 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1441  | total loss: \u001b[1m\u001b[32m0.57191\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1441 | loss: 0.57191 - acc: 0.7311 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1442  | total loss: \u001b[1m\u001b[32m0.59680\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1442 | loss: 0.59680 - acc: 0.7098 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1443  | total loss: \u001b[1m\u001b[32m0.58503\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1443 | loss: 0.58503 - acc: 0.7195 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1444  | total loss: \u001b[1m\u001b[32m0.57509\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1444 | loss: 0.57509 - acc: 0.7283 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1445  | total loss: \u001b[1m\u001b[32m0.56622\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1445 | loss: 0.56622 - acc: 0.7360 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1446  | total loss: \u001b[1m\u001b[32m0.58711\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1446 | loss: 0.58711 - acc: 0.7166 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1447  | total loss: \u001b[1m\u001b[32m0.57684\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1447 | loss: 0.57684 - acc: 0.7257 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1448  | total loss: \u001b[1m\u001b[32m0.56740\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1448 | loss: 0.56740 - acc: 0.7343 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1449  | total loss: \u001b[1m\u001b[32m0.55842\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1449 | loss: 0.55842 - acc: 0.7419 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1450  | total loss: \u001b[1m\u001b[32m0.54974\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1450 | loss: 0.54974 - acc: 0.7490 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1451  | total loss: \u001b[1m\u001b[32m0.54141\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1451 | loss: 0.54141 - acc: 0.7558 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1452  | total loss: \u001b[1m\u001b[32m0.57063\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1452 | loss: 0.57063 - acc: 0.7331 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1453  | total loss: \u001b[1m\u001b[32m0.55987\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1453 | loss: 0.55987 - acc: 0.7412 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1454  | total loss: \u001b[1m\u001b[32m0.55015\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1454 | loss: 0.55015 - acc: 0.7483 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1455  | total loss: \u001b[1m\u001b[32m0.54135\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1455 | loss: 0.54135 - acc: 0.7547 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1456  | total loss: \u001b[1m\u001b[32m0.53330\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1456 | loss: 0.53330 - acc: 0.7605 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1457  | total loss: \u001b[1m\u001b[32m0.52585\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1457 | loss: 0.52585 - acc: 0.7657 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1458  | total loss: \u001b[1m\u001b[32m0.56052\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1458 | loss: 0.56052 - acc: 0.7437 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1459  | total loss: \u001b[1m\u001b[32m0.55008\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1459 | loss: 0.55008 - acc: 0.7508 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1460  | total loss: \u001b[1m\u001b[32m0.57786\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1460 | loss: 0.57786 - acc: 0.7295 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1461  | total loss: \u001b[1m\u001b[32m0.56593\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1461 | loss: 0.56593 - acc: 0.7381 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1462  | total loss: \u001b[1m\u001b[32m0.55558\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1462 | loss: 0.55558 - acc: 0.7456 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1463  | total loss: \u001b[1m\u001b[32m0.54657\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1463 | loss: 0.54657 - acc: 0.7523 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1464  | total loss: \u001b[1m\u001b[32m0.53859\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1464 | loss: 0.53859 - acc: 0.7583 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1465  | total loss: \u001b[1m\u001b[32m0.53130\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1465 | loss: 0.53130 - acc: 0.7638 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1466  | total loss: \u001b[1m\u001b[32m0.55819\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1466 | loss: 0.55819 - acc: 0.7435 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1467  | total loss: \u001b[1m\u001b[32m0.54889\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1467 | loss: 0.54889 - acc: 0.7504 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1468  | total loss: \u001b[1m\u001b[32m0.57297\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1468 | loss: 0.57297 - acc: 0.7302 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1469  | total loss: \u001b[1m\u001b[32m0.56283\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1469 | loss: 0.56283 - acc: 0.7385 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1470  | total loss: \u001b[1m\u001b[32m0.58213\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1470 | loss: 0.58213 - acc: 0.7202 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1471  | total loss: \u001b[1m\u001b[32m0.57207\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1471 | loss: 0.57207 - acc: 0.7294 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1472  | total loss: \u001b[1m\u001b[32m0.59142\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1472 | loss: 0.59142 - acc: 0.7096 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1473  | total loss: \u001b[1m\u001b[32m0.58145\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1473 | loss: 0.58145 - acc: 0.7197 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1474  | total loss: \u001b[1m\u001b[32m0.57273\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1474 | loss: 0.57273 - acc: 0.7288 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1475  | total loss: \u001b[1m\u001b[32m0.56472\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1475 | loss: 0.56472 - acc: 0.7367 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1476  | total loss: \u001b[1m\u001b[32m0.55700\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1476 | loss: 0.55700 - acc: 0.7441 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1477  | total loss: \u001b[1m\u001b[32m0.54937\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1477 | loss: 0.54937 - acc: 0.7508 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1478  | total loss: \u001b[1m\u001b[32m0.57077\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1478 | loss: 0.57077 - acc: 0.7328 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1479  | total loss: \u001b[1m\u001b[32m0.56094\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1479 | loss: 0.56094 - acc: 0.7410 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1480  | total loss: \u001b[1m\u001b[32m0.58446\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1480 | loss: 0.58446 - acc: 0.7211 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1481  | total loss: \u001b[1m\u001b[32m0.57327\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1481 | loss: 0.57327 - acc: 0.7303 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1482  | total loss: \u001b[1m\u001b[32m0.56326\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1482 | loss: 0.56326 - acc: 0.7388 -- iter: 891/891\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1483  | total loss: \u001b[1m\u001b[32m0.55415\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1483 | loss: 0.55415 - acc: 0.7465 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1484  | total loss: \u001b[1m\u001b[32m0.54570\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1484 | loss: 0.54570 - acc: 0.7533 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1485  | total loss: \u001b[1m\u001b[32m0.53773\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1485 | loss: 0.53773 - acc: 0.7595 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1486  | total loss: \u001b[1m\u001b[32m0.56775\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1486 | loss: 0.56775 - acc: 0.7361 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1487  | total loss: \u001b[1m\u001b[32m0.55720\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1487 | loss: 0.55720 - acc: 0.7442 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1488  | total loss: \u001b[1m\u001b[32m0.54764\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1488 | loss: 0.54764 - acc: 0.7511 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1489  | total loss: \u001b[1m\u001b[32m0.53888\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1489 | loss: 0.53888 - acc: 0.7576 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1490  | total loss: \u001b[1m\u001b[32m0.56724\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1490 | loss: 0.56724 - acc: 0.7353 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1491  | total loss: \u001b[1m\u001b[32m0.55648\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1491 | loss: 0.55648 - acc: 0.7428 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1492  | total loss: \u001b[1m\u001b[32m0.58218\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1492 | loss: 0.58218 - acc: 0.7211 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1493  | total loss: \u001b[1m\u001b[32m0.57083\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1493 | loss: 0.57083 - acc: 0.7303 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1494  | total loss: \u001b[1m\u001b[32m0.56117\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1494 | loss: 0.56117 - acc: 0.7378 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1495  | total loss: \u001b[1m\u001b[32m0.55266\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1495 | loss: 0.55266 - acc: 0.7450 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1496  | total loss: \u001b[1m\u001b[32m0.57190\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1496 | loss: 0.57190 - acc: 0.7276 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1497  | total loss: \u001b[1m\u001b[32m0.56249\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1497 | loss: 0.56249 - acc: 0.7358 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1498  | total loss: \u001b[1m\u001b[32m0.55396\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1498 | loss: 0.55396 - acc: 0.7432 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1499  | total loss: \u001b[1m\u001b[32m0.54585\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1499 | loss: 0.54585 - acc: 0.7497 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1500  | total loss: \u001b[1m\u001b[32m0.56879\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1500 | loss: 0.56879 - acc: 0.7304 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1501  | total loss: \u001b[1m\u001b[32m0.55860\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1501 | loss: 0.55860 - acc: 0.7384 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1502  | total loss: \u001b[1m\u001b[32m0.54926\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1502 | loss: 0.54926 - acc: 0.7457 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1503  | total loss: \u001b[1m\u001b[32m0.54056\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1503 | loss: 0.54056 - acc: 0.7524 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1504  | total loss: \u001b[1m\u001b[32m0.57054\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1504 | loss: 0.57054 - acc: 0.7301 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1505  | total loss: \u001b[1m\u001b[32m0.55943\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1505 | loss: 0.55943 - acc: 0.7386 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1506  | total loss: \u001b[1m\u001b[32m0.54939\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1506 | loss: 0.54939 - acc: 0.7462 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1507  | total loss: \u001b[1m\u001b[32m0.54022\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1507 | loss: 0.54022 - acc: 0.7531 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1508  | total loss: \u001b[1m\u001b[32m0.53176\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1508 | loss: 0.53176 - acc: 0.7592 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1509  | total loss: \u001b[1m\u001b[32m0.52390\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1509 | loss: 0.52390 - acc: 0.7645 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1510  | total loss: \u001b[1m\u001b[32m0.51661\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1510 | loss: 0.51661 - acc: 0.7691 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1511  | total loss: \u001b[1m\u001b[32m0.50985\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1511 | loss: 0.50985 - acc: 0.7733 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1512  | total loss: \u001b[1m\u001b[32m0.54973\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1512 | loss: 0.54973 - acc: 0.7495 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1513  | total loss: \u001b[1m\u001b[32m0.53945\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1513 | loss: 0.53945 - acc: 0.7552 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1514  | total loss: \u001b[1m\u001b[32m0.53017\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1514 | loss: 0.53017 - acc: 0.7602 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1515  | total loss: \u001b[1m\u001b[32m0.52178\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1515 | loss: 0.52178 - acc: 0.7652 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1516  | total loss: \u001b[1m\u001b[32m0.51421\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1516 | loss: 0.51421 - acc: 0.7695 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1517  | total loss: \u001b[1m\u001b[32m0.50737\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1517 | loss: 0.50737 - acc: 0.7735 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1518  | total loss: \u001b[1m\u001b[32m0.50114\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1518 | loss: 0.50114 - acc: 0.7773 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1519  | total loss: \u001b[1m\u001b[32m0.49542\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1519 | loss: 0.49542 - acc: 0.7800 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1520  | total loss: \u001b[1m\u001b[32m0.53201\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1520 | loss: 0.53201 - acc: 0.7578 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1521  | total loss: \u001b[1m\u001b[32m0.52334\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1521 | loss: 0.52334 - acc: 0.7621 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1522  | total loss: \u001b[1m\u001b[32m0.51567\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1522 | loss: 0.51567 - acc: 0.7661 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1523  | total loss: \u001b[1m\u001b[32m0.50878\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1523 | loss: 0.50878 - acc: 0.7694 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1524  | total loss: \u001b[1m\u001b[32m0.50254\u001b[0m\u001b[0m | time: 0.013s\n",
      "| Adam | epoch: 1524 | loss: 0.50254 - acc: 0.7732 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1525  | total loss: \u001b[1m\u001b[32m0.49681\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1525 | loss: 0.49681 - acc: 0.7769 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1526  | total loss: \u001b[1m\u001b[32m0.53618\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1526 | loss: 0.53618 - acc: 0.7555 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1527  | total loss: \u001b[1m\u001b[32m0.52707\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1527 | loss: 0.52707 - acc: 0.7605 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1528  | total loss: \u001b[1m\u001b[32m0.56379\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1528 | loss: 0.56379 - acc: 0.7374 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1529  | total loss: \u001b[1m\u001b[32m0.55263\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1529 | loss: 0.55263 - acc: 0.7439 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1530  | total loss: \u001b[1m\u001b[32m0.54305\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1530 | loss: 0.54305 - acc: 0.7498 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1531  | total loss: \u001b[1m\u001b[32m0.53466\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1531 | loss: 0.53466 - acc: 0.7557 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1532  | total loss: \u001b[1m\u001b[32m0.52710\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1532 | loss: 0.52710 - acc: 0.7614 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1533  | total loss: \u001b[1m\u001b[32m0.52009\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1533 | loss: 0.52009 - acc: 0.7662 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1534  | total loss: \u001b[1m\u001b[32m0.51350\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1534 | loss: 0.51350 - acc: 0.7706 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1535  | total loss: \u001b[1m\u001b[32m0.50728\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1535 | loss: 0.50728 - acc: 0.7745 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1536  | total loss: \u001b[1m\u001b[32m0.54324\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1536 | loss: 0.54324 - acc: 0.7500 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1537  | total loss: \u001b[1m\u001b[32m0.53390\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1537 | loss: 0.53390 - acc: 0.7559 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1538  | total loss: \u001b[1m\u001b[32m0.56869\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1538 | loss: 0.56869 - acc: 0.7335 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1539  | total loss: \u001b[1m\u001b[32m0.55712\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1539 | loss: 0.55712 - acc: 0.7411 -- iter: 891/891\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1540  | total loss: \u001b[1m\u001b[32m0.54697\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1540 | loss: 0.54697 - acc: 0.7480 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1541  | total loss: \u001b[1m\u001b[32m0.53800\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1541 | loss: 0.53800 - acc: 0.7544 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1542  | total loss: \u001b[1m\u001b[32m0.56733\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1542 | loss: 0.56733 - acc: 0.7310 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1543  | total loss: \u001b[1m\u001b[32m0.55702\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1543 | loss: 0.55702 - acc: 0.7389 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1544  | total loss: \u001b[1m\u001b[32m0.54821\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1544 | loss: 0.54821 - acc: 0.7464 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1545  | total loss: \u001b[1m\u001b[32m0.54041\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1545 | loss: 0.54041 - acc: 0.7530 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1546  | total loss: \u001b[1m\u001b[32m0.56482\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1546 | loss: 0.56482 - acc: 0.7322 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1547  | total loss: \u001b[1m\u001b[32m0.55568\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1547 | loss: 0.55568 - acc: 0.7400 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1548  | total loss: \u001b[1m\u001b[32m0.57678\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1548 | loss: 0.57678 - acc: 0.7198 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1549  | total loss: \u001b[1m\u001b[32m0.56722\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1549 | loss: 0.56722 - acc: 0.7289 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1550  | total loss: \u001b[1m\u001b[32m0.55884\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1550 | loss: 0.55884 - acc: 0.7373 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1551  | total loss: \u001b[1m\u001b[32m0.55115\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1551 | loss: 0.55115 - acc: 0.7448 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1552  | total loss: \u001b[1m\u001b[32m0.54380\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1552 | loss: 0.54380 - acc: 0.7513 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1553  | total loss: \u001b[1m\u001b[32m0.53657\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1553 | loss: 0.53657 - acc: 0.7574 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1554  | total loss: \u001b[1m\u001b[32m0.56675\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1554 | loss: 0.56675 - acc: 0.7336 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1555  | total loss: \u001b[1m\u001b[32m0.55658\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1555 | loss: 0.55658 - acc: 0.7414 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1556  | total loss: \u001b[1m\u001b[32m0.54728\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1556 | loss: 0.54728 - acc: 0.7483 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1557  | total loss: \u001b[1m\u001b[32m0.53869\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1557 | loss: 0.53869 - acc: 0.7545 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1558  | total loss: \u001b[1m\u001b[32m0.53068\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1558 | loss: 0.53068 - acc: 0.7599 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1559  | total loss: \u001b[1m\u001b[32m0.52319\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1559 | loss: 0.52319 - acc: 0.7649 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1560  | total loss: \u001b[1m\u001b[32m0.55615\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1560 | loss: 0.55615 - acc: 0.7424 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1561  | total loss: \u001b[1m\u001b[32m0.54584\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1561 | loss: 0.54584 - acc: 0.7492 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1562  | total loss: \u001b[1m\u001b[32m0.53651\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1562 | loss: 0.53651 - acc: 0.7555 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1563  | total loss: \u001b[1m\u001b[32m0.52803\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1563 | loss: 0.52803 - acc: 0.7611 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1564  | total loss: \u001b[1m\u001b[32m0.52027\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1564 | loss: 0.52027 - acc: 0.7661 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1565  | total loss: \u001b[1m\u001b[32m0.51313\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1565 | loss: 0.51313 - acc: 0.7707 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1566  | total loss: \u001b[1m\u001b[32m0.55145\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1566 | loss: 0.55145 - acc: 0.7477 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1567  | total loss: \u001b[1m\u001b[32m0.54120\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1567 | loss: 0.54120 - acc: 0.7538 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1568  | total loss: \u001b[1m\u001b[32m0.53216\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1568 | loss: 0.53216 - acc: 0.7596 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1569  | total loss: \u001b[1m\u001b[32m0.52411\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1569 | loss: 0.52411 - acc: 0.7646 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1570  | total loss: \u001b[1m\u001b[32m0.51682\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1570 | loss: 0.51682 - acc: 0.7684 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1571  | total loss: \u001b[1m\u001b[32m0.51009\u001b[0m\u001b[0m | time: 0.012s\n",
      "| Adam | epoch: 1571 | loss: 0.51009 - acc: 0.7719 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1572  | total loss: \u001b[1m\u001b[32m0.50377\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1572 | loss: 0.50377 - acc: 0.7751 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1573  | total loss: \u001b[1m\u001b[32m0.49784\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1573 | loss: 0.49784 - acc: 0.7780 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1574  | total loss: \u001b[1m\u001b[32m0.53846\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1574 | loss: 0.53846 - acc: 0.7561 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1575  | total loss: \u001b[1m\u001b[32m0.52894\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1575 | loss: 0.52894 - acc: 0.7614 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1576  | total loss: \u001b[1m\u001b[32m0.52038\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1576 | loss: 0.52038 - acc: 0.7663 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1577  | total loss: \u001b[1m\u001b[32m0.51265\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1577 | loss: 0.51265 - acc: 0.7706 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1578  | total loss: \u001b[1m\u001b[32m0.50568\u001b[0m\u001b[0m | time: 0.011s\n",
      "| Adam | epoch: 1578 | loss: 0.50568 - acc: 0.7739 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1579  | total loss: \u001b[1m\u001b[32m0.49937\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1579 | loss: 0.49937 - acc: 0.7766 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1580  | total loss: \u001b[1m\u001b[32m0.53813\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1580 | loss: 0.53813 - acc: 0.7542 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1581  | total loss: \u001b[1m\u001b[32m0.52864\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1581 | loss: 0.52864 - acc: 0.7594 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1582  | total loss: \u001b[1m\u001b[32m0.56639\u001b[0m\u001b[0m | time: 0.012s\n",
      "| Adam | epoch: 1582 | loss: 0.56639 - acc: 0.7363 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1583  | total loss: \u001b[1m\u001b[32m0.55483\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1583 | loss: 0.55483 - acc: 0.7432 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1584  | total loss: \u001b[1m\u001b[32m0.58415\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1584 | loss: 0.58415 - acc: 0.7223 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1585  | total loss: \u001b[1m\u001b[32m0.57283\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1585 | loss: 0.57283 - acc: 0.7299 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1586  | total loss: \u001b[1m\u001b[32m0.59406\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1586 | loss: 0.59406 - acc: 0.7111 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1587  | total loss: \u001b[1m\u001b[32m0.58423\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1587 | loss: 0.58423 - acc: 0.7197 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1588  | total loss: \u001b[1m\u001b[32m0.57608\u001b[0m\u001b[0m | time: 0.011s\n",
      "| Adam | epoch: 1588 | loss: 0.57608 - acc: 0.7271 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1589  | total loss: \u001b[1m\u001b[32m0.56878\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1589 | loss: 0.56878 - acc: 0.7345 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1590  | total loss: \u001b[1m\u001b[32m0.56177\u001b[0m\u001b[0m | time: 0.011s\n",
      "| Adam | epoch: 1590 | loss: 0.56177 - acc: 0.7411 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1591  | total loss: \u001b[1m\u001b[32m0.55472\u001b[0m\u001b[0m | time: 0.011s\n",
      "| Adam | epoch: 1591 | loss: 0.55472 - acc: 0.7478 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1592  | total loss: \u001b[1m\u001b[32m0.57710\u001b[0m\u001b[0m | time: 0.011s\n",
      "| Adam | epoch: 1592 | loss: 0.57710 - acc: 0.7266 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1593  | total loss: \u001b[1m\u001b[32m0.56759\u001b[0m\u001b[0m | time: 0.011s\n",
      "| Adam | epoch: 1593 | loss: 0.56759 - acc: 0.7351 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1594  | total loss: \u001b[1m\u001b[32m0.58849\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1594 | loss: 0.58849 - acc: 0.7153 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1595  | total loss: \u001b[1m\u001b[32m0.57795\u001b[0m\u001b[0m | time: 0.013s\n",
      "| Adam | epoch: 1595 | loss: 0.57795 - acc: 0.7243 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1596  | total loss: \u001b[1m\u001b[32m0.59703\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1596 | loss: 0.59703 - acc: 0.7056 -- iter: 891/891\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1597  | total loss: \u001b[1m\u001b[32m0.58624\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1597 | loss: 0.58624 - acc: 0.7156 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1598  | total loss: \u001b[1m\u001b[32m0.57679\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1598 | loss: 0.57679 - acc: 0.7244 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1599  | total loss: \u001b[1m\u001b[32m0.56828\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1599 | loss: 0.56828 - acc: 0.7326 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1600  | total loss: \u001b[1m\u001b[32m0.58632\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1600 | loss: 0.58632 - acc: 0.7142 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1601  | total loss: \u001b[1m\u001b[32m0.57686\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1601 | loss: 0.57686 - acc: 0.7232 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1602  | total loss: \u001b[1m\u001b[32m0.59479\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1602 | loss: 0.59479 - acc: 0.7046 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1603  | total loss: \u001b[1m\u001b[32m0.58485\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1603 | loss: 0.58485 - acc: 0.7144 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1604  | total loss: \u001b[1m\u001b[32m0.57606\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1604 | loss: 0.57606 - acc: 0.7237 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1605  | total loss: \u001b[1m\u001b[32m0.56802\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1605 | loss: 0.56802 - acc: 0.7320 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1606  | total loss: \u001b[1m\u001b[32m0.56044\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1606 | loss: 0.56044 - acc: 0.7396 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1607  | total loss: \u001b[1m\u001b[32m0.55307\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1607 | loss: 0.55307 - acc: 0.7464 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1608  | total loss: \u001b[1m\u001b[32m0.57407\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1608 | loss: 0.57407 - acc: 0.7289 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1609  | total loss: \u001b[1m\u001b[32m0.56452\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1609 | loss: 0.56452 - acc: 0.7376 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1610  | total loss: \u001b[1m\u001b[32m0.58390\u001b[0m\u001b[0m | time: 0.011s\n",
      "| Adam | epoch: 1610 | loss: 0.58390 - acc: 0.7193 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1611  | total loss: \u001b[1m\u001b[32m0.57321\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1611 | loss: 0.57321 - acc: 0.7289 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1612  | total loss: \u001b[1m\u001b[32m0.59434\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1612 | loss: 0.59434 - acc: 0.7110 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1613  | total loss: \u001b[1m\u001b[32m0.58307\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1613 | loss: 0.58307 - acc: 0.7214 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1614  | total loss: \u001b[1m\u001b[32m0.57316\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1614 | loss: 0.57316 - acc: 0.7304 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1615  | total loss: \u001b[1m\u001b[32m0.56416\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1615 | loss: 0.56416 - acc: 0.7388 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1616  | total loss: \u001b[1m\u001b[32m0.55569\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1616 | loss: 0.55569 - acc: 0.7464 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1617  | total loss: \u001b[1m\u001b[32m0.54748\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1617 | loss: 0.54748 - acc: 0.7532 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1618  | total loss: \u001b[1m\u001b[32m0.57397\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1618 | loss: 0.57397 - acc: 0.7318 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1619  | total loss: \u001b[1m\u001b[32m0.56319\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1619 | loss: 0.56319 - acc: 0.7402 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1620  | total loss: \u001b[1m\u001b[32m0.58520\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1620 | loss: 0.58520 - acc: 0.7216 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1621  | total loss: \u001b[1m\u001b[32m0.57348\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1621 | loss: 0.57348 - acc: 0.7306 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1622  | total loss: \u001b[1m\u001b[32m0.59516\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1622 | loss: 0.59516 - acc: 0.7130 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1623  | total loss: \u001b[1m\u001b[32m0.58306\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1623 | loss: 0.58306 - acc: 0.7232 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1624  | total loss: \u001b[1m\u001b[32m0.57245\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1624 | loss: 0.57245 - acc: 0.7321 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1625  | total loss: \u001b[1m\u001b[32m0.56292\u001b[0m\u001b[0m | time: 0.011s\n",
      "| Adam | epoch: 1625 | loss: 0.56292 - acc: 0.7402 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1626  | total loss: \u001b[1m\u001b[32m0.58303\u001b[0m\u001b[0m | time: 0.011s\n",
      "| Adam | epoch: 1626 | loss: 0.58303 - acc: 0.7224 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1627  | total loss: \u001b[1m\u001b[32m0.57235\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1627 | loss: 0.57235 - acc: 0.7315 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1628  | total loss: \u001b[1m\u001b[32m0.59153\u001b[0m\u001b[0m | time: 0.012s\n",
      "| Adam | epoch: 1628 | loss: 0.59153 - acc: 0.7128 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1629  | total loss: \u001b[1m\u001b[32m0.58041\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1629 | loss: 0.58041 - acc: 0.7230 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1630  | total loss: \u001b[1m\u001b[32m0.59812\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1630 | loss: 0.59812 - acc: 0.7051 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1631  | total loss: \u001b[1m\u001b[32m0.58714\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1631 | loss: 0.58714 - acc: 0.7159 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1632  | total loss: \u001b[1m\u001b[32m0.57748\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1632 | loss: 0.57748 - acc: 0.7254 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1633  | total loss: \u001b[1m\u001b[32m0.56865\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1633 | loss: 0.56865 - acc: 0.7341 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1634  | total loss: \u001b[1m\u001b[32m0.58969\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1634 | loss: 0.58969 - acc: 0.7122 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1635  | total loss: \u001b[1m\u001b[32m0.57947\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1635 | loss: 0.57947 - acc: 0.7227 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1636  | total loss: \u001b[1m\u001b[32m0.59505\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1636 | loss: 0.59505 - acc: 0.7067 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1637  | total loss: \u001b[1m\u001b[32m0.58452\u001b[0m\u001b[0m | time: 0.011s\n",
      "| Adam | epoch: 1637 | loss: 0.58452 - acc: 0.7175 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1638  | total loss: \u001b[1m\u001b[32m0.60114\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1638 | loss: 0.60114 - acc: 0.7007 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1639  | total loss: \u001b[1m\u001b[32m0.59049\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1639 | loss: 0.59049 - acc: 0.7117 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1640  | total loss: \u001b[1m\u001b[32m0.60577\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1640 | loss: 0.60577 - acc: 0.6946 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1641  | total loss: \u001b[1m\u001b[32m0.59533\u001b[0m\u001b[0m | time: 0.014s\n",
      "| Adam | epoch: 1641 | loss: 0.59533 - acc: 0.7063 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1642  | total loss: \u001b[1m\u001b[32m0.58610\u001b[0m\u001b[0m | time: 0.011s\n",
      "| Adam | epoch: 1642 | loss: 0.58610 - acc: 0.7168 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1643  | total loss: \u001b[1m\u001b[32m0.57760\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1643 | loss: 0.57760 - acc: 0.7266 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1644  | total loss: \u001b[1m\u001b[32m0.56948\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1644 | loss: 0.56948 - acc: 0.7354 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1645  | total loss: \u001b[1m\u001b[32m0.56150\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1645 | loss: 0.56150 - acc: 0.7428 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1646  | total loss: \u001b[1m\u001b[32m0.58019\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1646 | loss: 0.58019 - acc: 0.7247 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1647  | total loss: \u001b[1m\u001b[32m0.57012\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1647 | loss: 0.57012 - acc: 0.7328 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1648  | total loss: \u001b[1m\u001b[32m0.56065\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1648 | loss: 0.56065 - acc: 0.7411 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1649  | total loss: \u001b[1m\u001b[32m0.55167\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1649 | loss: 0.55167 - acc: 0.7487 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1650  | total loss: \u001b[1m\u001b[32m0.54310\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1650 | loss: 0.54310 - acc: 0.7554 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1651  | total loss: \u001b[1m\u001b[32m0.53494\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1651 | loss: 0.53494 - acc: 0.7615 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1652  | total loss: \u001b[1m\u001b[32m0.56642\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1652 | loss: 0.56642 - acc: 0.7394 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1653  | total loss: \u001b[1m\u001b[32m0.55539\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1653 | loss: 0.55539 - acc: 0.7468 -- iter: 891/891\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1654  | total loss: \u001b[1m\u001b[32m0.54528\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1654 | loss: 0.54528 - acc: 0.7532 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1655  | total loss: \u001b[1m\u001b[32m0.53597\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1655 | loss: 0.53597 - acc: 0.7592 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1656  | total loss: \u001b[1m\u001b[32m0.52739\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1656 | loss: 0.52739 - acc: 0.7645 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1657  | total loss: \u001b[1m\u001b[32m0.51947\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1657 | loss: 0.51947 - acc: 0.7692 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1658  | total loss: \u001b[1m\u001b[32m0.51218\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1658 | loss: 0.51218 - acc: 0.7738 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1659  | total loss: \u001b[1m\u001b[32m0.50546\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1659 | loss: 0.50546 - acc: 0.7778 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1660  | total loss: \u001b[1m\u001b[32m0.54662\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1660 | loss: 0.54662 - acc: 0.7542 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1661  | total loss: \u001b[1m\u001b[32m0.53651\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1661 | loss: 0.53651 - acc: 0.7593 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1662  | total loss: \u001b[1m\u001b[32m0.52763\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1662 | loss: 0.52763 - acc: 0.7636 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1663  | total loss: \u001b[1m\u001b[32m0.51979\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1663 | loss: 0.51979 - acc: 0.7678 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1664  | total loss: \u001b[1m\u001b[32m0.55541\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1664 | loss: 0.55541 - acc: 0.7465 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1665  | total loss: \u001b[1m\u001b[32m0.54539\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1665 | loss: 0.54539 - acc: 0.7522 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1666  | total loss: \u001b[1m\u001b[32m0.53664\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1666 | loss: 0.53664 - acc: 0.7573 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1667  | total loss: \u001b[1m\u001b[32m0.52873\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1667 | loss: 0.52873 - acc: 0.7615 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1668  | total loss: \u001b[1m\u001b[32m0.55659\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1668 | loss: 0.55659 - acc: 0.7410 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1669  | total loss: \u001b[1m\u001b[32m0.54682\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1669 | loss: 0.54682 - acc: 0.7471 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1670  | total loss: \u001b[1m\u001b[32m0.53818\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1670 | loss: 0.53818 - acc: 0.7529 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1671  | total loss: \u001b[1m\u001b[32m0.53039\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1671 | loss: 0.53039 - acc: 0.7587 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1672  | total loss: \u001b[1m\u001b[32m0.56137\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1672 | loss: 0.56137 - acc: 0.7370 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1673  | total loss: \u001b[1m\u001b[32m0.55154\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1673 | loss: 0.55154 - acc: 0.7444 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1674  | total loss: \u001b[1m\u001b[32m0.57790\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1674 | loss: 0.57790 - acc: 0.7227 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1675  | total loss: \u001b[1m\u001b[32m0.56741\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1675 | loss: 0.56741 - acc: 0.7315 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1676  | total loss: \u001b[1m\u001b[32m0.55853\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1676 | loss: 0.55853 - acc: 0.7395 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1677  | total loss: \u001b[1m\u001b[32m0.55077\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1677 | loss: 0.55077 - acc: 0.7467 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1678  | total loss: \u001b[1m\u001b[32m0.54372\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1678 | loss: 0.54372 - acc: 0.7531 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1679  | total loss: \u001b[1m\u001b[32m0.53707\u001b[0m\u001b[0m | time: 0.012s\n",
      "| Adam | epoch: 1679 | loss: 0.53707 - acc: 0.7587 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1680  | total loss: \u001b[1m\u001b[32m0.53066\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1680 | loss: 0.53066 - acc: 0.7642 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1681  | total loss: \u001b[1m\u001b[32m0.52440\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1681 | loss: 0.52440 - acc: 0.7691 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1682  | total loss: \u001b[1m\u001b[32m0.51826\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1682 | loss: 0.51826 - acc: 0.7736 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1683  | total loss: \u001b[1m\u001b[32m0.51230\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1683 | loss: 0.51230 - acc: 0.7774 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1684  | total loss: \u001b[1m\u001b[32m0.54626\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1684 | loss: 0.54626 - acc: 0.7548 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1685  | total loss: \u001b[1m\u001b[32m0.53709\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1685 | loss: 0.53709 - acc: 0.7599 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1686  | total loss: \u001b[1m\u001b[32m0.52868\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1686 | loss: 0.52868 - acc: 0.7642 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1687  | total loss: \u001b[1m\u001b[32m0.52090\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1687 | loss: 0.52090 - acc: 0.7683 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1688  | total loss: \u001b[1m\u001b[32m0.51367\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1688 | loss: 0.51367 - acc: 0.7723 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1689  | total loss: \u001b[1m\u001b[32m0.50700\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1689 | loss: 0.50700 - acc: 0.7760 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1690  | total loss: \u001b[1m\u001b[32m0.50083\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1690 | loss: 0.50083 - acc: 0.7795 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1691  | total loss: \u001b[1m\u001b[32m0.49512\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1691 | loss: 0.49512 - acc: 0.7825 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1692  | total loss: \u001b[1m\u001b[32m0.48987\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1692 | loss: 0.48987 - acc: 0.7847 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1693  | total loss: \u001b[1m\u001b[32m0.48511\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1693 | loss: 0.48511 - acc: 0.7866 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1694  | total loss: \u001b[1m\u001b[32m0.53414\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1694 | loss: 0.53414 - acc: 0.7602 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1695  | total loss: \u001b[1m\u001b[32m0.52515\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1695 | loss: 0.52515 - acc: 0.7646 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1696  | total loss: \u001b[1m\u001b[32m0.56792\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1696 | loss: 0.56792 - acc: 0.7403 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1697  | total loss: \u001b[1m\u001b[32m0.55681\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1697 | loss: 0.55681 - acc: 0.7465 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1698  | total loss: \u001b[1m\u001b[32m0.54750\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1698 | loss: 0.54750 - acc: 0.7519 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1699  | total loss: \u001b[1m\u001b[32m0.53922\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1699 | loss: 0.53922 - acc: 0.7568 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1700  | total loss: \u001b[1m\u001b[32m0.57032\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1700 | loss: 0.57032 - acc: 0.7346 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1701  | total loss: \u001b[1m\u001b[32m0.55988\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1701 | loss: 0.55988 - acc: 0.7410 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1702  | total loss: \u001b[1m\u001b[32m0.58369\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1702 | loss: 0.58369 - acc: 0.7220 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1703  | total loss: \u001b[1m\u001b[32m0.57282\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1703 | loss: 0.57282 - acc: 0.7304 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1704  | total loss: \u001b[1m\u001b[32m0.56345\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1704 | loss: 0.56345 - acc: 0.7381 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1705  | total loss: \u001b[1m\u001b[32m0.55509\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1705 | loss: 0.55509 - acc: 0.7452 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1706  | total loss: \u001b[1m\u001b[32m0.57731\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1706 | loss: 0.57731 - acc: 0.7250 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1707  | total loss: \u001b[1m\u001b[32m0.56781\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1707 | loss: 0.56781 - acc: 0.7335 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1708  | total loss: \u001b[1m\u001b[32m0.55941\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1708 | loss: 0.55941 - acc: 0.7414 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1709  | total loss: \u001b[1m\u001b[32m0.55175\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1709 | loss: 0.55175 - acc: 0.7480 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1710  | total loss: \u001b[1m\u001b[32m0.57568\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1710 | loss: 0.57568 - acc: 0.7259 -- iter: 891/891\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1711  | total loss: \u001b[1m\u001b[32m0.56640\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1711 | loss: 0.56640 - acc: 0.7339 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1712  | total loss: \u001b[1m\u001b[32m0.55809\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1712 | loss: 0.55809 - acc: 0.7411 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1713  | total loss: \u001b[1m\u001b[32m0.55045\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1713 | loss: 0.55045 - acc: 0.7476 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1714  | total loss: \u001b[1m\u001b[32m0.57507\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1714 | loss: 0.57507 - acc: 0.7266 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1715  | total loss: \u001b[1m\u001b[32m0.56561\u001b[0m\u001b[0m | time: 0.011s\n",
      "| Adam | epoch: 1715 | loss: 0.56561 - acc: 0.7345 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1716  | total loss: \u001b[1m\u001b[32m0.55710\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1716 | loss: 0.55710 - acc: 0.7421 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1717  | total loss: \u001b[1m\u001b[32m0.54925\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1717 | loss: 0.54925 - acc: 0.7491 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1718  | total loss: \u001b[1m\u001b[32m0.54185\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1718 | loss: 0.54185 - acc: 0.7556 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1719  | total loss: \u001b[1m\u001b[32m0.53474\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1719 | loss: 0.53474 - acc: 0.7615 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1720  | total loss: \u001b[1m\u001b[32m0.52783\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1720 | loss: 0.52783 - acc: 0.7670 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1721  | total loss: \u001b[1m\u001b[32m0.52108\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1721 | loss: 0.52108 - acc: 0.7716 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1722  | total loss: \u001b[1m\u001b[32m0.51453\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1722 | loss: 0.51453 - acc: 0.7760 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1723  | total loss: \u001b[1m\u001b[32m0.50824\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1723 | loss: 0.50824 - acc: 0.7796 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1724  | total loss: \u001b[1m\u001b[32m0.54979\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1724 | loss: 0.54979 - acc: 0.7550 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1725  | total loss: \u001b[1m\u001b[32m0.53957\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1725 | loss: 0.53957 - acc: 0.7605 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1726  | total loss: \u001b[1m\u001b[32m0.57708\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1726 | loss: 0.57708 - acc: 0.7378 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1727  | total loss: \u001b[1m\u001b[32m0.56406\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1727 | loss: 0.56406 - acc: 0.7451 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1728  | total loss: \u001b[1m\u001b[32m0.55260\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1728 | loss: 0.55260 - acc: 0.7520 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1729  | total loss: \u001b[1m\u001b[32m0.54255\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1729 | loss: 0.54255 - acc: 0.7583 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1730  | total loss: \u001b[1m\u001b[32m0.57705\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1730 | loss: 0.57705 - acc: 0.7342 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1731  | total loss: \u001b[1m\u001b[32m0.56540\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1731 | loss: 0.56540 - acc: 0.7412 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1732  | total loss: \u001b[1m\u001b[32m0.59047\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1732 | loss: 0.59047 - acc: 0.7218 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1733  | total loss: \u001b[1m\u001b[32m0.57908\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1733 | loss: 0.57908 - acc: 0.7295 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1734  | total loss: \u001b[1m\u001b[32m0.59833\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1734 | loss: 0.59833 - acc: 0.7109 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1735  | total loss: \u001b[1m\u001b[32m0.58771\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1735 | loss: 0.58771 - acc: 0.7196 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1736  | total loss: \u001b[1m\u001b[32m0.60548\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1736 | loss: 0.60548 - acc: 0.7005 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1737  | total loss: \u001b[1m\u001b[32m0.59536\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1737 | loss: 0.59536 - acc: 0.7108 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1738  | total loss: \u001b[1m\u001b[32m0.58648\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1738 | loss: 0.58648 - acc: 0.7201 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1739  | total loss: \u001b[1m\u001b[32m0.57822\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1739 | loss: 0.57822 - acc: 0.7289 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1740  | total loss: \u001b[1m\u001b[32m0.57020\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1740 | loss: 0.57020 - acc: 0.7368 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1741  | total loss: \u001b[1m\u001b[32m0.56223\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1741 | loss: 0.56223 - acc: 0.7437 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1742  | total loss: \u001b[1m\u001b[32m0.55426\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1742 | loss: 0.55426 - acc: 0.7497 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1743  | total loss: \u001b[1m\u001b[32m0.54637\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1743 | loss: 0.54637 - acc: 0.7554 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1744  | total loss: \u001b[1m\u001b[32m0.57204\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1744 | loss: 0.57204 - acc: 0.7337 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1745  | total loss: \u001b[1m\u001b[32m0.56183\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1745 | loss: 0.56183 - acc: 0.7407 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1746  | total loss: \u001b[1m\u001b[32m0.59151\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1746 | loss: 0.59151 - acc: 0.7193 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1747  | total loss: \u001b[1m\u001b[32m0.57953\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1747 | loss: 0.57953 - acc: 0.7282 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1748  | total loss: \u001b[1m\u001b[32m0.60851\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1748 | loss: 0.60851 - acc: 0.7046 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1749  | total loss: \u001b[1m\u001b[32m0.59518\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1749 | loss: 0.59518 - acc: 0.7153 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1750  | total loss: \u001b[1m\u001b[32m0.58340\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1750 | loss: 0.58340 - acc: 0.7248 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1751  | total loss: \u001b[1m\u001b[32m0.57294\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1751 | loss: 0.57294 - acc: 0.7328 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1752  | total loss: \u001b[1m\u001b[32m0.56360\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1752 | loss: 0.56360 - acc: 0.7403 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1753  | total loss: \u001b[1m\u001b[32m0.55509\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1753 | loss: 0.55509 - acc: 0.7468 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1754  | total loss: \u001b[1m\u001b[32m0.57816\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1754 | loss: 0.57816 - acc: 0.7275 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1755  | total loss: \u001b[1m\u001b[32m0.56798\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1755 | loss: 0.56798 - acc: 0.7360 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1756  | total loss: \u001b[1m\u001b[32m0.58771\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1756 | loss: 0.58771 - acc: 0.7165 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1757  | total loss: \u001b[1m\u001b[32m0.57744\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1757 | loss: 0.57744 - acc: 0.7262 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1758  | total loss: \u001b[1m\u001b[32m0.59649\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1758 | loss: 0.59649 - acc: 0.7061 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1759  | total loss: \u001b[1m\u001b[32m0.58664\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1759 | loss: 0.58664 - acc: 0.7168 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1760  | total loss: \u001b[1m\u001b[32m0.57821\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1760 | loss: 0.57821 - acc: 0.7257 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1761  | total loss: \u001b[1m\u001b[32m0.57049\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1761 | loss: 0.57049 - acc: 0.7342 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1762  | total loss: \u001b[1m\u001b[32m0.56290\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1762 | loss: 0.56290 - acc: 0.7417 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1763  | total loss: \u001b[1m\u001b[32m0.55506\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1763 | loss: 0.55506 - acc: 0.7486 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1764  | total loss: \u001b[1m\u001b[32m0.58027\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1764 | loss: 0.58027 - acc: 0.7257 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1765  | total loss: \u001b[1m\u001b[32m0.56940\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1765 | loss: 0.56940 - acc: 0.7344 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1766  | total loss: \u001b[1m\u001b[32m0.55932\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1766 | loss: 0.55932 - acc: 0.7424 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1767  | total loss: \u001b[1m\u001b[32m0.54986\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1767 | loss: 0.54986 - acc: 0.7491 -- iter: 891/891\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1768  | total loss: \u001b[1m\u001b[32m0.54094\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1768 | loss: 0.54094 - acc: 0.7556 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1769  | total loss: \u001b[1m\u001b[32m0.53258\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1769 | loss: 0.53258 - acc: 0.7615 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1770  | total loss: \u001b[1m\u001b[32m0.52484\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1770 | loss: 0.52484 - acc: 0.7665 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1771  | total loss: \u001b[1m\u001b[32m0.51773\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1771 | loss: 0.51773 - acc: 0.7711 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1772  | total loss: \u001b[1m\u001b[32m0.51119\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1772 | loss: 0.51119 - acc: 0.7754 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1773  | total loss: \u001b[1m\u001b[32m0.50521\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1773 | loss: 0.50521 - acc: 0.7788 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1774  | total loss: \u001b[1m\u001b[32m0.49975\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1774 | loss: 0.49975 - acc: 0.7811 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1775  | total loss: \u001b[1m\u001b[32m0.49469\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1775 | loss: 0.49469 - acc: 0.7832 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1776  | total loss: \u001b[1m\u001b[32m0.54356\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1776 | loss: 0.54356 - acc: 0.7611 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1777  | total loss: \u001b[1m\u001b[32m0.53359\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1777 | loss: 0.53359 - acc: 0.7657 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1778  | total loss: \u001b[1m\u001b[32m0.52456\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1778 | loss: 0.52456 - acc: 0.7701 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1779  | total loss: \u001b[1m\u001b[32m0.51664\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1779 | loss: 0.51664 - acc: 0.7744 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1780  | total loss: \u001b[1m\u001b[32m0.55514\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1780 | loss: 0.55514 - acc: 0.7525 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1781  | total loss: \u001b[1m\u001b[32m0.54542\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1781 | loss: 0.54542 - acc: 0.7578 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1782  | total loss: \u001b[1m\u001b[32m0.53740\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1782 | loss: 0.53740 - acc: 0.7622 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1783  | total loss: \u001b[1m\u001b[32m0.53023\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1783 | loss: 0.53023 - acc: 0.7662 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1784  | total loss: \u001b[1m\u001b[32m0.56059\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1784 | loss: 0.56059 - acc: 0.7433 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1785  | total loss: \u001b[1m\u001b[32m0.55095\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1785 | loss: 0.55095 - acc: 0.7497 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1786  | total loss: \u001b[1m\u001b[32m0.57709\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1786 | loss: 0.57709 - acc: 0.7282 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1787  | total loss: \u001b[1m\u001b[32m0.56633\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1787 | loss: 0.56633 - acc: 0.7363 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1788  | total loss: \u001b[1m\u001b[32m0.55694\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1788 | loss: 0.55694 - acc: 0.7439 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1789  | total loss: \u001b[1m\u001b[32m0.54851\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1789 | loss: 0.54851 - acc: 0.7505 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1790  | total loss: \u001b[1m\u001b[32m0.56930\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1790 | loss: 0.56930 - acc: 0.7308 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1791  | total loss: \u001b[1m\u001b[32m0.55983\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1791 | loss: 0.55983 - acc: 0.7389 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1792  | total loss: \u001b[1m\u001b[32m0.55142\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1792 | loss: 0.55142 - acc: 0.7464 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1793  | total loss: \u001b[1m\u001b[32m0.54380\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1793 | loss: 0.54380 - acc: 0.7526 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1794  | total loss: \u001b[1m\u001b[32m0.53676\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1794 | loss: 0.53676 - acc: 0.7580 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1795  | total loss: \u001b[1m\u001b[32m0.53016\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1795 | loss: 0.53016 - acc: 0.7629 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1796  | total loss: \u001b[1m\u001b[32m0.52391\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1796 | loss: 0.52391 - acc: 0.7669 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1797  | total loss: \u001b[1m\u001b[32m0.51796\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1797 | loss: 0.51796 - acc: 0.7708 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1798  | total loss: \u001b[1m\u001b[32m0.51232\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1798 | loss: 0.51232 - acc: 0.7747 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1799  | total loss: \u001b[1m\u001b[32m0.50702\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1799 | loss: 0.50702 - acc: 0.7785 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1800  | total loss: \u001b[1m\u001b[32m0.50204\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1800 | loss: 0.50204 - acc: 0.7812 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1801  | total loss: \u001b[1m\u001b[32m0.49732\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1801 | loss: 0.49732 - acc: 0.7840 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1802  | total loss: \u001b[1m\u001b[32m0.49285\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1802 | loss: 0.49285 - acc: 0.7869 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1803  | total loss: \u001b[1m\u001b[32m0.48858\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1803 | loss: 0.48858 - acc: 0.7896 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1804  | total loss: \u001b[1m\u001b[32m0.48446\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1804 | loss: 0.48446 - acc: 0.7919 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1805  | total loss: \u001b[1m\u001b[32m0.48047\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1805 | loss: 0.48047 - acc: 0.7937 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1806  | total loss: \u001b[1m\u001b[32m0.47669\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1806 | loss: 0.47669 - acc: 0.7948 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1807  | total loss: \u001b[1m\u001b[32m0.47325\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1807 | loss: 0.47325 - acc: 0.7958 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1808  | total loss: \u001b[1m\u001b[32m0.47034\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1808 | loss: 0.47034 - acc: 0.7965 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1809  | total loss: \u001b[1m\u001b[32m0.46795\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1809 | loss: 0.46795 - acc: 0.7975 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1810  | total loss: \u001b[1m\u001b[32m0.51827\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1810 | loss: 0.51827 - acc: 0.7714 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1811  | total loss: \u001b[1m\u001b[32m0.51185\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1811 | loss: 0.51185 - acc: 0.7745 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1812  | total loss: \u001b[1m\u001b[32m0.50622\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1812 | loss: 0.50622 - acc: 0.7772 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1813  | total loss: \u001b[1m\u001b[32m0.50087\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1813 | loss: 0.50087 - acc: 0.7796 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1814  | total loss: \u001b[1m\u001b[32m0.49579\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1814 | loss: 0.49579 - acc: 0.7828 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1815  | total loss: \u001b[1m\u001b[32m0.49094\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1815 | loss: 0.49094 - acc: 0.7858 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1816  | total loss: \u001b[1m\u001b[32m0.53156\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1816 | loss: 0.53156 - acc: 0.7618 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1817  | total loss: \u001b[1m\u001b[32m0.52333\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1817 | loss: 0.52333 - acc: 0.7666 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1818  | total loss: \u001b[1m\u001b[32m0.51618\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1818 | loss: 0.51618 - acc: 0.7705 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1819  | total loss: \u001b[1m\u001b[32m0.50986\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1819 | loss: 0.50986 - acc: 0.7741 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1820  | total loss: \u001b[1m\u001b[32m0.54681\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1820 | loss: 0.54681 - acc: 0.7495 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1821  | total loss: \u001b[1m\u001b[32m0.53791\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1821 | loss: 0.53791 - acc: 0.7555 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1822  | total loss: \u001b[1m\u001b[32m0.53024\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1822 | loss: 0.53024 - acc: 0.7608 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1823  | total loss: \u001b[1m\u001b[32m0.52354\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1823 | loss: 0.52354 - acc: 0.7660 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1824  | total loss: \u001b[1m\u001b[32m0.54989\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1824 | loss: 0.54989 - acc: 0.7450 -- iter: 891/891\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1825  | total loss: \u001b[1m\u001b[32m0.54182\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1825 | loss: 0.54182 - acc: 0.7516 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1826  | total loss: \u001b[1m\u001b[32m0.56661\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1826 | loss: 0.56661 - acc: 0.7283 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1827  | total loss: \u001b[1m\u001b[32m0.55832\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1827 | loss: 0.55832 - acc: 0.7366 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1828  | total loss: \u001b[1m\u001b[32m0.57757\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1828 | loss: 0.57757 - acc: 0.7183 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1829  | total loss: \u001b[1m\u001b[32m0.57005\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1829 | loss: 0.57005 - acc: 0.7275 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1830  | total loss: \u001b[1m\u001b[32m0.56386\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1830 | loss: 0.56386 - acc: 0.7353 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1831  | total loss: \u001b[1m\u001b[32m0.55822\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1831 | loss: 0.55822 - acc: 0.7419 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1832  | total loss: \u001b[1m\u001b[32m0.57658\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1832 | loss: 0.57658 - acc: 0.7217 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1833  | total loss: \u001b[1m\u001b[32m0.56910\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1833 | loss: 0.56910 - acc: 0.7306 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1834  | total loss: \u001b[1m\u001b[32m0.58608\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1834 | loss: 0.58608 - acc: 0.7113 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1835  | total loss: \u001b[1m\u001b[32m0.57735\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1835 | loss: 0.57735 - acc: 0.7213 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1836  | total loss: \u001b[1m\u001b[32m0.56922\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1836 | loss: 0.56922 - acc: 0.7306 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1837  | total loss: \u001b[1m\u001b[32m0.56122\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1837 | loss: 0.56122 - acc: 0.7387 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1838  | total loss: \u001b[1m\u001b[32m0.58025\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1838 | loss: 0.58025 - acc: 0.7197 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1839  | total loss: \u001b[1m\u001b[32m0.57009\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1839 | loss: 0.57009 - acc: 0.7289 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1840  | total loss: \u001b[1m\u001b[32m0.56059\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1840 | loss: 0.56059 - acc: 0.7374 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1841  | total loss: \u001b[1m\u001b[32m0.55156\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1841 | loss: 0.55156 - acc: 0.7451 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1842  | total loss: \u001b[1m\u001b[32m0.54295\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1842 | loss: 0.54295 - acc: 0.7516 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1843  | total loss: \u001b[1m\u001b[32m0.53474\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1843 | loss: 0.53474 - acc: 0.7577 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1844  | total loss: \u001b[1m\u001b[32m0.56683\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1844 | loss: 0.56683 - acc: 0.7363 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1845  | total loss: \u001b[1m\u001b[32m0.55579\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1845 | loss: 0.55579 - acc: 0.7441 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1846  | total loss: \u001b[1m\u001b[32m0.54572\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1846 | loss: 0.54572 - acc: 0.7508 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1847  | total loss: \u001b[1m\u001b[32m0.53647\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1847 | loss: 0.53647 - acc: 0.7568 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1848  | total loss: \u001b[1m\u001b[32m0.57097\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1848 | loss: 0.57097 - acc: 0.7347 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1849  | total loss: \u001b[1m\u001b[32m0.55904\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1849 | loss: 0.55904 - acc: 0.7427 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1850  | total loss: \u001b[1m\u001b[32m0.58746\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1850 | loss: 0.58746 - acc: 0.7226 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1851  | total loss: \u001b[1m\u001b[32m0.57467\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1851 | loss: 0.57467 - acc: 0.7319 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1852  | total loss: \u001b[1m\u001b[32m0.56383\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1852 | loss: 0.56383 - acc: 0.7399 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1853  | total loss: \u001b[1m\u001b[32m0.55449\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1853 | loss: 0.55449 - acc: 0.7475 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1854  | total loss: \u001b[1m\u001b[32m0.54613\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1854 | loss: 0.54613 - acc: 0.7540 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1855  | total loss: \u001b[1m\u001b[32m0.53827\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1855 | loss: 0.53827 - acc: 0.7593 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1856  | total loss: \u001b[1m\u001b[32m0.53060\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1856 | loss: 0.53060 - acc: 0.7649 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1857  | total loss: \u001b[1m\u001b[32m0.52304\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1857 | loss: 0.52304 - acc: 0.7698 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1858  | total loss: \u001b[1m\u001b[32m0.55895\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1858 | loss: 0.55895 - acc: 0.7461 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1859  | total loss: \u001b[1m\u001b[32m0.54801\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1859 | loss: 0.54801 - acc: 0.7527 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1860  | total loss: \u001b[1m\u001b[32m0.53810\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1860 | loss: 0.53810 - acc: 0.7587 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1861  | total loss: \u001b[1m\u001b[32m0.52911\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1861 | loss: 0.52911 - acc: 0.7639 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1862  | total loss: \u001b[1m\u001b[32m0.56331\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1862 | loss: 0.56331 - acc: 0.7428 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1863  | total loss: \u001b[1m\u001b[32m0.55187\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1863 | loss: 0.55187 - acc: 0.7492 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1864  | total loss: \u001b[1m\u001b[32m0.58413\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1864 | loss: 0.58413 - acc: 0.7276 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1865  | total loss: \u001b[1m\u001b[32m0.57126\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1865 | loss: 0.57126 - acc: 0.7360 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1866  | total loss: \u001b[1m\u001b[32m0.59173\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1866 | loss: 0.59173 - acc: 0.7196 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1867  | total loss: \u001b[1m\u001b[32m0.57970\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1867 | loss: 0.57970 - acc: 0.7287 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1868  | total loss: \u001b[1m\u001b[32m0.60063\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1868 | loss: 0.60063 - acc: 0.7080 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1869  | total loss: \u001b[1m\u001b[32m0.58982\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1869 | loss: 0.58982 - acc: 0.7175 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1870  | total loss: \u001b[1m\u001b[32m0.60743\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1870 | loss: 0.60743 - acc: 0.6980 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1871  | total loss: \u001b[1m\u001b[32m0.59791\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1871 | loss: 0.59791 - acc: 0.7082 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1872  | total loss: \u001b[1m\u001b[32m0.61191\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1872 | loss: 0.61191 - acc: 0.6914 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1873  | total loss: \u001b[1m\u001b[32m0.60328\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1873 | loss: 0.60328 - acc: 0.7022 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1874  | total loss: \u001b[1m\u001b[32m0.61396\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1874 | loss: 0.61396 - acc: 0.6881 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1875  | total loss: \u001b[1m\u001b[32m0.60569\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1875 | loss: 0.60569 - acc: 0.6993 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1876  | total loss: \u001b[1m\u001b[32m0.61541\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1876 | loss: 0.61541 - acc: 0.6829 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1877  | total loss: \u001b[1m\u001b[32m0.60708\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1877 | loss: 0.60708 - acc: 0.6945 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1878  | total loss: \u001b[1m\u001b[32m0.61786\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1878 | loss: 0.61786 - acc: 0.6807 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1879  | total loss: \u001b[1m\u001b[32m0.60915\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1879 | loss: 0.60915 - acc: 0.6921 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1880  | total loss: \u001b[1m\u001b[32m0.61860\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1880 | loss: 0.61860 - acc: 0.6786 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1881  | total loss: \u001b[1m\u001b[32m0.60956\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1881 | loss: 0.60956 - acc: 0.6902 -- iter: 891/891\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1882  | total loss: \u001b[1m\u001b[32m0.60115\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1882 | loss: 0.60115 - acc: 0.6997 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1883  | total loss: \u001b[1m\u001b[32m0.59304\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1883 | loss: 0.59304 - acc: 0.7075 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1884  | total loss: \u001b[1m\u001b[32m0.60725\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1884 | loss: 0.60725 - acc: 0.6918 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1885  | total loss: \u001b[1m\u001b[32m0.59758\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1885 | loss: 0.59758 - acc: 0.7005 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1886  | total loss: \u001b[1m\u001b[32m0.58845\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1886 | loss: 0.58845 - acc: 0.7081 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1887  | total loss: \u001b[1m\u001b[32m0.57966\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1887 | loss: 0.57966 - acc: 0.7148 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1888  | total loss: \u001b[1m\u001b[32m0.59490\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1888 | loss: 0.59490 - acc: 0.7022 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1889  | total loss: \u001b[1m\u001b[32m0.58458\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1889 | loss: 0.58458 - acc: 0.7103 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1890  | total loss: \u001b[1m\u001b[32m0.57492\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1890 | loss: 0.57492 - acc: 0.7178 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1891  | total loss: \u001b[1m\u001b[32m0.56578\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1891 | loss: 0.56578 - acc: 0.7248 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1892  | total loss: \u001b[1m\u001b[32m0.55705\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1892 | loss: 0.55705 - acc: 0.7317 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1893  | total loss: \u001b[1m\u001b[32m0.54868\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1893 | loss: 0.54868 - acc: 0.7379 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1894  | total loss: \u001b[1m\u001b[32m0.54065\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1894 | loss: 0.54065 - acc: 0.7447 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1895  | total loss: \u001b[1m\u001b[32m0.53296\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1895 | loss: 0.53296 - acc: 0.7515 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1896  | total loss: \u001b[1m\u001b[32m0.52562\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1896 | loss: 0.52562 - acc: 0.7571 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1897  | total loss: \u001b[1m\u001b[32m0.51865\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1897 | loss: 0.51865 - acc: 0.7627 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1898  | total loss: \u001b[1m\u001b[32m0.56365\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1898 | loss: 0.56365 - acc: 0.7375 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1899  | total loss: \u001b[1m\u001b[32m0.55229\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1899 | loss: 0.55229 - acc: 0.7443 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1900  | total loss: \u001b[1m\u001b[32m0.58905\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1900 | loss: 0.58905 - acc: 0.7220 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1901  | total loss: \u001b[1m\u001b[32m0.57536\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1901 | loss: 0.57536 - acc: 0.7309 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1902  | total loss: \u001b[1m\u001b[32m0.56390\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1902 | loss: 0.56390 - acc: 0.7393 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1903  | total loss: \u001b[1m\u001b[32m0.55413\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1903 | loss: 0.55413 - acc: 0.7462 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1904  | total loss: \u001b[1m\u001b[32m0.58021\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1904 | loss: 0.58021 - acc: 0.7272 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1905  | total loss: \u001b[1m\u001b[32m0.56921\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1905 | loss: 0.56921 - acc: 0.7353 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1906  | total loss: \u001b[1m\u001b[32m0.59326\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1906 | loss: 0.59326 - acc: 0.7153 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1907  | total loss: \u001b[1m\u001b[32m0.58135\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1907 | loss: 0.58135 - acc: 0.7245 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1908  | total loss: \u001b[1m\u001b[32m0.57070\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1908 | loss: 0.57070 - acc: 0.7329 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1909  | total loss: \u001b[1m\u001b[32m0.56091\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1909 | loss: 0.56091 - acc: 0.7407 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1910  | total loss: \u001b[1m\u001b[32m0.58211\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1910 | loss: 0.58211 - acc: 0.7213 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1911  | total loss: \u001b[1m\u001b[32m0.57122\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1911 | loss: 0.57122 - acc: 0.7304 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1912  | total loss: \u001b[1m\u001b[32m0.56156\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1912 | loss: 0.56156 - acc: 0.7387 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1913  | total loss: \u001b[1m\u001b[32m0.55282\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1913 | loss: 0.55282 - acc: 0.7460 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1914  | total loss: \u001b[1m\u001b[32m0.54475\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1914 | loss: 0.54475 - acc: 0.7523 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1915  | total loss: \u001b[1m\u001b[32m0.53722\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1915 | loss: 0.53722 - acc: 0.7575 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1916  | total loss: \u001b[1m\u001b[32m0.53015\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1916 | loss: 0.53015 - acc: 0.7614 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1917  | total loss: \u001b[1m\u001b[32m0.52348\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1917 | loss: 0.52348 - acc: 0.7651 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1918  | total loss: \u001b[1m\u001b[32m0.51719\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1918 | loss: 0.51719 - acc: 0.7689 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1919  | total loss: \u001b[1m\u001b[32m0.51126\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1919 | loss: 0.51126 - acc: 0.7730 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1920  | total loss: \u001b[1m\u001b[32m0.55103\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1920 | loss: 0.55103 - acc: 0.7504 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1921  | total loss: \u001b[1m\u001b[32m0.54121\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1921 | loss: 0.54121 - acc: 0.7555 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1922  | total loss: \u001b[1m\u001b[32m0.53210\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1922 | loss: 0.53210 - acc: 0.7603 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1923  | total loss: \u001b[1m\u001b[32m0.52368\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1923 | loss: 0.52368 - acc: 0.7651 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1924  | total loss: \u001b[1m\u001b[32m0.51599\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1924 | loss: 0.51599 - acc: 0.7698 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1925  | total loss: \u001b[1m\u001b[32m0.50902\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1925 | loss: 0.50902 - acc: 0.7735 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1926  | total loss: \u001b[1m\u001b[32m0.54669\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1926 | loss: 0.54669 - acc: 0.7502 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1927  | total loss: \u001b[1m\u001b[32m0.53709\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1927 | loss: 0.53709 - acc: 0.7554 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1928  | total loss: \u001b[1m\u001b[32m0.56845\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1928 | loss: 0.56845 - acc: 0.7346 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1929  | total loss: \u001b[1m\u001b[32m0.55844\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1929 | loss: 0.55844 - acc: 0.7411 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1930  | total loss: \u001b[1m\u001b[32m0.58327\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1930 | loss: 0.58327 - acc: 0.7226 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1931  | total loss: \u001b[1m\u001b[32m0.57355\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1931 | loss: 0.57355 - acc: 0.7301 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1932  | total loss: \u001b[1m\u001b[32m0.56501\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1932 | loss: 0.56501 - acc: 0.7370 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1933  | total loss: \u001b[1m\u001b[32m0.55690\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1933 | loss: 0.55690 - acc: 0.7431 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1934  | total loss: \u001b[1m\u001b[32m0.54886\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1934 | loss: 0.54886 - acc: 0.7495 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1935  | total loss: \u001b[1m\u001b[32m0.54090\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1935 | loss: 0.54090 - acc: 0.7557 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1936  | total loss: \u001b[1m\u001b[32m0.53321\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1936 | loss: 0.53321 - acc: 0.7612 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1937  | total loss: \u001b[1m\u001b[32m0.52598\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1937 | loss: 0.52598 - acc: 0.7666 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1938  | total loss: \u001b[1m\u001b[32m0.51936\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1938 | loss: 0.51936 - acc: 0.7707 -- iter: 891/891\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1939  | total loss: \u001b[1m\u001b[32m0.51336\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1939 | loss: 0.51336 - acc: 0.7745 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1940  | total loss: \u001b[1m\u001b[32m0.55442\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1940 | loss: 0.55442 - acc: 0.7496 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1941  | total loss: \u001b[1m\u001b[32m0.54506\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1941 | loss: 0.54506 - acc: 0.7547 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1942  | total loss: \u001b[1m\u001b[32m0.53667\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1942 | loss: 0.53667 - acc: 0.7584 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1943  | total loss: \u001b[1m\u001b[32m0.52885\u001b[0m\u001b[0m | time: 0.012s\n",
      "| Adam | epoch: 1943 | loss: 0.52885 - acc: 0.7626 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1944  | total loss: \u001b[1m\u001b[32m0.52151\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1944 | loss: 0.52151 - acc: 0.7676 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1945  | total loss: \u001b[1m\u001b[32m0.51480\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1945 | loss: 0.51480 - acc: 0.7721 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1946  | total loss: \u001b[1m\u001b[32m0.54980\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1946 | loss: 0.54980 - acc: 0.7500 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1947  | total loss: \u001b[1m\u001b[32m0.54025\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1947 | loss: 0.54025 - acc: 0.7563 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1948  | total loss: \u001b[1m\u001b[32m0.53200\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1948 | loss: 0.53200 - acc: 0.7617 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1949  | total loss: \u001b[1m\u001b[32m0.52507\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1949 | loss: 0.52507 - acc: 0.7659 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1950  | total loss: \u001b[1m\u001b[32m0.51889\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1950 | loss: 0.51889 - acc: 0.7698 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1951  | total loss: \u001b[1m\u001b[32m0.51280\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1951 | loss: 0.51280 - acc: 0.7732 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1952  | total loss: \u001b[1m\u001b[32m0.50671\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1952 | loss: 0.50671 - acc: 0.7771 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1953  | total loss: \u001b[1m\u001b[32m0.50080\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1953 | loss: 0.50080 - acc: 0.7806 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1954  | total loss: \u001b[1m\u001b[32m0.49524\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1954 | loss: 0.49524 - acc: 0.7841 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1955  | total loss: \u001b[1m\u001b[32m0.49020\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1955 | loss: 0.49020 - acc: 0.7865 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1956  | total loss: \u001b[1m\u001b[32m0.48581\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1956 | loss: 0.48581 - acc: 0.7881 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1957  | total loss: \u001b[1m\u001b[32m0.48200\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1957 | loss: 0.48200 - acc: 0.7892 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1958  | total loss: \u001b[1m\u001b[32m0.47860\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1958 | loss: 0.47860 - acc: 0.7906 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1959  | total loss: \u001b[1m\u001b[32m0.47548\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1959 | loss: 0.47548 - acc: 0.7924 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1960  | total loss: \u001b[1m\u001b[32m0.47255\u001b[0m\u001b[0m | time: 0.012s\n",
      "| Adam | epoch: 1960 | loss: 0.47255 - acc: 0.7941 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1961  | total loss: \u001b[1m\u001b[32m0.46969\u001b[0m\u001b[0m | time: 0.012s\n",
      "| Adam | epoch: 1961 | loss: 0.46969 - acc: 0.7953 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1962  | total loss: \u001b[1m\u001b[32m0.46695\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1962 | loss: 0.46695 - acc: 0.7959 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1963  | total loss: \u001b[1m\u001b[32m0.46445\u001b[0m\u001b[0m | time: 0.013s\n",
      "| Adam | epoch: 1963 | loss: 0.46445 - acc: 0.7966 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1964  | total loss: \u001b[1m\u001b[32m0.46227\u001b[0m\u001b[0m | time: 0.011s\n",
      "| Adam | epoch: 1964 | loss: 0.46227 - acc: 0.7972 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1965  | total loss: \u001b[1m\u001b[32m0.46043\u001b[0m\u001b[0m | time: 0.011s\n",
      "| Adam | epoch: 1965 | loss: 0.46043 - acc: 0.7975 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1966  | total loss: \u001b[1m\u001b[32m0.51429\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1966 | loss: 0.51429 - acc: 0.7691 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1967  | total loss: \u001b[1m\u001b[32m0.50785\u001b[0m\u001b[0m | time: 0.011s\n",
      "| Adam | epoch: 1967 | loss: 0.50785 - acc: 0.7724 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1968  | total loss: \u001b[1m\u001b[32m0.55227\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1968 | loss: 0.55227 - acc: 0.7452 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1969  | total loss: \u001b[1m\u001b[32m0.54365\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1969 | loss: 0.54365 - acc: 0.7507 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1970  | total loss: \u001b[1m\u001b[32m0.57212\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1970 | loss: 0.57212 - acc: 0.7289 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1971  | total loss: \u001b[1m\u001b[32m0.56339\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1971 | loss: 0.56339 - acc: 0.7357 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1972  | total loss: \u001b[1m\u001b[32m0.55599\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1972 | loss: 0.55599 - acc: 0.7422 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1973  | total loss: \u001b[1m\u001b[32m0.54924\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1973 | loss: 0.54924 - acc: 0.7484 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1974  | total loss: \u001b[1m\u001b[32m0.54276\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1974 | loss: 0.54276 - acc: 0.7545 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1975  | total loss: \u001b[1m\u001b[32m0.53643\u001b[0m\u001b[0m | time: 0.011s\n",
      "| Adam | epoch: 1975 | loss: 0.53643 - acc: 0.7603 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1976  | total loss: \u001b[1m\u001b[32m0.53026\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1976 | loss: 0.53026 - acc: 0.7658 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1977  | total loss: \u001b[1m\u001b[32m0.52434\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1977 | loss: 0.52434 - acc: 0.7700 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1978  | total loss: \u001b[1m\u001b[32m0.55353\u001b[0m\u001b[0m | time: 0.011s\n",
      "| Adam | epoch: 1978 | loss: 0.55353 - acc: 0.7468 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1979  | total loss: \u001b[1m\u001b[32m0.54519\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1979 | loss: 0.54519 - acc: 0.7524 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1980  | total loss: \u001b[1m\u001b[32m0.53776\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1980 | loss: 0.53776 - acc: 0.7580 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1981  | total loss: \u001b[1m\u001b[32m0.53102\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1981 | loss: 0.53102 - acc: 0.7628 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1982  | total loss: \u001b[1m\u001b[32m0.56213\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1982 | loss: 0.56213 - acc: 0.7413 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1983  | total loss: \u001b[1m\u001b[32m0.55295\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1983 | loss: 0.55295 - acc: 0.7478 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1984  | total loss: \u001b[1m\u001b[32m0.54475\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1984 | loss: 0.54475 - acc: 0.7541 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1985  | total loss: \u001b[1m\u001b[32m0.53732\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1985 | loss: 0.53732 - acc: 0.7596 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1986  | total loss: \u001b[1m\u001b[32m0.56292\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1986 | loss: 0.56292 - acc: 0.7388 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1987  | total loss: \u001b[1m\u001b[32m0.55388\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1987 | loss: 0.55388 - acc: 0.7466 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1988  | total loss: \u001b[1m\u001b[32m0.54603\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1988 | loss: 0.54603 - acc: 0.7534 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1989  | total loss: \u001b[1m\u001b[32m0.53906\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1989 | loss: 0.53906 - acc: 0.7595 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1990  | total loss: \u001b[1m\u001b[32m0.56574\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1990 | loss: 0.56574 - acc: 0.7356 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1991  | total loss: \u001b[1m\u001b[32m0.55732\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1991 | loss: 0.55732 - acc: 0.7435 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1992  | total loss: \u001b[1m\u001b[32m0.55007\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1992 | loss: 0.55007 - acc: 0.7503 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1993  | total loss: \u001b[1m\u001b[32m0.54343\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1993 | loss: 0.54343 - acc: 0.7564 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1994  | total loss: \u001b[1m\u001b[32m0.56769\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1994 | loss: 0.56769 - acc: 0.7338 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1995  | total loss: \u001b[1m\u001b[32m0.55896\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1995 | loss: 0.55896 - acc: 0.7416 -- iter: 891/891\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1996  | total loss: \u001b[1m\u001b[32m0.58000\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1996 | loss: 0.58000 - acc: 0.7219 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1997  | total loss: \u001b[1m\u001b[32m0.57024\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1997 | loss: 0.57024 - acc: 0.7309 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1998  | total loss: \u001b[1m\u001b[32m0.58668\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1998 | loss: 0.58668 - acc: 0.7144 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1999  | total loss: \u001b[1m\u001b[32m0.57662\u001b[0m\u001b[0m | time: 0.013s\n",
      "| Adam | epoch: 1999 | loss: 0.57662 - acc: 0.7241 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2000  | total loss: \u001b[1m\u001b[32m0.59708\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2000 | loss: 0.59708 - acc: 0.7031 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2001  | total loss: \u001b[1m\u001b[32m0.58648\u001b[0m\u001b[0m | time: 0.012s\n",
      "| Adam | epoch: 2001 | loss: 0.58648 - acc: 0.7138 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2002  | total loss: \u001b[1m\u001b[32m0.57700\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 2002 | loss: 0.57700 - acc: 0.7238 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2003  | total loss: \u001b[1m\u001b[32m0.56820\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2003 | loss: 0.56820 - acc: 0.7327 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2004  | total loss: \u001b[1m\u001b[32m0.58550\u001b[0m\u001b[0m | time: 0.011s\n",
      "| Adam | epoch: 2004 | loss: 0.58550 - acc: 0.7157 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2005  | total loss: \u001b[1m\u001b[32m0.57533\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2005 | loss: 0.57533 - acc: 0.7252 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2006  | total loss: \u001b[1m\u001b[32m0.59451\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 2006 | loss: 0.59451 - acc: 0.7084 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2007  | total loss: \u001b[1m\u001b[32m0.58355\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2007 | loss: 0.58355 - acc: 0.7190 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2008  | total loss: \u001b[1m\u001b[32m0.57373\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2008 | loss: 0.57373 - acc: 0.7285 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2009  | total loss: \u001b[1m\u001b[32m0.56468\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2009 | loss: 0.56468 - acc: 0.7373 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2010  | total loss: \u001b[1m\u001b[32m0.55614\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2010 | loss: 0.55614 - acc: 0.7453 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2011  | total loss: \u001b[1m\u001b[32m0.54799\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 2011 | loss: 0.54799 - acc: 0.7520 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2012  | total loss: \u001b[1m\u001b[32m0.57163\u001b[0m\u001b[0m | time: 0.012s\n",
      "| Adam | epoch: 2012 | loss: 0.57163 - acc: 0.7324 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2013  | total loss: \u001b[1m\u001b[32m0.56135\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2013 | loss: 0.56135 - acc: 0.7406 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2014  | total loss: \u001b[1m\u001b[32m0.58409\u001b[0m\u001b[0m | time: 0.011s\n",
      "| Adam | epoch: 2014 | loss: 0.58409 - acc: 0.7227 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2015  | total loss: \u001b[1m\u001b[32m0.57272\u001b[0m\u001b[0m | time: 0.012s\n",
      "| Adam | epoch: 2015 | loss: 0.57272 - acc: 0.7316 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2016  | total loss: \u001b[1m\u001b[32m0.59076\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2016 | loss: 0.59076 - acc: 0.7151 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2017  | total loss: \u001b[1m\u001b[32m0.57948\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 2017 | loss: 0.57948 - acc: 0.7247 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2018  | total loss: \u001b[1m\u001b[32m0.56965\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2018 | loss: 0.56965 - acc: 0.7335 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2019  | total loss: \u001b[1m\u001b[32m0.56091\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2019 | loss: 0.56091 - acc: 0.7412 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2020  | total loss: \u001b[1m\u001b[32m0.58138\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2020 | loss: 0.58138 - acc: 0.7209 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2021  | total loss: \u001b[1m\u001b[32m0.57174\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2021 | loss: 0.57174 - acc: 0.7302 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2022  | total loss: \u001b[1m\u001b[32m0.58834\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2022 | loss: 0.58834 - acc: 0.7131 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2023  | total loss: \u001b[1m\u001b[32m0.57848\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2023 | loss: 0.57848 - acc: 0.7229 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2024  | total loss: \u001b[1m\u001b[32m0.59738\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2024 | loss: 0.59738 - acc: 0.7044 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2025  | total loss: \u001b[1m\u001b[32m0.58731\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2025 | loss: 0.58731 - acc: 0.7149 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2026  | total loss: \u001b[1m\u001b[32m0.57829\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2026 | loss: 0.57829 - acc: 0.7248 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2027  | total loss: \u001b[1m\u001b[32m0.56978\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2027 | loss: 0.56978 - acc: 0.7334 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2028  | total loss: \u001b[1m\u001b[32m0.58586\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2028 | loss: 0.58586 - acc: 0.7154 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2029  | total loss: \u001b[1m\u001b[32m0.57580\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2029 | loss: 0.57580 - acc: 0.7251 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2030  | total loss: \u001b[1m\u001b[32m0.59538\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2030 | loss: 0.59538 - acc: 0.7075 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2031  | total loss: \u001b[1m\u001b[32m0.58412\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 2031 | loss: 0.58412 - acc: 0.7180 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2032  | total loss: \u001b[1m\u001b[32m0.60167\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2032 | loss: 0.60167 - acc: 0.7002 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2033  | total loss: \u001b[1m\u001b[32m0.59004\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 2033 | loss: 0.59004 - acc: 0.7113 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2034  | total loss: \u001b[1m\u001b[32m0.57966\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2034 | loss: 0.57966 - acc: 0.7214 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2035  | total loss: \u001b[1m\u001b[32m0.57012\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2035 | loss: 0.57012 - acc: 0.7308 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2036  | total loss: \u001b[1m\u001b[32m0.58774\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2036 | loss: 0.58774 - acc: 0.7130 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2037  | total loss: \u001b[1m\u001b[32m0.57710\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2037 | loss: 0.57710 - acc: 0.7231 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2038  | total loss: \u001b[1m\u001b[32m0.56740\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2038 | loss: 0.56740 - acc: 0.7317 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2039  | total loss: \u001b[1m\u001b[32m0.55839\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2039 | loss: 0.55839 - acc: 0.7397 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2040  | total loss: \u001b[1m\u001b[32m0.57863\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 2040 | loss: 0.57863 - acc: 0.7224 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2041  | total loss: \u001b[1m\u001b[32m0.56809\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2041 | loss: 0.56809 - acc: 0.7319 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2042  | total loss: \u001b[1m\u001b[32m0.59267\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2042 | loss: 0.59267 - acc: 0.7110 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2043  | total loss: \u001b[1m\u001b[32m0.58101\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2043 | loss: 0.58101 - acc: 0.7211 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2044  | total loss: \u001b[1m\u001b[32m0.59683\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2044 | loss: 0.59683 - acc: 0.7058 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2045  | total loss: \u001b[1m\u001b[32m0.58548\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2045 | loss: 0.58548 - acc: 0.7166 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2046  | total loss: \u001b[1m\u001b[32m0.60268\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2046 | loss: 0.60268 - acc: 0.6990 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2047  | total loss: \u001b[1m\u001b[32m0.59170\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2047 | loss: 0.59170 - acc: 0.7104 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2048  | total loss: \u001b[1m\u001b[32m0.58217\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2048 | loss: 0.58217 - acc: 0.7206 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2049  | total loss: \u001b[1m\u001b[32m0.57358\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2049 | loss: 0.57358 - acc: 0.7298 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2050  | total loss: \u001b[1m\u001b[32m0.56553\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 2050 | loss: 0.56553 - acc: 0.7381 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2051  | total loss: \u001b[1m\u001b[32m0.55771\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2051 | loss: 0.55771 - acc: 0.7458 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2052  | total loss: \u001b[1m\u001b[32m0.54995\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2052 | loss: 0.54995 - acc: 0.7525 -- iter: 891/891\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 2053  | total loss: \u001b[1m\u001b[32m0.54221\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2053 | loss: 0.54221 - acc: 0.7587 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2054  | total loss: \u001b[1m\u001b[32m0.56752\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2054 | loss: 0.56752 - acc: 0.7373 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2055  | total loss: \u001b[1m\u001b[32m0.55720\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2055 | loss: 0.55720 - acc: 0.7450 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2056  | total loss: \u001b[1m\u001b[32m0.54770\u001b[0m\u001b[0m | time: 0.011s\n",
      "| Adam | epoch: 2056 | loss: 0.54770 - acc: 0.7520 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2057  | total loss: \u001b[1m\u001b[32m0.53890\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2057 | loss: 0.53890 - acc: 0.7584 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2058  | total loss: \u001b[1m\u001b[32m0.57097\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2058 | loss: 0.57097 - acc: 0.7359 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2059  | total loss: \u001b[1m\u001b[32m0.55962\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2059 | loss: 0.55962 - acc: 0.7435 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2060  | total loss: \u001b[1m\u001b[32m0.58995\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 2060 | loss: 0.58995 - acc: 0.7239 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2061  | total loss: \u001b[1m\u001b[32m0.57701\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2061 | loss: 0.57701 - acc: 0.7330 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2062  | total loss: \u001b[1m\u001b[32m0.59992\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 2062 | loss: 0.59992 - acc: 0.7138 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2063  | total loss: \u001b[1m\u001b[32m0.58717\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 2063 | loss: 0.58717 - acc: 0.7237 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2064  | total loss: \u001b[1m\u001b[32m0.60390\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2064 | loss: 0.60390 - acc: 0.7060 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2065  | total loss: \u001b[1m\u001b[32m0.59277\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2065 | loss: 0.59277 - acc: 0.7167 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2066  | total loss: \u001b[1m\u001b[32m0.60724\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2066 | loss: 0.60724 - acc: 0.7001 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2067  | total loss: \u001b[1m\u001b[32m0.59770\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 2067 | loss: 0.59770 - acc: 0.7112 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2068  | total loss: \u001b[1m\u001b[32m0.61067\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 2068 | loss: 0.61067 - acc: 0.6954 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2069  | total loss: \u001b[1m\u001b[32m0.60209\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2069 | loss: 0.60209 - acc: 0.7060 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2070  | total loss: \u001b[1m\u001b[32m0.61419\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 2070 | loss: 0.61419 - acc: 0.6897 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2071  | total loss: \u001b[1m\u001b[32m0.60587\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2071 | loss: 0.60587 - acc: 0.7007 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2072  | total loss: \u001b[1m\u001b[32m0.59835\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 2072 | loss: 0.59835 - acc: 0.7108 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2073  | total loss: \u001b[1m\u001b[32m0.59118\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2073 | loss: 0.59118 - acc: 0.7201 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2074  | total loss: \u001b[1m\u001b[32m0.60483\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2074 | loss: 0.60483 - acc: 0.7021 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2075  | total loss: \u001b[1m\u001b[32m0.59606\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 2075 | loss: 0.59606 - acc: 0.7128 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2076  | total loss: \u001b[1m\u001b[32m0.60759\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2076 | loss: 0.60759 - acc: 0.6980 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2077  | total loss: \u001b[1m\u001b[32m0.59784\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 2077 | loss: 0.59784 - acc: 0.7089 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2078  | total loss: \u001b[1m\u001b[32m0.61130\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2078 | loss: 0.61130 - acc: 0.6925 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2079  | total loss: \u001b[1m\u001b[32m0.60080\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2079 | loss: 0.60080 - acc: 0.7036 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2080  | total loss: \u001b[1m\u001b[32m0.61266\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 2080 | loss: 0.61266 - acc: 0.6898 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2081  | total loss: \u001b[1m\u001b[32m0.60188\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2081 | loss: 0.60188 - acc: 0.7013 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2082  | total loss: \u001b[1m\u001b[32m0.59202\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2082 | loss: 0.59202 - acc: 0.7118 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2083  | total loss: \u001b[1m\u001b[32m0.58280\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2083 | loss: 0.58280 - acc: 0.7212 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2084  | total loss: \u001b[1m\u001b[32m0.57401\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 2084 | loss: 0.57401 - acc: 0.7296 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2085  | total loss: \u001b[1m\u001b[32m0.56554\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2085 | loss: 0.56554 - acc: 0.7360 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2086  | total loss: \u001b[1m\u001b[32m0.55733\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 2086 | loss: 0.55733 - acc: 0.7413 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2087  | total loss: \u001b[1m\u001b[32m0.54937\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2087 | loss: 0.54937 - acc: 0.7461 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2088  | total loss: \u001b[1m\u001b[32m0.54168\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2088 | loss: 0.54168 - acc: 0.7505 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2089  | total loss: \u001b[1m\u001b[32m0.53430\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2089 | loss: 0.53430 - acc: 0.7563 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2090  | total loss: \u001b[1m\u001b[32m0.52732\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2090 | loss: 0.52732 - acc: 0.7610 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2091  | total loss: \u001b[1m\u001b[32m0.52079\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2091 | loss: 0.52079 - acc: 0.7656 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2092  | total loss: \u001b[1m\u001b[32m0.51473\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2092 | loss: 0.51473 - acc: 0.7702 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2093  | total loss: \u001b[1m\u001b[32m0.50909\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2093 | loss: 0.50909 - acc: 0.7743 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2094  | total loss: \u001b[1m\u001b[32m0.55004\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2094 | loss: 0.55004 - acc: 0.7532 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2095  | total loss: \u001b[1m\u001b[32m0.54025\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2095 | loss: 0.54025 - acc: 0.7588 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2096  | total loss: \u001b[1m\u001b[32m0.57871\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2096 | loss: 0.57871 - acc: 0.7393 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2097  | total loss: \u001b[1m\u001b[32m0.56548\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 2097 | loss: 0.56548 - acc: 0.7463 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2098  | total loss: \u001b[1m\u001b[32m0.55379\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2098 | loss: 0.55379 - acc: 0.7529 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2099  | total loss: \u001b[1m\u001b[32m0.54377\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2099 | loss: 0.54377 - acc: 0.7586 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2100  | total loss: \u001b[1m\u001b[32m0.57346\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2100 | loss: 0.57346 - acc: 0.7386 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2101  | total loss: \u001b[1m\u001b[32m0.56310\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2101 | loss: 0.56310 - acc: 0.7452 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2102  | total loss: \u001b[1m\u001b[32m0.55449\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 2102 | loss: 0.55449 - acc: 0.7509 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2103  | total loss: \u001b[1m\u001b[32m0.54680\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2103 | loss: 0.54680 - acc: 0.7561 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2104  | total loss: \u001b[1m\u001b[32m0.53940\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 2104 | loss: 0.53940 - acc: 0.7610 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2105  | total loss: \u001b[1m\u001b[32m0.53197\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2105 | loss: 0.53197 - acc: 0.7657 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2106  | total loss: \u001b[1m\u001b[32m0.56152\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2106 | loss: 0.56152 - acc: 0.7415 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2107  | total loss: \u001b[1m\u001b[32m0.55112\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2107 | loss: 0.55112 - acc: 0.7485 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2108  | total loss: \u001b[1m\u001b[32m0.57598\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2108 | loss: 0.57598 - acc: 0.7297 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2109  | total loss: \u001b[1m\u001b[32m0.56453\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 2109 | loss: 0.56453 - acc: 0.7381 -- iter: 891/891\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 2110  | total loss: \u001b[1m\u001b[32m0.55453\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2110 | loss: 0.55453 - acc: 0.7457 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2111  | total loss: \u001b[1m\u001b[32m0.54565\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2111 | loss: 0.54565 - acc: 0.7524 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2112  | total loss: \u001b[1m\u001b[32m0.53763\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2112 | loss: 0.53763 - acc: 0.7584 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2113  | total loss: \u001b[1m\u001b[32m0.53030\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2113 | loss: 0.53030 - acc: 0.7645 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2114  | total loss: \u001b[1m\u001b[32m0.55465\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2114 | loss: 0.55465 - acc: 0.7463 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2115  | total loss: \u001b[1m\u001b[32m0.54557\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 2115 | loss: 0.54557 - acc: 0.7534 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2116  | total loss: \u001b[1m\u001b[32m0.53744\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2116 | loss: 0.53744 - acc: 0.7588 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2117  | total loss: \u001b[1m\u001b[32m0.53006\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2117 | loss: 0.53006 - acc: 0.7639 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2118  | total loss: \u001b[1m\u001b[32m0.52326\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 2118 | loss: 0.52326 - acc: 0.7683 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2119  | total loss: \u001b[1m\u001b[32m0.51689\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2119 | loss: 0.51689 - acc: 0.7722 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2120  | total loss: \u001b[1m\u001b[32m0.54956\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 2120 | loss: 0.54956 - acc: 0.7494 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2121  | total loss: \u001b[1m\u001b[32m0.54037\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2121 | loss: 0.54037 - acc: 0.7554 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2122  | total loss: \u001b[1m\u001b[32m0.57143\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 2122 | loss: 0.57143 - acc: 0.7315 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2123  | total loss: \u001b[1m\u001b[32m0.56055\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2123 | loss: 0.56055 - acc: 0.7393 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2124  | total loss: \u001b[1m\u001b[32m0.55117\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2124 | loss: 0.55117 - acc: 0.7464 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2125  | total loss: \u001b[1m\u001b[32m0.54294\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2125 | loss: 0.54294 - acc: 0.7525 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2126  | total loss: \u001b[1m\u001b[32m0.53553\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2126 | loss: 0.53553 - acc: 0.7580 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2127  | total loss: \u001b[1m\u001b[32m0.52861\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2127 | loss: 0.52861 - acc: 0.7631 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2128  | total loss: \u001b[1m\u001b[32m0.52195\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2128 | loss: 0.52195 - acc: 0.7678 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2129  | total loss: \u001b[1m\u001b[32m0.51545\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2129 | loss: 0.51545 - acc: 0.7720 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2130  | total loss: \u001b[1m\u001b[32m0.50912\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2130 | loss: 0.50912 - acc: 0.7761 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2131  | total loss: \u001b[1m\u001b[32m0.50306\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2131 | loss: 0.50306 - acc: 0.7796 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2132  | total loss: \u001b[1m\u001b[32m0.49739\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2132 | loss: 0.49739 - acc: 0.7827 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2133  | total loss: \u001b[1m\u001b[32m0.49223\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 2133 | loss: 0.49223 - acc: 0.7855 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2134  | total loss: \u001b[1m\u001b[32m0.53750\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2134 | loss: 0.53750 - acc: 0.7631 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2135  | total loss: \u001b[1m\u001b[32m0.52826\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2135 | loss: 0.52826 - acc: 0.7677 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2136  | total loss: \u001b[1m\u001b[32m0.51984\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2136 | loss: 0.51984 - acc: 0.7719 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2137  | total loss: \u001b[1m\u001b[32m0.51216\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2137 | loss: 0.51216 - acc: 0.7756 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2138  | total loss: \u001b[1m\u001b[32m0.55586\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 2138 | loss: 0.55586 - acc: 0.7525 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2139  | total loss: \u001b[1m\u001b[32m0.54460\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2139 | loss: 0.54460 - acc: 0.7578 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2140  | total loss: \u001b[1m\u001b[32m0.57883\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 2140 | loss: 0.57883 - acc: 0.7366 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2141  | total loss: \u001b[1m\u001b[32m0.56669\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2141 | loss: 0.56669 - acc: 0.7427 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2142  | total loss: \u001b[1m\u001b[32m0.59477\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2142 | loss: 0.59477 - acc: 0.7214 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2143  | total loss: \u001b[1m\u001b[32m0.58391\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2143 | loss: 0.58391 - acc: 0.7292 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2144  | total loss: \u001b[1m\u001b[32m0.60407\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2144 | loss: 0.60407 - acc: 0.7088 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2145  | total loss: \u001b[1m\u001b[32m0.59489\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2145 | loss: 0.59489 - acc: 0.7173 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2146  | total loss: \u001b[1m\u001b[32m0.61072\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2146 | loss: 0.61072 - acc: 0.6974 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2147  | total loss: \u001b[1m\u001b[32m0.60259\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2147 | loss: 0.60259 - acc: 0.7067 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2148  | total loss: \u001b[1m\u001b[32m0.59566\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2148 | loss: 0.59566 - acc: 0.7152 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2149  | total loss: \u001b[1m\u001b[32m0.58933\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2149 | loss: 0.58933 - acc: 0.7233 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2150  | total loss: \u001b[1m\u001b[32m0.58318\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2150 | loss: 0.58318 - acc: 0.7308 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2151  | total loss: \u001b[1m\u001b[32m0.57694\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2151 | loss: 0.57694 - acc: 0.7384 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2152  | total loss: \u001b[1m\u001b[32m0.57044\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 2152 | loss: 0.57044 - acc: 0.7457 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2153  | total loss: \u001b[1m\u001b[32m0.56366\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2153 | loss: 0.56366 - acc: 0.7511 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2154  | total loss: \u001b[1m\u001b[32m0.55666\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2154 | loss: 0.55666 - acc: 0.7560 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2155  | total loss: \u001b[1m\u001b[32m0.54957\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2155 | loss: 0.54957 - acc: 0.7587 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2156  | total loss: \u001b[1m\u001b[32m0.57560\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2156 | loss: 0.57560 - acc: 0.7387 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2157  | total loss: \u001b[1m\u001b[32m0.56591\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2157 | loss: 0.56591 - acc: 0.7433 -- iter: 891/891\n",
      "--\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data = df\n",
    "net = tflearn.input_data(shape=[None, 9]) \n",
    "net = tflearn.fully_connected(net, 100)\n",
    "net = tflearn.fully_connected(net, 100)\n",
    "net = tflearn.fully_connected(net, 100)\n",
    "net = tflearn.fully_connected(net, 100)\n",
    "net = tflearn.fully_connected(net, 100)\n",
    "net = tflearn.fully_connected(net, 2, activation='softmax') \n",
    "net = tflearn.regression(net)\n",
    "model = tflearn.DNN(net, tensorboard_verbose=0)\n",
    "model.fit(data, labels, n_epoch=5000, batch_size=891, show_metric=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(test)\n",
    "pred = pd.DataFrame(pred)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "end = pd.read_csv('../titanic_kaggle/test.csv')\n",
    "end['Survival_chance'] = pred[1]\n",
    "end['Die_chance'] = pred[0]\n",
    "state = []\n",
    "for i in range(len(end.Survival_chance)):\n",
    "    if end.Survival_chance[i] > end.Die_chance[i]:\n",
    "        state.append(1)\n",
    "    else:\n",
    "        state.append(0)\n",
    "state\n",
    "end['Survived'] = state\n",
    "submit = end[['PassengerId', 'Survived']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submit.to_csv('titanic_pred.csv',  index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
