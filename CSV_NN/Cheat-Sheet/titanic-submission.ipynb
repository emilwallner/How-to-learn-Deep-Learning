{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hdf5 is not supported on this machine (please install/reinstall h5py for optimal experience)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tflearn as tflearn\n",
    "from clean_data import clean_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>Moran, Mr. James</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>330877</td>\n",
       "      <td>8.4583</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>McCarthy, Mr. Timothy J</td>\n",
       "      <td>male</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17463</td>\n",
       "      <td>51.8625</td>\n",
       "      <td>E46</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>Palsson, Master. Gosta Leonard</td>\n",
       "      <td>male</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>349909</td>\n",
       "      <td>21.0750</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)</td>\n",
       "      <td>female</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>347742</td>\n",
       "      <td>11.1333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>Nasser, Mrs. Nicholas (Adele Achem)</td>\n",
       "      <td>female</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>237736</td>\n",
       "      <td>30.0708</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>Sandstrom, Miss. Marguerite Rut</td>\n",
       "      <td>female</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>PP 9549</td>\n",
       "      <td>16.7000</td>\n",
       "      <td>G6</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>Bonnell, Miss. Elizabeth</td>\n",
       "      <td>female</td>\n",
       "      <td>58.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>113783</td>\n",
       "      <td>26.5500</td>\n",
       "      <td>C103</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>Saundercock, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5. 2151</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "      <td>Andersson, Mr. Anders Johan</td>\n",
       "      <td>male</td>\n",
       "      <td>39.0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>347082</td>\n",
       "      <td>31.2750</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>Vestrom, Miss. Hulda Amanda Adolfina</td>\n",
       "      <td>female</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>350406</td>\n",
       "      <td>7.8542</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>Hewlett, Mrs. (Mary D Kingcome)</td>\n",
       "      <td>female</td>\n",
       "      <td>55.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>248706</td>\n",
       "      <td>16.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>3</td>\n",
       "      <td>Rice, Master. Eugene</td>\n",
       "      <td>male</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>382652</td>\n",
       "      <td>29.1250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>2</td>\n",
       "      <td>Williams, Mr. Charles Eugene</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>244373</td>\n",
       "      <td>13.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>3</td>\n",
       "      <td>Vander Planke, Mrs. Julius (Emelia Maria Vande...</td>\n",
       "      <td>female</td>\n",
       "      <td>31.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>345763</td>\n",
       "      <td>18.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>3</td>\n",
       "      <td>Masselmani, Mrs. Fatima</td>\n",
       "      <td>female</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2649</td>\n",
       "      <td>7.2250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>2</td>\n",
       "      <td>Fynney, Mr. Joseph J</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>239865</td>\n",
       "      <td>26.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>2</td>\n",
       "      <td>Beesley, Mr. Lawrence</td>\n",
       "      <td>male</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>248698</td>\n",
       "      <td>13.0000</td>\n",
       "      <td>D56</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>3</td>\n",
       "      <td>McGowan, Miss. Anna \"Annie\"</td>\n",
       "      <td>female</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>330923</td>\n",
       "      <td>8.0292</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>Sloper, Mr. William Thompson</td>\n",
       "      <td>male</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>113788</td>\n",
       "      <td>35.5000</td>\n",
       "      <td>A6</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>3</td>\n",
       "      <td>Palsson, Miss. Torborg Danira</td>\n",
       "      <td>female</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>349909</td>\n",
       "      <td>21.0750</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>3</td>\n",
       "      <td>Asplund, Mrs. Carl Oscar (Selma Augusta Emilia...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>347077</td>\n",
       "      <td>31.3875</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>3</td>\n",
       "      <td>Emir, Mr. Farred Chehab</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2631</td>\n",
       "      <td>7.2250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "      <td>Fortune, Mr. Charles Alexander</td>\n",
       "      <td>male</td>\n",
       "      <td>19.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>19950</td>\n",
       "      <td>263.0000</td>\n",
       "      <td>C23 C25 C27</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29</td>\n",
       "      <td>3</td>\n",
       "      <td>O'Dwyer, Miss. Ellen \"Nellie\"</td>\n",
       "      <td>female</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>330959</td>\n",
       "      <td>7.8792</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30</td>\n",
       "      <td>3</td>\n",
       "      <td>Todoroff, Mr. Lalio</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>349216</td>\n",
       "      <td>7.8958</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>861</th>\n",
       "      <td>862</td>\n",
       "      <td>2</td>\n",
       "      <td>Giles, Mr. Frederick Edward</td>\n",
       "      <td>male</td>\n",
       "      <td>21.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>28134</td>\n",
       "      <td>11.5000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>862</th>\n",
       "      <td>863</td>\n",
       "      <td>1</td>\n",
       "      <td>Swift, Mrs. Frederick Joel (Margaret Welles Ba...</td>\n",
       "      <td>female</td>\n",
       "      <td>48.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17466</td>\n",
       "      <td>25.9292</td>\n",
       "      <td>D17</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>863</th>\n",
       "      <td>864</td>\n",
       "      <td>3</td>\n",
       "      <td>Sage, Miss. Dorothy Edith \"Dolly\"</td>\n",
       "      <td>female</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>CA. 2343</td>\n",
       "      <td>69.5500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>864</th>\n",
       "      <td>865</td>\n",
       "      <td>2</td>\n",
       "      <td>Gill, Mr. John William</td>\n",
       "      <td>male</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>233866</td>\n",
       "      <td>13.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>865</th>\n",
       "      <td>866</td>\n",
       "      <td>2</td>\n",
       "      <td>Bystrom, Mrs. (Karolina)</td>\n",
       "      <td>female</td>\n",
       "      <td>42.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>236852</td>\n",
       "      <td>13.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>866</th>\n",
       "      <td>867</td>\n",
       "      <td>2</td>\n",
       "      <td>Duran y More, Miss. Asuncion</td>\n",
       "      <td>female</td>\n",
       "      <td>27.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>SC/PARIS 2149</td>\n",
       "      <td>13.8583</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>867</th>\n",
       "      <td>868</td>\n",
       "      <td>1</td>\n",
       "      <td>Roebling, Mr. Washington Augustus II</td>\n",
       "      <td>male</td>\n",
       "      <td>31.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17590</td>\n",
       "      <td>50.4958</td>\n",
       "      <td>A24</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>868</th>\n",
       "      <td>869</td>\n",
       "      <td>3</td>\n",
       "      <td>van Melkebeke, Mr. Philemon</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>345777</td>\n",
       "      <td>9.5000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>869</th>\n",
       "      <td>870</td>\n",
       "      <td>3</td>\n",
       "      <td>Johnson, Master. Harold Theodor</td>\n",
       "      <td>male</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>347742</td>\n",
       "      <td>11.1333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>870</th>\n",
       "      <td>871</td>\n",
       "      <td>3</td>\n",
       "      <td>Balkic, Mr. Cerin</td>\n",
       "      <td>male</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>349248</td>\n",
       "      <td>7.8958</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>871</th>\n",
       "      <td>872</td>\n",
       "      <td>1</td>\n",
       "      <td>Beckwith, Mrs. Richard Leonard (Sallie Monypeny)</td>\n",
       "      <td>female</td>\n",
       "      <td>47.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>11751</td>\n",
       "      <td>52.5542</td>\n",
       "      <td>D35</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>872</th>\n",
       "      <td>873</td>\n",
       "      <td>1</td>\n",
       "      <td>Carlsson, Mr. Frans Olof</td>\n",
       "      <td>male</td>\n",
       "      <td>33.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>695</td>\n",
       "      <td>5.0000</td>\n",
       "      <td>B51 B53 B55</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>873</th>\n",
       "      <td>874</td>\n",
       "      <td>3</td>\n",
       "      <td>Vander Cruyssen, Mr. Victor</td>\n",
       "      <td>male</td>\n",
       "      <td>47.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>345765</td>\n",
       "      <td>9.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>874</th>\n",
       "      <td>875</td>\n",
       "      <td>2</td>\n",
       "      <td>Abelson, Mrs. Samuel (Hannah Wizosky)</td>\n",
       "      <td>female</td>\n",
       "      <td>28.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>P/PP 3381</td>\n",
       "      <td>24.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>875</th>\n",
       "      <td>876</td>\n",
       "      <td>3</td>\n",
       "      <td>Najib, Miss. Adele Kiamie \"Jane\"</td>\n",
       "      <td>female</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2667</td>\n",
       "      <td>7.2250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>876</th>\n",
       "      <td>877</td>\n",
       "      <td>3</td>\n",
       "      <td>Gustafsson, Mr. Alfred Ossian</td>\n",
       "      <td>male</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7534</td>\n",
       "      <td>9.8458</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>877</th>\n",
       "      <td>878</td>\n",
       "      <td>3</td>\n",
       "      <td>Petroff, Mr. Nedelio</td>\n",
       "      <td>male</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>349212</td>\n",
       "      <td>7.8958</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>878</th>\n",
       "      <td>879</td>\n",
       "      <td>3</td>\n",
       "      <td>Laleff, Mr. Kristo</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>349217</td>\n",
       "      <td>7.8958</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>879</th>\n",
       "      <td>880</td>\n",
       "      <td>1</td>\n",
       "      <td>Potter, Mrs. Thomas Jr (Lily Alexenia Wilson)</td>\n",
       "      <td>female</td>\n",
       "      <td>56.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>11767</td>\n",
       "      <td>83.1583</td>\n",
       "      <td>C50</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>880</th>\n",
       "      <td>881</td>\n",
       "      <td>2</td>\n",
       "      <td>Shelley, Mrs. William (Imanita Parrish Hall)</td>\n",
       "      <td>female</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>230433</td>\n",
       "      <td>26.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>881</th>\n",
       "      <td>882</td>\n",
       "      <td>3</td>\n",
       "      <td>Markun, Mr. Johann</td>\n",
       "      <td>male</td>\n",
       "      <td>33.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>349257</td>\n",
       "      <td>7.8958</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>882</th>\n",
       "      <td>883</td>\n",
       "      <td>3</td>\n",
       "      <td>Dahlberg, Miss. Gerda Ulrika</td>\n",
       "      <td>female</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7552</td>\n",
       "      <td>10.5167</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>883</th>\n",
       "      <td>884</td>\n",
       "      <td>2</td>\n",
       "      <td>Banfield, Mr. Frederick James</td>\n",
       "      <td>male</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>C.A./SOTON 34068</td>\n",
       "      <td>10.5000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>884</th>\n",
       "      <td>885</td>\n",
       "      <td>3</td>\n",
       "      <td>Sutehall, Mr. Henry Jr</td>\n",
       "      <td>male</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>SOTON/OQ 392076</td>\n",
       "      <td>7.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>885</th>\n",
       "      <td>886</td>\n",
       "      <td>3</td>\n",
       "      <td>Rice, Mrs. William (Margaret Norton)</td>\n",
       "      <td>female</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>382652</td>\n",
       "      <td>29.1250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>886</th>\n",
       "      <td>887</td>\n",
       "      <td>2</td>\n",
       "      <td>Montvila, Rev. Juozas</td>\n",
       "      <td>male</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>211536</td>\n",
       "      <td>13.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>888</td>\n",
       "      <td>1</td>\n",
       "      <td>Graham, Miss. Margaret Edith</td>\n",
       "      <td>female</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>112053</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>B42</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>889</td>\n",
       "      <td>3</td>\n",
       "      <td>Johnston, Miss. Catherine Helen \"Carrie\"</td>\n",
       "      <td>female</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>W./C. 6607</td>\n",
       "      <td>23.4500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>890</td>\n",
       "      <td>1</td>\n",
       "      <td>Behr, Mr. Karl Howell</td>\n",
       "      <td>male</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>111369</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>C148</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>891</td>\n",
       "      <td>3</td>\n",
       "      <td>Dooley, Mr. Patrick</td>\n",
       "      <td>male</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>370376</td>\n",
       "      <td>7.7500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>891 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     PassengerId  Pclass                                               Name  \\\n",
       "0              1       3                            Braund, Mr. Owen Harris   \n",
       "1              2       1  Cumings, Mrs. John Bradley (Florence Briggs Th...   \n",
       "2              3       3                             Heikkinen, Miss. Laina   \n",
       "3              4       1       Futrelle, Mrs. Jacques Heath (Lily May Peel)   \n",
       "4              5       3                           Allen, Mr. William Henry   \n",
       "5              6       3                                   Moran, Mr. James   \n",
       "6              7       1                            McCarthy, Mr. Timothy J   \n",
       "7              8       3                     Palsson, Master. Gosta Leonard   \n",
       "8              9       3  Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)   \n",
       "9             10       2                Nasser, Mrs. Nicholas (Adele Achem)   \n",
       "10            11       3                    Sandstrom, Miss. Marguerite Rut   \n",
       "11            12       1                           Bonnell, Miss. Elizabeth   \n",
       "12            13       3                     Saundercock, Mr. William Henry   \n",
       "13            14       3                        Andersson, Mr. Anders Johan   \n",
       "14            15       3               Vestrom, Miss. Hulda Amanda Adolfina   \n",
       "15            16       2                   Hewlett, Mrs. (Mary D Kingcome)    \n",
       "16            17       3                               Rice, Master. Eugene   \n",
       "17            18       2                       Williams, Mr. Charles Eugene   \n",
       "18            19       3  Vander Planke, Mrs. Julius (Emelia Maria Vande...   \n",
       "19            20       3                            Masselmani, Mrs. Fatima   \n",
       "20            21       2                               Fynney, Mr. Joseph J   \n",
       "21            22       2                              Beesley, Mr. Lawrence   \n",
       "22            23       3                        McGowan, Miss. Anna \"Annie\"   \n",
       "23            24       1                       Sloper, Mr. William Thompson   \n",
       "24            25       3                      Palsson, Miss. Torborg Danira   \n",
       "25            26       3  Asplund, Mrs. Carl Oscar (Selma Augusta Emilia...   \n",
       "26            27       3                            Emir, Mr. Farred Chehab   \n",
       "27            28       1                     Fortune, Mr. Charles Alexander   \n",
       "28            29       3                      O'Dwyer, Miss. Ellen \"Nellie\"   \n",
       "29            30       3                                Todoroff, Mr. Lalio   \n",
       "..           ...     ...                                                ...   \n",
       "861          862       2                        Giles, Mr. Frederick Edward   \n",
       "862          863       1  Swift, Mrs. Frederick Joel (Margaret Welles Ba...   \n",
       "863          864       3                  Sage, Miss. Dorothy Edith \"Dolly\"   \n",
       "864          865       2                             Gill, Mr. John William   \n",
       "865          866       2                           Bystrom, Mrs. (Karolina)   \n",
       "866          867       2                       Duran y More, Miss. Asuncion   \n",
       "867          868       1               Roebling, Mr. Washington Augustus II   \n",
       "868          869       3                        van Melkebeke, Mr. Philemon   \n",
       "869          870       3                    Johnson, Master. Harold Theodor   \n",
       "870          871       3                                  Balkic, Mr. Cerin   \n",
       "871          872       1   Beckwith, Mrs. Richard Leonard (Sallie Monypeny)   \n",
       "872          873       1                           Carlsson, Mr. Frans Olof   \n",
       "873          874       3                        Vander Cruyssen, Mr. Victor   \n",
       "874          875       2              Abelson, Mrs. Samuel (Hannah Wizosky)   \n",
       "875          876       3                   Najib, Miss. Adele Kiamie \"Jane\"   \n",
       "876          877       3                      Gustafsson, Mr. Alfred Ossian   \n",
       "877          878       3                               Petroff, Mr. Nedelio   \n",
       "878          879       3                                 Laleff, Mr. Kristo   \n",
       "879          880       1      Potter, Mrs. Thomas Jr (Lily Alexenia Wilson)   \n",
       "880          881       2       Shelley, Mrs. William (Imanita Parrish Hall)   \n",
       "881          882       3                                 Markun, Mr. Johann   \n",
       "882          883       3                       Dahlberg, Miss. Gerda Ulrika   \n",
       "883          884       2                      Banfield, Mr. Frederick James   \n",
       "884          885       3                             Sutehall, Mr. Henry Jr   \n",
       "885          886       3               Rice, Mrs. William (Margaret Norton)   \n",
       "886          887       2                              Montvila, Rev. Juozas   \n",
       "887          888       1                       Graham, Miss. Margaret Edith   \n",
       "888          889       3           Johnston, Miss. Catherine Helen \"Carrie\"   \n",
       "889          890       1                              Behr, Mr. Karl Howell   \n",
       "890          891       3                                Dooley, Mr. Patrick   \n",
       "\n",
       "        Sex   Age  SibSp  Parch            Ticket      Fare        Cabin  \\\n",
       "0      male  22.0      1      0         A/5 21171    7.2500          NaN   \n",
       "1    female  38.0      1      0          PC 17599   71.2833          C85   \n",
       "2    female  26.0      0      0  STON/O2. 3101282    7.9250          NaN   \n",
       "3    female  35.0      1      0            113803   53.1000         C123   \n",
       "4      male  35.0      0      0            373450    8.0500          NaN   \n",
       "5      male   NaN      0      0            330877    8.4583          NaN   \n",
       "6      male  54.0      0      0             17463   51.8625          E46   \n",
       "7      male   2.0      3      1            349909   21.0750          NaN   \n",
       "8    female  27.0      0      2            347742   11.1333          NaN   \n",
       "9    female  14.0      1      0            237736   30.0708          NaN   \n",
       "10   female   4.0      1      1           PP 9549   16.7000           G6   \n",
       "11   female  58.0      0      0            113783   26.5500         C103   \n",
       "12     male  20.0      0      0         A/5. 2151    8.0500          NaN   \n",
       "13     male  39.0      1      5            347082   31.2750          NaN   \n",
       "14   female  14.0      0      0            350406    7.8542          NaN   \n",
       "15   female  55.0      0      0            248706   16.0000          NaN   \n",
       "16     male   2.0      4      1            382652   29.1250          NaN   \n",
       "17     male   NaN      0      0            244373   13.0000          NaN   \n",
       "18   female  31.0      1      0            345763   18.0000          NaN   \n",
       "19   female   NaN      0      0              2649    7.2250          NaN   \n",
       "20     male  35.0      0      0            239865   26.0000          NaN   \n",
       "21     male  34.0      0      0            248698   13.0000          D56   \n",
       "22   female  15.0      0      0            330923    8.0292          NaN   \n",
       "23     male  28.0      0      0            113788   35.5000           A6   \n",
       "24   female   8.0      3      1            349909   21.0750          NaN   \n",
       "25   female  38.0      1      5            347077   31.3875          NaN   \n",
       "26     male   NaN      0      0              2631    7.2250          NaN   \n",
       "27     male  19.0      3      2             19950  263.0000  C23 C25 C27   \n",
       "28   female   NaN      0      0            330959    7.8792          NaN   \n",
       "29     male   NaN      0      0            349216    7.8958          NaN   \n",
       "..      ...   ...    ...    ...               ...       ...          ...   \n",
       "861    male  21.0      1      0             28134   11.5000          NaN   \n",
       "862  female  48.0      0      0             17466   25.9292          D17   \n",
       "863  female   NaN      8      2          CA. 2343   69.5500          NaN   \n",
       "864    male  24.0      0      0            233866   13.0000          NaN   \n",
       "865  female  42.0      0      0            236852   13.0000          NaN   \n",
       "866  female  27.0      1      0     SC/PARIS 2149   13.8583          NaN   \n",
       "867    male  31.0      0      0          PC 17590   50.4958          A24   \n",
       "868    male   NaN      0      0            345777    9.5000          NaN   \n",
       "869    male   4.0      1      1            347742   11.1333          NaN   \n",
       "870    male  26.0      0      0            349248    7.8958          NaN   \n",
       "871  female  47.0      1      1             11751   52.5542          D35   \n",
       "872    male  33.0      0      0               695    5.0000  B51 B53 B55   \n",
       "873    male  47.0      0      0            345765    9.0000          NaN   \n",
       "874  female  28.0      1      0         P/PP 3381   24.0000          NaN   \n",
       "875  female  15.0      0      0              2667    7.2250          NaN   \n",
       "876    male  20.0      0      0              7534    9.8458          NaN   \n",
       "877    male  19.0      0      0            349212    7.8958          NaN   \n",
       "878    male   NaN      0      0            349217    7.8958          NaN   \n",
       "879  female  56.0      0      1             11767   83.1583          C50   \n",
       "880  female  25.0      0      1            230433   26.0000          NaN   \n",
       "881    male  33.0      0      0            349257    7.8958          NaN   \n",
       "882  female  22.0      0      0              7552   10.5167          NaN   \n",
       "883    male  28.0      0      0  C.A./SOTON 34068   10.5000          NaN   \n",
       "884    male  25.0      0      0   SOTON/OQ 392076    7.0500          NaN   \n",
       "885  female  39.0      0      5            382652   29.1250          NaN   \n",
       "886    male  27.0      0      0            211536   13.0000          NaN   \n",
       "887  female  19.0      0      0            112053   30.0000          B42   \n",
       "888  female   NaN      1      2        W./C. 6607   23.4500          NaN   \n",
       "889    male  26.0      0      0            111369   30.0000         C148   \n",
       "890    male  32.0      0      0            370376    7.7500          NaN   \n",
       "\n",
       "    Embarked  \n",
       "0          S  \n",
       "1          C  \n",
       "2          S  \n",
       "3          S  \n",
       "4          S  \n",
       "5          Q  \n",
       "6          S  \n",
       "7          S  \n",
       "8          S  \n",
       "9          C  \n",
       "10         S  \n",
       "11         S  \n",
       "12         S  \n",
       "13         S  \n",
       "14         S  \n",
       "15         S  \n",
       "16         Q  \n",
       "17         S  \n",
       "18         S  \n",
       "19         C  \n",
       "20         S  \n",
       "21         S  \n",
       "22         Q  \n",
       "23         S  \n",
       "24         S  \n",
       "25         S  \n",
       "26         C  \n",
       "27         S  \n",
       "28         Q  \n",
       "29         S  \n",
       "..       ...  \n",
       "861        S  \n",
       "862        S  \n",
       "863        S  \n",
       "864        S  \n",
       "865        S  \n",
       "866        C  \n",
       "867        S  \n",
       "868        S  \n",
       "869        S  \n",
       "870        S  \n",
       "871        S  \n",
       "872        S  \n",
       "873        S  \n",
       "874        C  \n",
       "875        C  \n",
       "876        S  \n",
       "877        S  \n",
       "878        S  \n",
       "879        C  \n",
       "880        S  \n",
       "881        S  \n",
       "882        S  \n",
       "883        S  \n",
       "884        S  \n",
       "885        Q  \n",
       "886        S  \n",
       "887        S  \n",
       "888        S  \n",
       "889        C  \n",
       "890        Q  \n",
       "\n",
       "[891 rows x 11 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../titanic_kaggle/train.csv')\n",
    "labels = pd.get_dummies(df.Survived)\n",
    "df = df.drop('Survived', 1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 22.        ,   1.        ,   0.        , ...,   1.        ,\n",
       "          0.        ,   1.        ],\n",
       "       [ 38.        ,   1.        ,   0.        , ...,   0.        ,\n",
       "          0.        ,   0.        ],\n",
       "       [ 26.        ,   0.        ,   0.        , ...,   1.        ,\n",
       "          0.        ,   1.        ],\n",
       "       ..., \n",
       "       [ 29.69911766,   1.        ,   2.        , ...,   1.        ,\n",
       "          0.        ,   1.        ],\n",
       "       [ 26.        ,   0.        ,   0.        , ...,   0.        ,\n",
       "          0.        ,   0.        ],\n",
       "       [ 32.        ,   0.        ,   0.        , ...,   0.        ,\n",
       "          0.        ,   1.        ]], dtype=float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_csv('../titanic_kaggle/test.csv')\n",
    "df = clean_data(df)\n",
    "test = clean_data(test)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels = np.array(labels, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "Run id: AJGFGU\n",
      "Log directory: /tmp/tflearn_logs/\n",
      "INFO:tensorflow:Summary name Accuracy/ (raw) is illegal; using Accuracy/__raw_ instead.\n",
      "---------------------------------\n",
      "Training samples: 891\n",
      "Validation samples: 0\n",
      "--\n",
      "Training Step: 1  | time: 0.125s\n",
      "| Adam | epoch: 001 | loss: 0.00000 - acc: 0.0000 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2  | total loss: \u001b[1m\u001b[32m0.62382\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 002 | loss: 0.62382 - acc: 0.5455 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3  | total loss: \u001b[1m\u001b[32m0.68006\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 003 | loss: 0.68006 - acc: 0.6033 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4  | total loss: \u001b[1m\u001b[32m0.68856\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 004 | loss: 0.68856 - acc: 0.6129 -- iter: 891/891\n",
      "--\n",
      "Training Step: 5  | total loss: \u001b[1m\u001b[32m0.68981\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 005 | loss: 0.68981 - acc: 0.6152 -- iter: 891/891\n",
      "--\n",
      "Training Step: 6  | total loss: \u001b[1m\u001b[32m0.68813\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 006 | loss: 0.68813 - acc: 0.6158 -- iter: 891/891\n",
      "--\n",
      "Training Step: 7  | total loss: \u001b[1m\u001b[32m0.68647\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 007 | loss: 0.68647 - acc: 0.6160 -- iter: 891/891\n",
      "--\n",
      "Training Step: 8  | total loss: \u001b[1m\u001b[32m0.68283\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 008 | loss: 0.68283 - acc: 0.6161 -- iter: 891/891\n",
      "--\n",
      "Training Step: 9  | total loss: \u001b[1m\u001b[32m0.67947\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 009 | loss: 0.67947 - acc: 0.6161 -- iter: 891/891\n",
      "--\n",
      "Training Step: 10  | total loss: \u001b[1m\u001b[32m0.67394\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 010 | loss: 0.67394 - acc: 0.6161 -- iter: 891/891\n",
      "--\n",
      "Training Step: 11  | total loss: \u001b[1m\u001b[32m0.67288\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 011 | loss: 0.67288 - acc: 0.6162 -- iter: 891/891\n",
      "--\n",
      "Training Step: 12  | total loss: \u001b[1m\u001b[32m0.67379\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 012 | loss: 0.67379 - acc: 0.6162 -- iter: 891/891\n",
      "--\n",
      "Training Step: 13  | total loss: \u001b[1m\u001b[32m0.66521\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 013 | loss: 0.66521 - acc: 0.6181 -- iter: 891/891\n",
      "--\n",
      "Training Step: 14  | total loss: \u001b[1m\u001b[32m0.66732\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 014 | loss: 0.66732 - acc: 0.6173 -- iter: 891/891\n",
      "--\n",
      "Training Step: 15  | total loss: \u001b[1m\u001b[32m0.65889\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 015 | loss: 0.65889 - acc: 0.6243 -- iter: 891/891\n",
      "--\n",
      "Training Step: 16  | total loss: \u001b[1m\u001b[32m0.66535\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 016 | loss: 0.66535 - acc: 0.6133 -- iter: 891/891\n",
      "--\n",
      "Training Step: 17  | total loss: \u001b[1m\u001b[32m0.65532\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 017 | loss: 0.65532 - acc: 0.6280 -- iter: 891/891\n",
      "--\n",
      "Training Step: 18  | total loss: \u001b[1m\u001b[32m0.66286\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 018 | loss: 0.66286 - acc: 0.6189 -- iter: 891/891\n",
      "--\n",
      "Training Step: 19  | total loss: \u001b[1m\u001b[32m0.65135\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 019 | loss: 0.65135 - acc: 0.6352 -- iter: 891/891\n",
      "--\n",
      "Training Step: 20  | total loss: \u001b[1m\u001b[32m0.66474\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 020 | loss: 0.66474 - acc: 0.6172 -- iter: 891/891\n",
      "--\n",
      "Training Step: 21  | total loss: \u001b[1m\u001b[32m0.65216\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 021 | loss: 0.65216 - acc: 0.6325 -- iter: 891/891\n",
      "--\n",
      "Training Step: 22  | total loss: \u001b[1m\u001b[32m0.66532\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 022 | loss: 0.66532 - acc: 0.6216 -- iter: 891/891\n",
      "--\n",
      "Training Step: 23  | total loss: \u001b[1m\u001b[32m0.65391\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 023 | loss: 0.65391 - acc: 0.6320 -- iter: 891/891\n",
      "--\n",
      "Training Step: 24  | total loss: \u001b[1m\u001b[32m0.65887\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 024 | loss: 0.65887 - acc: 0.6244 -- iter: 891/891\n",
      "--\n",
      "Training Step: 25  | total loss: \u001b[1m\u001b[32m0.65103\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 025 | loss: 0.65103 - acc: 0.6320 -- iter: 891/891\n",
      "--\n",
      "Training Step: 26  | total loss: \u001b[1m\u001b[32m0.65553\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 026 | loss: 0.65553 - acc: 0.6290 -- iter: 891/891\n",
      "--\n",
      "Training Step: 27  | total loss: \u001b[1m\u001b[32m0.64978\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 027 | loss: 0.64978 - acc: 0.6335 -- iter: 891/891\n",
      "--\n",
      "Training Step: 28  | total loss: \u001b[1m\u001b[32m0.65535\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 028 | loss: 0.65535 - acc: 0.6277 -- iter: 891/891\n",
      "--\n",
      "Training Step: 29  | total loss: \u001b[1m\u001b[32m0.65030\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 029 | loss: 0.65030 - acc: 0.6304 -- iter: 891/891\n",
      "--\n",
      "Training Step: 30  | total loss: \u001b[1m\u001b[32m0.65407\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 030 | loss: 0.65407 - acc: 0.6246 -- iter: 891/891\n",
      "--\n",
      "Training Step: 31  | total loss: \u001b[1m\u001b[32m0.64950\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 031 | loss: 0.64950 - acc: 0.6278 -- iter: 891/891\n",
      "--\n",
      "Training Step: 32  | total loss: \u001b[1m\u001b[32m0.65420\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 032 | loss: 0.65420 - acc: 0.6235 -- iter: 891/891\n",
      "--\n",
      "Training Step: 33  | total loss: \u001b[1m\u001b[32m0.64949\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 033 | loss: 0.64949 - acc: 0.6268 -- iter: 891/891\n",
      "--\n",
      "Training Step: 34  | total loss: \u001b[1m\u001b[32m0.65493\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 034 | loss: 0.65493 - acc: 0.6233 -- iter: 891/891\n",
      "--\n",
      "Training Step: 35  | total loss: \u001b[1m\u001b[32m0.64991\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 035 | loss: 0.64991 - acc: 0.6267 -- iter: 891/891\n",
      "--\n",
      "Training Step: 36  | total loss: \u001b[1m\u001b[32m0.65529\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 036 | loss: 0.65529 - acc: 0.6227 -- iter: 891/891\n",
      "--\n",
      "Training Step: 37  | total loss: \u001b[1m\u001b[32m0.65016\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 037 | loss: 0.65016 - acc: 0.6273 -- iter: 891/891\n",
      "--\n",
      "Training Step: 38  | total loss: \u001b[1m\u001b[32m0.64587\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 038 | loss: 0.64587 - acc: 0.6310 -- iter: 891/891\n",
      "--\n",
      "Training Step: 39  | total loss: \u001b[1m\u001b[32m0.64193\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 039 | loss: 0.64193 - acc: 0.6357 -- iter: 891/891\n",
      "--\n",
      "Training Step: 40  | total loss: \u001b[1m\u001b[32m0.63805\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 040 | loss: 0.63805 - acc: 0.6402 -- iter: 891/891\n",
      "--\n",
      "Training Step: 41  | total loss: \u001b[1m\u001b[32m0.63416\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 041 | loss: 0.63416 - acc: 0.6459 -- iter: 891/891\n",
      "--\n",
      "Training Step: 42  | total loss: \u001b[1m\u001b[32m0.63040\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 042 | loss: 0.63040 - acc: 0.6513 -- iter: 891/891\n",
      "--\n",
      "Training Step: 43  | total loss: \u001b[1m\u001b[32m0.62700\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 043 | loss: 0.62700 - acc: 0.6568 -- iter: 891/891\n",
      "--\n",
      "Training Step: 44  | total loss: \u001b[1m\u001b[32m0.62424\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 044 | loss: 0.62424 - acc: 0.6631 -- iter: 891/891\n",
      "--\n",
      "Training Step: 45  | total loss: \u001b[1m\u001b[32m0.62224\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 045 | loss: 0.62224 - acc: 0.6683 -- iter: 891/891\n",
      "--\n",
      "Training Step: 46  | total loss: \u001b[1m\u001b[32m0.62087\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 046 | loss: 0.62087 - acc: 0.6731 -- iter: 891/891\n",
      "--\n",
      "Training Step: 47  | total loss: \u001b[1m\u001b[32m0.61968\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 047 | loss: 0.61968 - acc: 0.6763 -- iter: 891/891\n",
      "--\n",
      "Training Step: 48  | total loss: \u001b[1m\u001b[32m0.64705\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 048 | loss: 0.64705 - acc: 0.6587 -- iter: 891/891\n",
      "--\n",
      "Training Step: 49  | total loss: \u001b[1m\u001b[32m0.64077\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 049 | loss: 0.64077 - acc: 0.6652 -- iter: 891/891\n",
      "--\n",
      "Training Step: 50  | total loss: \u001b[1m\u001b[32m0.63593\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 050 | loss: 0.63593 - acc: 0.6672 -- iter: 891/891\n",
      "--\n",
      "Training Step: 51  | total loss: \u001b[1m\u001b[32m0.63254\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 051 | loss: 0.63254 - acc: 0.6680 -- iter: 891/891\n",
      "--\n",
      "Training Step: 52  | total loss: \u001b[1m\u001b[32m0.63025\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 052 | loss: 0.63025 - acc: 0.6676 -- iter: 891/891\n",
      "--\n",
      "Training Step: 53  | total loss: \u001b[1m\u001b[32m0.62865\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 053 | loss: 0.62865 - acc: 0.6658 -- iter: 891/891\n",
      "--\n",
      "Training Step: 54  | total loss: \u001b[1m\u001b[32m0.62744\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 054 | loss: 0.62744 - acc: 0.6635 -- iter: 891/891\n",
      "--\n",
      "Training Step: 55  | total loss: \u001b[1m\u001b[32m0.62636\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 055 | loss: 0.62636 - acc: 0.6611 -- iter: 891/891\n",
      "--\n",
      "Training Step: 56  | total loss: \u001b[1m\u001b[32m0.63486\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 056 | loss: 0.63486 - acc: 0.6530 -- iter: 891/891\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 57  | total loss: \u001b[1m\u001b[32m0.63256\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 057 | loss: 0.63256 - acc: 0.6521 -- iter: 891/891\n",
      "--\n",
      "Training Step: 58  | total loss: \u001b[1m\u001b[32m0.64104\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 058 | loss: 0.64104 - acc: 0.6445 -- iter: 891/891\n",
      "--\n",
      "Training Step: 59  | total loss: \u001b[1m\u001b[32m0.63796\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 059 | loss: 0.63796 - acc: 0.6453 -- iter: 891/891\n",
      "--\n",
      "Training Step: 60  | total loss: \u001b[1m\u001b[32m0.64369\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 060 | loss: 0.64369 - acc: 0.6391 -- iter: 891/891\n",
      "--\n",
      "Training Step: 61  | total loss: \u001b[1m\u001b[32m0.64067\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 061 | loss: 0.64067 - acc: 0.6411 -- iter: 891/891\n",
      "--\n",
      "Training Step: 62  | total loss: \u001b[1m\u001b[32m0.63810\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 062 | loss: 0.63810 - acc: 0.6435 -- iter: 891/891\n",
      "--\n",
      "Training Step: 63  | total loss: \u001b[1m\u001b[32m0.63564\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 063 | loss: 0.63564 - acc: 0.6462 -- iter: 891/891\n",
      "--\n",
      "Training Step: 64  | total loss: \u001b[1m\u001b[32m0.63300\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 064 | loss: 0.63300 - acc: 0.6489 -- iter: 891/891\n",
      "--\n",
      "Training Step: 65  | total loss: \u001b[1m\u001b[32m0.62999\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 065 | loss: 0.62999 - acc: 0.6512 -- iter: 891/891\n",
      "--\n",
      "Training Step: 66  | total loss: \u001b[1m\u001b[32m0.62658\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 066 | loss: 0.62658 - acc: 0.6538 -- iter: 891/891\n",
      "--\n",
      "Training Step: 67  | total loss: \u001b[1m\u001b[32m0.62289\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 067 | loss: 0.62289 - acc: 0.6568 -- iter: 891/891\n",
      "--\n",
      "Training Step: 68  | total loss: \u001b[1m\u001b[32m0.61919\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 068 | loss: 0.61919 - acc: 0.6605 -- iter: 891/891\n",
      "--\n",
      "Training Step: 69  | total loss: \u001b[1m\u001b[32m0.61574\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 069 | loss: 0.61574 - acc: 0.6649 -- iter: 891/891\n",
      "--\n",
      "Training Step: 70  | total loss: \u001b[1m\u001b[32m0.61276\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 070 | loss: 0.61276 - acc: 0.6696 -- iter: 891/891\n",
      "--\n",
      "Training Step: 71  | total loss: \u001b[1m\u001b[32m0.61033\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 071 | loss: 0.61033 - acc: 0.6744 -- iter: 891/891\n",
      "--\n",
      "Training Step: 72  | total loss: \u001b[1m\u001b[32m0.60824\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 072 | loss: 0.60824 - acc: 0.6782 -- iter: 891/891\n",
      "--\n",
      "Training Step: 73  | total loss: \u001b[1m\u001b[32m0.60592\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 073 | loss: 0.60592 - acc: 0.6818 -- iter: 891/891\n",
      "--\n",
      "Training Step: 74  | total loss: \u001b[1m\u001b[32m0.63199\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 074 | loss: 0.63199 - acc: 0.6703 -- iter: 891/891\n",
      "--\n",
      "Training Step: 75  | total loss: \u001b[1m\u001b[32m0.62587\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 075 | loss: 0.62587 - acc: 0.6732 -- iter: 891/891\n",
      "--\n",
      "Training Step: 76  | total loss: \u001b[1m\u001b[32m0.63729\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 076 | loss: 0.63729 - acc: 0.6614 -- iter: 891/891\n",
      "--\n",
      "Training Step: 77  | total loss: \u001b[1m\u001b[32m0.63321\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 077 | loss: 0.63321 - acc: 0.6620 -- iter: 891/891\n",
      "--\n",
      "Training Step: 78  | total loss: \u001b[1m\u001b[32m0.63756\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 078 | loss: 0.63756 - acc: 0.6563 -- iter: 891/891\n",
      "--\n",
      "Training Step: 79  | total loss: \u001b[1m\u001b[32m0.63706\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 079 | loss: 0.63706 - acc: 0.6523 -- iter: 891/891\n",
      "--\n",
      "Training Step: 80  | total loss: \u001b[1m\u001b[32m0.63770\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 080 | loss: 0.63770 - acc: 0.6486 -- iter: 891/891\n",
      "--\n",
      "Training Step: 81  | total loss: \u001b[1m\u001b[32m0.63883\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 081 | loss: 0.63883 - acc: 0.6453 -- iter: 891/891\n",
      "--\n",
      "Training Step: 82  | total loss: \u001b[1m\u001b[32m0.64002\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 082 | loss: 0.64002 - acc: 0.6424 -- iter: 891/891\n",
      "--\n",
      "Training Step: 83  | total loss: \u001b[1m\u001b[32m0.64102\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 083 | loss: 0.64102 - acc: 0.6398 -- iter: 891/891\n",
      "--\n",
      "Training Step: 84  | total loss: \u001b[1m\u001b[32m0.64164\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 084 | loss: 0.64164 - acc: 0.6374 -- iter: 891/891\n",
      "--\n",
      "Training Step: 85  | total loss: \u001b[1m\u001b[32m0.64174\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 085 | loss: 0.64174 - acc: 0.6353 -- iter: 891/891\n",
      "--\n",
      "Training Step: 86  | total loss: \u001b[1m\u001b[32m0.64485\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 086 | loss: 0.64485 - acc: 0.6337 -- iter: 891/891\n",
      "--\n",
      "Training Step: 87  | total loss: \u001b[1m\u001b[32m0.64340\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 087 | loss: 0.64340 - acc: 0.6329 -- iter: 891/891\n",
      "--\n",
      "Training Step: 88  | total loss: \u001b[1m\u001b[32m0.64131\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 088 | loss: 0.64131 - acc: 0.6327 -- iter: 891/891\n",
      "--\n",
      "Training Step: 89  | total loss: \u001b[1m\u001b[32m0.63847\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 089 | loss: 0.63847 - acc: 0.6336 -- iter: 891/891\n",
      "--\n",
      "Training Step: 90  | total loss: \u001b[1m\u001b[32m0.64430\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 090 | loss: 0.64430 - acc: 0.6302 -- iter: 891/891\n",
      "--\n",
      "Training Step: 91  | total loss: \u001b[1m\u001b[32m0.63950\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 091 | loss: 0.63950 - acc: 0.6338 -- iter: 891/891\n",
      "--\n",
      "Training Step: 92  | total loss: \u001b[1m\u001b[32m0.64468\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 092 | loss: 0.64468 - acc: 0.6309 -- iter: 891/891\n",
      "--\n",
      "Training Step: 93  | total loss: \u001b[1m\u001b[32m0.63878\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 093 | loss: 0.63878 - acc: 0.6360 -- iter: 891/891\n",
      "--\n",
      "Training Step: 94  | total loss: \u001b[1m\u001b[32m0.63302\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 094 | loss: 0.63302 - acc: 0.6413 -- iter: 891/891\n",
      "--\n",
      "Training Step: 95  | total loss: \u001b[1m\u001b[32m0.62734\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 095 | loss: 0.62734 - acc: 0.6472 -- iter: 891/891\n",
      "--\n",
      "Training Step: 96  | total loss: \u001b[1m\u001b[32m0.62177\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 096 | loss: 0.62177 - acc: 0.6531 -- iter: 891/891\n",
      "--\n",
      "Training Step: 97  | total loss: \u001b[1m\u001b[32m0.61640\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 097 | loss: 0.61640 - acc: 0.6594 -- iter: 891/891\n",
      "--\n",
      "Training Step: 98  | total loss: \u001b[1m\u001b[32m0.63387\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 098 | loss: 0.63387 - acc: 0.6499 -- iter: 891/891\n",
      "--\n",
      "Training Step: 99  | total loss: \u001b[1m\u001b[32m0.62693\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 099 | loss: 0.62693 - acc: 0.6571 -- iter: 891/891\n",
      "--\n",
      "Training Step: 100  | total loss: \u001b[1m\u001b[32m0.62052\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 100 | loss: 0.62052 - acc: 0.6630 -- iter: 891/891\n",
      "--\n",
      "Training Step: 101  | total loss: \u001b[1m\u001b[32m0.61460\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 101 | loss: 0.61460 - acc: 0.6680 -- iter: 891/891\n",
      "--\n",
      "Training Step: 102  | total loss: \u001b[1m\u001b[32m0.62592\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 102 | loss: 0.62592 - acc: 0.6623 -- iter: 891/891\n",
      "--\n",
      "Training Step: 103  | total loss: \u001b[1m\u001b[32m0.61952\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 103 | loss: 0.61952 - acc: 0.6673 -- iter: 891/891\n",
      "--\n",
      "Training Step: 104  | total loss: \u001b[1m\u001b[32m0.61401\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 104 | loss: 0.61401 - acc: 0.6718 -- iter: 891/891\n",
      "--\n",
      "Training Step: 105  | total loss: \u001b[1m\u001b[32m0.60917\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 105 | loss: 0.60917 - acc: 0.6755 -- iter: 891/891\n",
      "--\n",
      "Training Step: 106  | total loss: \u001b[1m\u001b[32m0.61991\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 106 | loss: 0.61991 - acc: 0.6661 -- iter: 891/891\n",
      "--\n",
      "Training Step: 107  | total loss: \u001b[1m\u001b[32m0.61487\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 107 | loss: 0.61487 - acc: 0.6704 -- iter: 891/891\n",
      "--\n",
      "Training Step: 108  | total loss: \u001b[1m\u001b[32m0.62306\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 108 | loss: 0.62306 - acc: 0.6621 -- iter: 891/891\n",
      "--\n",
      "Training Step: 109  | total loss: \u001b[1m\u001b[32m0.61862\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 109 | loss: 0.61862 - acc: 0.6678 -- iter: 891/891\n",
      "--\n",
      "Training Step: 110  | total loss: \u001b[1m\u001b[32m0.61482\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 110 | loss: 0.61482 - acc: 0.6725 -- iter: 891/891\n",
      "--\n",
      "Training Step: 111  | total loss: \u001b[1m\u001b[32m0.61098\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 111 | loss: 0.61098 - acc: 0.6774 -- iter: 891/891\n",
      "--\n",
      "Training Step: 112  | total loss: \u001b[1m\u001b[32m0.61988\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 112 | loss: 0.61988 - acc: 0.6676 -- iter: 891/891\n",
      "--\n",
      "Training Step: 113  | total loss: \u001b[1m\u001b[32m0.61421\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 113 | loss: 0.61421 - acc: 0.6745 -- iter: 891/891\n",
      "--\n",
      "Training Step: 114  | total loss: \u001b[1m\u001b[32m0.60828\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 114 | loss: 0.60828 - acc: 0.6817 -- iter: 891/891\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 115  | total loss: \u001b[1m\u001b[32m0.60185\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 115 | loss: 0.60185 - acc: 0.6898 -- iter: 891/891\n",
      "--\n",
      "Training Step: 116  | total loss: \u001b[1m\u001b[32m0.61581\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 116 | loss: 0.61581 - acc: 0.6780 -- iter: 891/891\n",
      "--\n",
      "Training Step: 117  | total loss: \u001b[1m\u001b[32m0.60740\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 117 | loss: 0.60740 - acc: 0.6866 -- iter: 891/891\n",
      "--\n",
      "Training Step: 118  | total loss: \u001b[1m\u001b[32m0.62158\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 118 | loss: 0.62158 - acc: 0.6751 -- iter: 891/891\n",
      "--\n",
      "Training Step: 119  | total loss: \u001b[1m\u001b[32m0.61254\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 119 | loss: 0.61254 - acc: 0.6839 -- iter: 891/891\n",
      "--\n",
      "Training Step: 120  | total loss: \u001b[1m\u001b[32m0.60453\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 120 | loss: 0.60453 - acc: 0.6925 -- iter: 891/891\n",
      "--\n",
      "Training Step: 121  | total loss: \u001b[1m\u001b[32m0.59724\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 121 | loss: 0.59724 - acc: 0.6999 -- iter: 891/891\n",
      "--\n",
      "Training Step: 122  | total loss: \u001b[1m\u001b[32m0.59025\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 122 | loss: 0.59025 - acc: 0.7066 -- iter: 891/891\n",
      "--\n",
      "Training Step: 123  | total loss: \u001b[1m\u001b[32m0.58329\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 123 | loss: 0.58329 - acc: 0.7134 -- iter: 891/891\n",
      "--\n",
      "Training Step: 124  | total loss: \u001b[1m\u001b[32m0.57636\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 124 | loss: 0.57636 - acc: 0.7203 -- iter: 891/891\n",
      "--\n",
      "Training Step: 125  | total loss: \u001b[1m\u001b[32m0.56955\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 125 | loss: 0.56955 - acc: 0.7266 -- iter: 891/891\n",
      "--\n",
      "Training Step: 126  | total loss: \u001b[1m\u001b[32m0.56316\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 126 | loss: 0.56316 - acc: 0.7322 -- iter: 891/891\n",
      "--\n",
      "Training Step: 127  | total loss: \u001b[1m\u001b[32m0.55722\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 127 | loss: 0.55722 - acc: 0.7368 -- iter: 891/891\n",
      "--\n",
      "Training Step: 128  | total loss: \u001b[1m\u001b[32m0.59548\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 128 | loss: 0.59548 - acc: 0.7185 -- iter: 891/891\n",
      "--\n",
      "Training Step: 129  | total loss: \u001b[1m\u001b[32m0.58535\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 129 | loss: 0.58535 - acc: 0.7255 -- iter: 891/891\n",
      "--\n",
      "Training Step: 130  | total loss: \u001b[1m\u001b[32m0.61304\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 130 | loss: 0.61304 - acc: 0.7073 -- iter: 891/891\n",
      "--\n",
      "Training Step: 131  | total loss: \u001b[1m\u001b[32m0.60525\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 131 | loss: 0.60525 - acc: 0.7143 -- iter: 891/891\n",
      "--\n",
      "Training Step: 132  | total loss: \u001b[1m\u001b[32m0.62109\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 132 | loss: 0.62109 - acc: 0.6962 -- iter: 891/891\n",
      "--\n",
      "Training Step: 133  | total loss: \u001b[1m\u001b[32m0.61704\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 133 | loss: 0.61704 - acc: 0.6979 -- iter: 891/891\n",
      "--\n",
      "Training Step: 134  | total loss: \u001b[1m\u001b[32m0.61250\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 134 | loss: 0.61250 - acc: 0.7032 -- iter: 891/891\n",
      "--\n",
      "Training Step: 135  | total loss: \u001b[1m\u001b[32m0.60639\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 135 | loss: 0.60639 - acc: 0.7115 -- iter: 891/891\n",
      "--\n",
      "Training Step: 136  | total loss: \u001b[1m\u001b[32m0.59925\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 136 | loss: 0.59925 - acc: 0.7211 -- iter: 891/891\n",
      "--\n",
      "Training Step: 137  | total loss: \u001b[1m\u001b[32m0.59184\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 137 | loss: 0.59184 - acc: 0.7280 -- iter: 891/891\n",
      "--\n",
      "Training Step: 138  | total loss: \u001b[1m\u001b[32m0.58475\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 138 | loss: 0.58475 - acc: 0.7328 -- iter: 891/891\n",
      "--\n",
      "Training Step: 139  | total loss: \u001b[1m\u001b[32m0.57862\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 139 | loss: 0.57862 - acc: 0.7356 -- iter: 891/891\n",
      "--\n",
      "Training Step: 140  | total loss: \u001b[1m\u001b[32m0.57366\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 140 | loss: 0.57366 - acc: 0.7379 -- iter: 891/891\n",
      "--\n",
      "Training Step: 141  | total loss: \u001b[1m\u001b[32m0.56966\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 141 | loss: 0.56966 - acc: 0.7398 -- iter: 891/891\n",
      "--\n",
      "Training Step: 142  | total loss: \u001b[1m\u001b[32m0.56638\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 142 | loss: 0.56638 - acc: 0.7420 -- iter: 891/891\n",
      "--\n",
      "Training Step: 143  | total loss: \u001b[1m\u001b[32m0.56349\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 143 | loss: 0.56349 - acc: 0.7440 -- iter: 891/891\n",
      "--\n",
      "Training Step: 144  | total loss: \u001b[1m\u001b[32m0.56031\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 144 | loss: 0.56031 - acc: 0.7458 -- iter: 891/891\n",
      "--\n",
      "Training Step: 145  | total loss: \u001b[1m\u001b[32m0.55646\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 145 | loss: 0.55646 - acc: 0.7488 -- iter: 891/891\n",
      "--\n",
      "Training Step: 146  | total loss: \u001b[1m\u001b[32m0.55204\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 146 | loss: 0.55204 - acc: 0.7521 -- iter: 891/891\n",
      "--\n",
      "Training Step: 147  | total loss: \u001b[1m\u001b[32m0.54720\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 147 | loss: 0.54720 - acc: 0.7549 -- iter: 891/891\n",
      "--\n",
      "Training Step: 148  | total loss: \u001b[1m\u001b[32m0.57623\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 148 | loss: 0.57623 - acc: 0.7350 -- iter: 891/891\n",
      "--\n",
      "Training Step: 149  | total loss: \u001b[1m\u001b[32m0.56861\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 149 | loss: 0.56861 - acc: 0.7420 -- iter: 891/891\n",
      "--\n",
      "Training Step: 150  | total loss: \u001b[1m\u001b[32m0.56316\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 150 | loss: 0.56316 - acc: 0.7487 -- iter: 891/891\n",
      "--\n",
      "Training Step: 151  | total loss: \u001b[1m\u001b[32m0.55957\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 151 | loss: 0.55957 - acc: 0.7541 -- iter: 891/891\n",
      "--\n",
      "Training Step: 152  | total loss: \u001b[1m\u001b[32m0.55651\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 152 | loss: 0.55651 - acc: 0.7591 -- iter: 891/891\n",
      "--\n",
      "Training Step: 153  | total loss: \u001b[1m\u001b[32m0.55239\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 153 | loss: 0.55239 - acc: 0.7635 -- iter: 891/891\n",
      "--\n",
      "Training Step: 154  | total loss: \u001b[1m\u001b[32m0.57514\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 154 | loss: 0.57514 - acc: 0.7430 -- iter: 891/891\n",
      "--\n",
      "Training Step: 155  | total loss: \u001b[1m\u001b[32m0.56668\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 155 | loss: 0.56668 - acc: 0.7493 -- iter: 891/891\n",
      "--\n",
      "Training Step: 156  | total loss: \u001b[1m\u001b[32m0.55838\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 156 | loss: 0.55838 - acc: 0.7551 -- iter: 891/891\n",
      "--\n",
      "Training Step: 157  | total loss: \u001b[1m\u001b[32m0.55018\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 157 | loss: 0.55018 - acc: 0.7600 -- iter: 891/891\n",
      "--\n",
      "Training Step: 158  | total loss: \u001b[1m\u001b[32m0.54251\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 158 | loss: 0.54251 - acc: 0.7647 -- iter: 891/891\n",
      "--\n",
      "Training Step: 159  | total loss: \u001b[1m\u001b[32m0.53560\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 159 | loss: 0.53560 - acc: 0.7691 -- iter: 891/891\n",
      "--\n",
      "Training Step: 160  | total loss: \u001b[1m\u001b[32m0.52952\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 160 | loss: 0.52952 - acc: 0.7730 -- iter: 891/891\n",
      "--\n",
      "Training Step: 161  | total loss: \u001b[1m\u001b[32m0.52405\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 161 | loss: 0.52405 - acc: 0.7765 -- iter: 891/891\n",
      "--\n",
      "Training Step: 162  | total loss: \u001b[1m\u001b[32m0.57468\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 162 | loss: 0.57468 - acc: 0.7531 -- iter: 891/891\n",
      "--\n",
      "Training Step: 163  | total loss: \u001b[1m\u001b[32m0.56358\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 163 | loss: 0.56358 - acc: 0.7587 -- iter: 891/891\n",
      "--\n",
      "Training Step: 164  | total loss: \u001b[1m\u001b[32m0.55354\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 164 | loss: 0.55354 - acc: 0.7646 -- iter: 891/891\n",
      "--\n",
      "Training Step: 165  | total loss: \u001b[1m\u001b[32m0.54504\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 165 | loss: 0.54504 - acc: 0.7695 -- iter: 891/891\n",
      "--\n",
      "Training Step: 166  | total loss: \u001b[1m\u001b[32m0.57454\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 166 | loss: 0.57454 - acc: 0.7464 -- iter: 891/891\n",
      "--\n",
      "Training Step: 167  | total loss: \u001b[1m\u001b[32m0.56604\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 167 | loss: 0.56604 - acc: 0.7525 -- iter: 891/891\n",
      "--\n",
      "Training Step: 168  | total loss: \u001b[1m\u001b[32m0.55877\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 168 | loss: 0.55877 - acc: 0.7581 -- iter: 891/891\n",
      "--\n",
      "Training Step: 169  | total loss: \u001b[1m\u001b[32m0.55121\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 169 | loss: 0.55121 - acc: 0.7630 -- iter: 891/891\n",
      "--\n",
      "Training Step: 170  | total loss: \u001b[1m\u001b[32m0.54310\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 170 | loss: 0.54310 - acc: 0.7676 -- iter: 891/891\n",
      "--\n",
      "Training Step: 171  | total loss: \u001b[1m\u001b[32m0.53518\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 171 | loss: 0.53518 - acc: 0.7722 -- iter: 891/891\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 172  | total loss: \u001b[1m\u001b[32m0.57376\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 172 | loss: 0.57376 - acc: 0.7498 -- iter: 891/891\n",
      "--\n",
      "Training Step: 173  | total loss: \u001b[1m\u001b[32m0.56340\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 173 | loss: 0.56340 - acc: 0.7554 -- iter: 891/891\n",
      "--\n",
      "Training Step: 174  | total loss: \u001b[1m\u001b[32m0.55393\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 174 | loss: 0.55393 - acc: 0.7606 -- iter: 891/891\n",
      "--\n",
      "Training Step: 175  | total loss: \u001b[1m\u001b[32m0.54549\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 175 | loss: 0.54549 - acc: 0.7650 -- iter: 891/891\n",
      "--\n",
      "Training Step: 176  | total loss: \u001b[1m\u001b[32m0.53763\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 176 | loss: 0.53763 - acc: 0.7687 -- iter: 891/891\n",
      "--\n",
      "Training Step: 177  | total loss: \u001b[1m\u001b[32m0.53018\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 177 | loss: 0.53018 - acc: 0.7731 -- iter: 891/891\n",
      "--\n",
      "Training Step: 178  | total loss: \u001b[1m\u001b[32m0.52333\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 178 | loss: 0.52333 - acc: 0.7772 -- iter: 891/891\n",
      "--\n",
      "Training Step: 179  | total loss: \u001b[1m\u001b[32m0.51673\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 179 | loss: 0.51673 - acc: 0.7803 -- iter: 891/891\n",
      "--\n",
      "Training Step: 180  | total loss: \u001b[1m\u001b[32m0.51063\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 180 | loss: 0.51063 - acc: 0.7820 -- iter: 891/891\n",
      "--\n",
      "Training Step: 181  | total loss: \u001b[1m\u001b[32m0.50494\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 181 | loss: 0.50494 - acc: 0.7838 -- iter: 891/891\n",
      "--\n",
      "Training Step: 182  | total loss: \u001b[1m\u001b[32m0.49968\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 182 | loss: 0.49968 - acc: 0.7860 -- iter: 891/891\n",
      "--\n",
      "Training Step: 183  | total loss: \u001b[1m\u001b[32m0.49487\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 183 | loss: 0.49487 - acc: 0.7879 -- iter: 891/891\n",
      "--\n",
      "Training Step: 184  | total loss: \u001b[1m\u001b[32m0.49057\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 184 | loss: 0.49057 - acc: 0.7894 -- iter: 891/891\n",
      "--\n",
      "Training Step: 185  | total loss: \u001b[1m\u001b[32m0.48677\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 185 | loss: 0.48677 - acc: 0.7907 -- iter: 891/891\n",
      "--\n",
      "Training Step: 186  | total loss: \u001b[1m\u001b[32m0.48350\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 186 | loss: 0.48350 - acc: 0.7921 -- iter: 891/891\n",
      "--\n",
      "Training Step: 187  | total loss: \u001b[1m\u001b[32m0.48048\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 187 | loss: 0.48048 - acc: 0.7933 -- iter: 891/891\n",
      "--\n",
      "Training Step: 188  | total loss: \u001b[1m\u001b[32m0.47772\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 188 | loss: 0.47772 - acc: 0.7941 -- iter: 891/891\n",
      "--\n",
      "Training Step: 189  | total loss: \u001b[1m\u001b[32m0.47501\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 189 | loss: 0.47501 - acc: 0.7951 -- iter: 891/891\n",
      "--\n",
      "Training Step: 190  | total loss: \u001b[1m\u001b[32m0.47252\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 190 | loss: 0.47252 - acc: 0.7959 -- iter: 891/891\n",
      "--\n",
      "Training Step: 191  | total loss: \u001b[1m\u001b[32m0.47013\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 191 | loss: 0.47013 - acc: 0.7965 -- iter: 891/891\n",
      "--\n",
      "Training Step: 192  | total loss: \u001b[1m\u001b[32m0.46797\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 192 | loss: 0.46797 - acc: 0.7969 -- iter: 891/891\n",
      "--\n",
      "Training Step: 193  | total loss: \u001b[1m\u001b[32m0.46594\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 193 | loss: 0.46594 - acc: 0.7976 -- iter: 891/891\n",
      "--\n",
      "Training Step: 194  | total loss: \u001b[1m\u001b[32m0.46413\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 194 | loss: 0.46413 - acc: 0.7986 -- iter: 891/891\n",
      "--\n",
      "Training Step: 195  | total loss: \u001b[1m\u001b[32m0.46249\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 195 | loss: 0.46249 - acc: 0.7992 -- iter: 891/891\n",
      "--\n",
      "Training Step: 196  | total loss: \u001b[1m\u001b[32m0.50637\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 196 | loss: 0.50637 - acc: 0.7744 -- iter: 891/891\n",
      "--\n",
      "Training Step: 197  | total loss: \u001b[1m\u001b[32m0.50077\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 197 | loss: 0.50077 - acc: 0.7774 -- iter: 891/891\n",
      "--\n",
      "Training Step: 198  | total loss: \u001b[1m\u001b[32m0.49620\u001b[0m\u001b[0m | time: 0.011s\n",
      "| Adam | epoch: 198 | loss: 0.49620 - acc: 0.7803 -- iter: 891/891\n",
      "--\n",
      "Training Step: 199  | total loss: \u001b[1m\u001b[32m0.49224\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 199 | loss: 0.49224 - acc: 0.7827 -- iter: 891/891\n",
      "--\n",
      "Training Step: 200  | total loss: \u001b[1m\u001b[32m0.53003\u001b[0m\u001b[0m | time: 0.011s\n",
      "| Adam | epoch: 200 | loss: 0.53003 - acc: 0.7575 -- iter: 891/891\n",
      "--\n",
      "Training Step: 201  | total loss: \u001b[1m\u001b[32m0.52336\u001b[0m\u001b[0m | time: 0.011s\n",
      "| Adam | epoch: 201 | loss: 0.52336 - acc: 0.7622 -- iter: 891/891\n",
      "--\n",
      "Training Step: 202  | total loss: \u001b[1m\u001b[32m0.51769\u001b[0m\u001b[0m | time: 0.011s\n",
      "| Adam | epoch: 202 | loss: 0.51769 - acc: 0.7665 -- iter: 891/891\n",
      "--\n",
      "Training Step: 203  | total loss: \u001b[1m\u001b[32m0.51234\u001b[0m\u001b[0m | time: 0.011s\n",
      "| Adam | epoch: 203 | loss: 0.51234 - acc: 0.7701 -- iter: 891/891\n",
      "--\n",
      "Training Step: 204  | total loss: \u001b[1m\u001b[32m0.50707\u001b[0m\u001b[0m | time: 0.011s\n",
      "| Adam | epoch: 204 | loss: 0.50707 - acc: 0.7737 -- iter: 891/891\n",
      "--\n",
      "Training Step: 205  | total loss: \u001b[1m\u001b[32m0.50199\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 205 | loss: 0.50199 - acc: 0.7769 -- iter: 891/891\n",
      "--\n",
      "Training Step: 206  | total loss: \u001b[1m\u001b[32m0.49729\u001b[0m\u001b[0m | time: 0.011s\n",
      "| Adam | epoch: 206 | loss: 0.49729 - acc: 0.7793 -- iter: 891/891\n",
      "--\n",
      "Training Step: 207  | total loss: \u001b[1m\u001b[32m0.49305\u001b[0m\u001b[0m | time: 0.011s\n",
      "| Adam | epoch: 207 | loss: 0.49305 - acc: 0.7817 -- iter: 891/891\n",
      "--\n",
      "Training Step: 208  | total loss: \u001b[1m\u001b[32m0.48915\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 208 | loss: 0.48915 - acc: 0.7838 -- iter: 891/891\n",
      "--\n",
      "Training Step: 209  | total loss: \u001b[1m\u001b[32m0.48537\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 209 | loss: 0.48537 - acc: 0.7855 -- iter: 891/891\n",
      "--\n",
      "Training Step: 210  | total loss: \u001b[1m\u001b[32m0.53264\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 210 | loss: 0.53264 - acc: 0.7625 -- iter: 891/891\n",
      "--\n",
      "Training Step: 211  | total loss: \u001b[1m\u001b[32m0.52422\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 211 | loss: 0.52422 - acc: 0.7670 -- iter: 891/891\n",
      "--\n",
      "Training Step: 212  | total loss: \u001b[1m\u001b[32m0.51729\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 212 | loss: 0.51729 - acc: 0.7710 -- iter: 891/891\n",
      "--\n",
      "Training Step: 213  | total loss: \u001b[1m\u001b[32m0.51163\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 213 | loss: 0.51163 - acc: 0.7739 -- iter: 891/891\n",
      "--\n",
      "Training Step: 214  | total loss: \u001b[1m\u001b[32m0.50562\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 214 | loss: 0.50562 - acc: 0.7769 -- iter: 891/891\n",
      "--\n",
      "Training Step: 215  | total loss: \u001b[1m\u001b[32m0.49974\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 215 | loss: 0.49974 - acc: 0.7799 -- iter: 891/891\n",
      "--\n",
      "Training Step: 216  | total loss: \u001b[1m\u001b[32m0.49442\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 216 | loss: 0.49442 - acc: 0.7827 -- iter: 891/891\n",
      "--\n",
      "Training Step: 217  | total loss: \u001b[1m\u001b[32m0.49006\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 217 | loss: 0.49006 - acc: 0.7844 -- iter: 891/891\n",
      "--\n",
      "Training Step: 218  | total loss: \u001b[1m\u001b[32m0.48635\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 218 | loss: 0.48635 - acc: 0.7857 -- iter: 891/891\n",
      "--\n",
      "Training Step: 219  | total loss: \u001b[1m\u001b[32m0.48284\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 219 | loss: 0.48284 - acc: 0.7876 -- iter: 891/891\n",
      "--\n",
      "Training Step: 220  | total loss: \u001b[1m\u001b[32m0.53011\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 220 | loss: 0.53011 - acc: 0.7636 -- iter: 891/891\n",
      "--\n",
      "Training Step: 221  | total loss: \u001b[1m\u001b[32m0.52185\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 221 | loss: 0.52185 - acc: 0.7677 -- iter: 891/891\n",
      "--\n",
      "Training Step: 222  | total loss: \u001b[1m\u001b[32m0.55731\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 222 | loss: 0.55731 - acc: 0.7428 -- iter: 891/891\n",
      "--\n",
      "Training Step: 223  | total loss: \u001b[1m\u001b[32m0.55027\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 223 | loss: 0.55027 - acc: 0.7486 -- iter: 891/891\n",
      "--\n",
      "Training Step: 224  | total loss: \u001b[1m\u001b[32m0.54590\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 224 | loss: 0.54590 - acc: 0.7533 -- iter: 891/891\n",
      "--\n",
      "Training Step: 225  | total loss: \u001b[1m\u001b[32m0.54187\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 225 | loss: 0.54187 - acc: 0.7577 -- iter: 891/891\n",
      "--\n",
      "Training Step: 226  | total loss: \u001b[1m\u001b[32m0.56504\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 226 | loss: 0.56504 - acc: 0.7361 -- iter: 891/891\n",
      "--\n",
      "Training Step: 227  | total loss: \u001b[1m\u001b[32m0.55779\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 227 | loss: 0.55779 - acc: 0.7430 -- iter: 891/891\n",
      "--\n",
      "Training Step: 228  | total loss: \u001b[1m\u001b[32m0.57511\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 228 | loss: 0.57511 - acc: 0.7254 -- iter: 891/891\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 229  | total loss: \u001b[1m\u001b[32m0.56705\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 229 | loss: 0.56705 - acc: 0.7335 -- iter: 891/891\n",
      "--\n",
      "Training Step: 230  | total loss: \u001b[1m\u001b[32m0.58677\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 230 | loss: 0.58677 - acc: 0.7133 -- iter: 891/891\n",
      "--\n",
      "Training Step: 231  | total loss: \u001b[1m\u001b[32m0.57870\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 231 | loss: 0.57870 - acc: 0.7217 -- iter: 891/891\n",
      "--\n",
      "Training Step: 232  | total loss: \u001b[1m\u001b[32m0.57188\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 232 | loss: 0.57188 - acc: 0.7287 -- iter: 891/891\n",
      "--\n",
      "Training Step: 233  | total loss: \u001b[1m\u001b[32m0.56558\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 233 | loss: 0.56558 - acc: 0.7350 -- iter: 891/891\n",
      "--\n",
      "Training Step: 234  | total loss: \u001b[1m\u001b[32m0.58205\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 234 | loss: 0.58205 - acc: 0.7166 -- iter: 891/891\n",
      "--\n",
      "Training Step: 235  | total loss: \u001b[1m\u001b[32m0.57433\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 235 | loss: 0.57433 - acc: 0.7248 -- iter: 891/891\n",
      "--\n",
      "Training Step: 236  | total loss: \u001b[1m\u001b[32m0.56710\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 236 | loss: 0.56710 - acc: 0.7323 -- iter: 891/891\n",
      "--\n",
      "Training Step: 237  | total loss: \u001b[1m\u001b[32m0.55997\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 237 | loss: 0.55997 - acc: 0.7374 -- iter: 891/891\n",
      "--\n",
      "Training Step: 238  | total loss: \u001b[1m\u001b[32m0.55276\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 238 | loss: 0.55276 - acc: 0.7422 -- iter: 891/891\n",
      "--\n",
      "Training Step: 239  | total loss: \u001b[1m\u001b[32m0.54547\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 239 | loss: 0.54547 - acc: 0.7468 -- iter: 891/891\n",
      "--\n",
      "Training Step: 240  | total loss: \u001b[1m\u001b[32m0.53819\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 240 | loss: 0.53819 - acc: 0.7525 -- iter: 891/891\n",
      "--\n",
      "Training Step: 241  | total loss: \u001b[1m\u001b[32m0.53111\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 241 | loss: 0.53111 - acc: 0.7579 -- iter: 891/891\n",
      "--\n",
      "Training Step: 242  | total loss: \u001b[1m\u001b[32m0.57332\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 242 | loss: 0.57332 - acc: 0.7351 -- iter: 891/891\n",
      "--\n",
      "Training Step: 243  | total loss: \u001b[1m\u001b[32m0.56184\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 243 | loss: 0.56184 - acc: 0.7424 -- iter: 891/891\n",
      "--\n",
      "Training Step: 244  | total loss: \u001b[1m\u001b[32m0.59945\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 244 | loss: 0.59945 - acc: 0.7210 -- iter: 891/891\n",
      "--\n",
      "Training Step: 245  | total loss: \u001b[1m\u001b[32m0.58498\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 245 | loss: 0.58498 - acc: 0.7299 -- iter: 891/891\n",
      "--\n",
      "Training Step: 246  | total loss: \u001b[1m\u001b[32m0.61124\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 246 | loss: 0.61124 - acc: 0.7093 -- iter: 891/891\n",
      "--\n",
      "Training Step: 247  | total loss: \u001b[1m\u001b[32m0.59992\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 247 | loss: 0.59992 - acc: 0.7187 -- iter: 891/891\n",
      "--\n",
      "Training Step: 248  | total loss: \u001b[1m\u001b[32m0.59179\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 248 | loss: 0.59179 - acc: 0.7265 -- iter: 891/891\n",
      "--\n",
      "Training Step: 249  | total loss: \u001b[1m\u001b[32m0.58478\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 249 | loss: 0.58478 - acc: 0.7336 -- iter: 891/891\n",
      "--\n",
      "Training Step: 250  | total loss: \u001b[1m\u001b[32m0.57693\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 250 | loss: 0.57693 - acc: 0.7404 -- iter: 891/891\n",
      "--\n",
      "Training Step: 251  | total loss: \u001b[1m\u001b[32m0.56763\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 251 | loss: 0.56763 - acc: 0.7476 -- iter: 891/891\n",
      "--\n",
      "Training Step: 252  | total loss: \u001b[1m\u001b[32m0.59079\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 252 | loss: 0.59079 - acc: 0.7271 -- iter: 891/891\n",
      "--\n",
      "Training Step: 253  | total loss: \u001b[1m\u001b[32m0.57891\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 253 | loss: 0.57891 - acc: 0.7352 -- iter: 891/891\n",
      "--\n",
      "Training Step: 254  | total loss: \u001b[1m\u001b[32m0.60703\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 254 | loss: 0.60703 - acc: 0.7141 -- iter: 891/891\n",
      "--\n",
      "Training Step: 255  | total loss: \u001b[1m\u001b[32m0.59410\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 255 | loss: 0.59410 - acc: 0.7230 -- iter: 891/891\n",
      "--\n",
      "Training Step: 256  | total loss: \u001b[1m\u001b[32m0.61346\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 256 | loss: 0.61346 - acc: 0.7048 -- iter: 891/891\n",
      "--\n",
      "Training Step: 257  | total loss: \u001b[1m\u001b[32m0.60125\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 257 | loss: 0.60125 - acc: 0.7151 -- iter: 891/891\n",
      "--\n",
      "Training Step: 258  | total loss: \u001b[1m\u001b[32m0.61479\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 258 | loss: 0.61479 - acc: 0.6996 -- iter: 891/891\n",
      "--\n",
      "Training Step: 259  | total loss: \u001b[1m\u001b[32m0.60462\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 259 | loss: 0.60462 - acc: 0.7108 -- iter: 891/891\n",
      "--\n",
      "Training Step: 260  | total loss: \u001b[1m\u001b[32m0.61452\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 260 | loss: 0.61452 - acc: 0.6962 -- iter: 891/891\n",
      "--\n",
      "Training Step: 261  | total loss: \u001b[1m\u001b[32m0.60678\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 261 | loss: 0.60678 - acc: 0.7067 -- iter: 891/891\n",
      "--\n",
      "Training Step: 262  | total loss: \u001b[1m\u001b[32m0.61532\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 262 | loss: 0.61532 - acc: 0.6920 -- iter: 891/891\n",
      "--\n",
      "Training Step: 263  | total loss: \u001b[1m\u001b[32m0.60919\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 263 | loss: 0.60919 - acc: 0.7008 -- iter: 891/891\n",
      "--\n",
      "Training Step: 264  | total loss: \u001b[1m\u001b[32m0.60381\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 264 | loss: 0.60381 - acc: 0.7092 -- iter: 891/891\n",
      "--\n",
      "Training Step: 265  | total loss: \u001b[1m\u001b[32m0.59831\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 265 | loss: 0.59831 - acc: 0.7179 -- iter: 891/891\n",
      "--\n",
      "Training Step: 266  | total loss: \u001b[1m\u001b[32m0.59206\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 266 | loss: 0.59206 - acc: 0.7273 -- iter: 891/891\n",
      "--\n",
      "Training Step: 267  | total loss: \u001b[1m\u001b[32m0.58469\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 267 | loss: 0.58469 - acc: 0.7363 -- iter: 891/891\n",
      "--\n",
      "Training Step: 268  | total loss: \u001b[1m\u001b[32m0.57617\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 268 | loss: 0.57617 - acc: 0.7443 -- iter: 891/891\n",
      "--\n",
      "Training Step: 269  | total loss: \u001b[1m\u001b[32m0.56687\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 269 | loss: 0.56687 - acc: 0.7508 -- iter: 891/891\n",
      "--\n",
      "Training Step: 270  | total loss: \u001b[1m\u001b[32m0.59825\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 270 | loss: 0.59825 - acc: 0.7306 -- iter: 891/891\n",
      "--\n",
      "Training Step: 271  | total loss: \u001b[1m\u001b[32m0.58570\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 271 | loss: 0.58570 - acc: 0.7364 -- iter: 891/891\n",
      "--\n",
      "Training Step: 272  | total loss: \u001b[1m\u001b[32m0.57429\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 272 | loss: 0.57429 - acc: 0.7421 -- iter: 891/891\n",
      "--\n",
      "Training Step: 273  | total loss: \u001b[1m\u001b[32m0.56375\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 273 | loss: 0.56375 - acc: 0.7486 -- iter: 891/891\n",
      "--\n",
      "Training Step: 274  | total loss: \u001b[1m\u001b[32m0.60422\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 274 | loss: 0.60422 - acc: 0.7273 -- iter: 891/891\n",
      "--\n",
      "Training Step: 275  | total loss: \u001b[1m\u001b[32m0.58992\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 275 | loss: 0.58992 - acc: 0.7355 -- iter: 891/891\n",
      "--\n",
      "Training Step: 276  | total loss: \u001b[1m\u001b[32m0.57641\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 276 | loss: 0.57641 - acc: 0.7426 -- iter: 891/891\n",
      "--\n",
      "Training Step: 277  | total loss: \u001b[1m\u001b[32m0.56410\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 277 | loss: 0.56410 - acc: 0.7495 -- iter: 891/891\n",
      "--\n",
      "Training Step: 278  | total loss: \u001b[1m\u001b[32m0.55357\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 278 | loss: 0.55357 - acc: 0.7561 -- iter: 891/891\n",
      "--\n",
      "Training Step: 279  | total loss: \u001b[1m\u001b[32m0.54402\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 279 | loss: 0.54402 - acc: 0.7618 -- iter: 891/891\n",
      "--\n",
      "Training Step: 280  | total loss: \u001b[1m\u001b[32m0.58009\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 280 | loss: 0.58009 - acc: 0.7380 -- iter: 891/891\n",
      "--\n",
      "Training Step: 281  | total loss: \u001b[1m\u001b[32m0.56868\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 281 | loss: 0.56868 - acc: 0.7441 -- iter: 891/891\n",
      "--\n",
      "Training Step: 282  | total loss: \u001b[1m\u001b[32m0.55803\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 282 | loss: 0.55803 - acc: 0.7506 -- iter: 891/891\n",
      "--\n",
      "Training Step: 283  | total loss: \u001b[1m\u001b[32m0.54795\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 283 | loss: 0.54795 - acc: 0.7567 -- iter: 891/891\n",
      "--\n",
      "Training Step: 284  | total loss: \u001b[1m\u001b[32m0.53816\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 284 | loss: 0.53816 - acc: 0.7624 -- iter: 891/891\n",
      "--\n",
      "Training Step: 285  | total loss: \u001b[1m\u001b[32m0.52915\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 285 | loss: 0.52915 - acc: 0.7671 -- iter: 891/891\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 286  | total loss: \u001b[1m\u001b[32m0.57442\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 286 | loss: 0.57442 - acc: 0.7429 -- iter: 891/891\n",
      "--\n",
      "Training Step: 287  | total loss: \u001b[1m\u001b[32m0.56219\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 287 | loss: 0.56219 - acc: 0.7484 -- iter: 891/891\n",
      "--\n",
      "Training Step: 288  | total loss: \u001b[1m\u001b[32m0.59883\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 288 | loss: 0.59883 - acc: 0.7283 -- iter: 891/891\n",
      "--\n",
      "Training Step: 289  | total loss: \u001b[1m\u001b[32m0.58422\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 289 | loss: 0.58422 - acc: 0.7370 -- iter: 891/891\n",
      "--\n",
      "Training Step: 290  | total loss: \u001b[1m\u001b[32m0.60819\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 290 | loss: 0.60819 - acc: 0.7175 -- iter: 891/891\n",
      "--\n",
      "Training Step: 291  | total loss: \u001b[1m\u001b[32m0.59466\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 291 | loss: 0.59466 - acc: 0.7267 -- iter: 891/891\n",
      "--\n",
      "Training Step: 292  | total loss: \u001b[1m\u001b[32m0.61099\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 292 | loss: 0.61099 - acc: 0.7098 -- iter: 891/891\n",
      "--\n",
      "Training Step: 293  | total loss: \u001b[1m\u001b[32m0.60113\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 293 | loss: 0.60113 - acc: 0.7186 -- iter: 891/891\n",
      "--\n",
      "Training Step: 294  | total loss: \u001b[1m\u001b[32m0.59354\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 294 | loss: 0.59354 - acc: 0.7263 -- iter: 891/891\n",
      "--\n",
      "Training Step: 295  | total loss: \u001b[1m\u001b[32m0.58667\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 295 | loss: 0.58667 - acc: 0.7334 -- iter: 891/891\n",
      "--\n",
      "Training Step: 296  | total loss: \u001b[1m\u001b[32m0.57947\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 296 | loss: 0.57947 - acc: 0.7405 -- iter: 891/891\n",
      "--\n",
      "Training Step: 297  | total loss: \u001b[1m\u001b[32m0.57144\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 297 | loss: 0.57144 - acc: 0.7475 -- iter: 891/891\n",
      "--\n",
      "Training Step: 298  | total loss: \u001b[1m\u001b[32m0.56270\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 298 | loss: 0.56270 - acc: 0.7538 -- iter: 891/891\n",
      "--\n",
      "Training Step: 299  | total loss: \u001b[1m\u001b[32m0.55380\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 299 | loss: 0.55380 - acc: 0.7589 -- iter: 891/891\n",
      "--\n",
      "Training Step: 300  | total loss: \u001b[1m\u001b[32m0.54540\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 300 | loss: 0.54540 - acc: 0.7633 -- iter: 891/891\n",
      "--\n",
      "Training Step: 301  | total loss: \u001b[1m\u001b[32m0.53794\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 301 | loss: 0.53794 - acc: 0.7673 -- iter: 891/891\n",
      "--\n",
      "Training Step: 302  | total loss: \u001b[1m\u001b[32m0.58941\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 302 | loss: 0.58941 - acc: 0.7416 -- iter: 891/891\n",
      "--\n",
      "Training Step: 303  | total loss: \u001b[1m\u001b[32m0.57809\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 303 | loss: 0.57809 - acc: 0.7470 -- iter: 891/891\n",
      "--\n",
      "Training Step: 304  | total loss: \u001b[1m\u001b[32m0.62021\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 304 | loss: 0.62021 - acc: 0.7258 -- iter: 891/891\n",
      "--\n",
      "Training Step: 305  | total loss: \u001b[1m\u001b[32m0.60458\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 305 | loss: 0.60458 - acc: 0.7337 -- iter: 891/891\n",
      "--\n",
      "Training Step: 306  | total loss: \u001b[1m\u001b[32m0.58994\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 306 | loss: 0.58994 - acc: 0.7416 -- iter: 891/891\n",
      "--\n",
      "Training Step: 307  | total loss: \u001b[1m\u001b[32m0.57676\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 307 | loss: 0.57676 - acc: 0.7492 -- iter: 891/891\n",
      "--\n",
      "Training Step: 308  | total loss: \u001b[1m\u001b[32m0.56511\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 308 | loss: 0.56511 - acc: 0.7555 -- iter: 891/891\n",
      "--\n",
      "Training Step: 309  | total loss: \u001b[1m\u001b[32m0.55475\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 309 | loss: 0.55475 - acc: 0.7610 -- iter: 891/891\n",
      "--\n",
      "Training Step: 310  | total loss: \u001b[1m\u001b[32m0.54551\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 310 | loss: 0.54551 - acc: 0.7658 -- iter: 891/891\n",
      "--\n",
      "Training Step: 311  | total loss: \u001b[1m\u001b[32m0.53705\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 311 | loss: 0.53705 - acc: 0.7699 -- iter: 891/891\n",
      "--\n",
      "Training Step: 312  | total loss: \u001b[1m\u001b[32m0.52878\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 312 | loss: 0.52878 - acc: 0.7735 -- iter: 891/891\n",
      "--\n",
      "Training Step: 313  | total loss: \u001b[1m\u001b[32m0.52074\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 313 | loss: 0.52074 - acc: 0.7774 -- iter: 891/891\n",
      "--\n",
      "Training Step: 314  | total loss: \u001b[1m\u001b[32m0.51335\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 314 | loss: 0.51335 - acc: 0.7810 -- iter: 891/891\n",
      "--\n",
      "Training Step: 315  | total loss: \u001b[1m\u001b[32m0.50668\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 315 | loss: 0.50668 - acc: 0.7842 -- iter: 891/891\n",
      "--\n",
      "Training Step: 316  | total loss: \u001b[1m\u001b[32m0.50096\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 316 | loss: 0.50096 - acc: 0.7866 -- iter: 891/891\n",
      "--\n",
      "Training Step: 317  | total loss: \u001b[1m\u001b[32m0.49607\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 317 | loss: 0.49607 - acc: 0.7883 -- iter: 891/891\n",
      "--\n",
      "Training Step: 318  | total loss: \u001b[1m\u001b[32m0.55346\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 318 | loss: 0.55346 - acc: 0.7647 -- iter: 891/891\n",
      "--\n",
      "Training Step: 319  | total loss: \u001b[1m\u001b[32m0.54284\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 319 | loss: 0.54284 - acc: 0.7694 -- iter: 891/891\n",
      "--\n",
      "Training Step: 320  | total loss: \u001b[1m\u001b[32m0.58660\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 320 | loss: 0.58660 - acc: 0.7463 -- iter: 891/891\n",
      "--\n",
      "Training Step: 321  | total loss: \u001b[1m\u001b[32m0.57366\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 321 | loss: 0.57366 - acc: 0.7524 -- iter: 891/891\n",
      "--\n",
      "Training Step: 322  | total loss: \u001b[1m\u001b[32m0.56315\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 322 | loss: 0.56315 - acc: 0.7575 -- iter: 891/891\n",
      "--\n",
      "Training Step: 323  | total loss: \u001b[1m\u001b[32m0.55403\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 323 | loss: 0.55403 - acc: 0.7616 -- iter: 891/891\n",
      "--\n",
      "Training Step: 324  | total loss: \u001b[1m\u001b[32m0.54500\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 324 | loss: 0.54500 - acc: 0.7656 -- iter: 891/891\n",
      "--\n",
      "Training Step: 325  | total loss: \u001b[1m\u001b[32m0.53588\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 325 | loss: 0.53588 - acc: 0.7695 -- iter: 891/891\n",
      "--\n",
      "Training Step: 326  | total loss: \u001b[1m\u001b[32m0.52741\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 326 | loss: 0.52741 - acc: 0.7734 -- iter: 891/891\n",
      "--\n",
      "Training Step: 327  | total loss: \u001b[1m\u001b[32m0.51999\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 327 | loss: 0.51999 - acc: 0.7775 -- iter: 891/891\n",
      "--\n",
      "Training Step: 328  | total loss: \u001b[1m\u001b[32m0.56014\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 328 | loss: 0.56014 - acc: 0.7552 -- iter: 891/891\n",
      "--\n",
      "Training Step: 329  | total loss: \u001b[1m\u001b[32m0.54984\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 329 | loss: 0.54984 - acc: 0.7606 -- iter: 891/891\n",
      "--\n",
      "Training Step: 330  | total loss: \u001b[1m\u001b[32m0.54065\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 330 | loss: 0.54065 - acc: 0.7649 -- iter: 891/891\n",
      "--\n",
      "Training Step: 331  | total loss: \u001b[1m\u001b[32m0.53235\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 331 | loss: 0.53235 - acc: 0.7689 -- iter: 891/891\n",
      "--\n",
      "Training Step: 332  | total loss: \u001b[1m\u001b[32m0.52472\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 332 | loss: 0.52472 - acc: 0.7726 -- iter: 891/891\n",
      "--\n",
      "Training Step: 333  | total loss: \u001b[1m\u001b[32m0.51769\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 333 | loss: 0.51769 - acc: 0.7761 -- iter: 891/891\n",
      "--\n",
      "Training Step: 334  | total loss: \u001b[1m\u001b[32m0.55266\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 334 | loss: 0.55266 - acc: 0.7543 -- iter: 891/891\n",
      "--\n",
      "Training Step: 335  | total loss: \u001b[1m\u001b[32m0.54309\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 335 | loss: 0.54309 - acc: 0.7597 -- iter: 891/891\n",
      "--\n",
      "Training Step: 336  | total loss: \u001b[1m\u001b[32m0.56840\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 336 | loss: 0.56840 - acc: 0.7378 -- iter: 891/891\n",
      "--\n",
      "Training Step: 337  | total loss: \u001b[1m\u001b[32m0.55987\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 337 | loss: 0.55987 - acc: 0.7446 -- iter: 891/891\n",
      "--\n",
      "Training Step: 338  | total loss: \u001b[1m\u001b[32m0.58123\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 338 | loss: 0.58123 - acc: 0.7240 -- iter: 891/891\n",
      "--\n",
      "Training Step: 339  | total loss: \u001b[1m\u001b[32m0.57485\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 339 | loss: 0.57485 - acc: 0.7311 -- iter: 891/891\n",
      "--\n",
      "Training Step: 340  | total loss: \u001b[1m\u001b[32m0.59059\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 340 | loss: 0.59059 - acc: 0.7131 -- iter: 891/891\n",
      "--\n",
      "Training Step: 341  | total loss: \u001b[1m\u001b[32m0.58471\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 341 | loss: 0.58471 - acc: 0.7212 -- iter: 891/891\n",
      "--\n",
      "Training Step: 342  | total loss: \u001b[1m\u001b[32m0.57913\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 342 | loss: 0.57913 - acc: 0.7285 -- iter: 891/891\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 343  | total loss: \u001b[1m\u001b[32m0.57308\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 343 | loss: 0.57308 - acc: 0.7362 -- iter: 891/891\n",
      "--\n",
      "Training Step: 344  | total loss: \u001b[1m\u001b[32m0.56626\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 344 | loss: 0.56626 - acc: 0.7436 -- iter: 891/891\n",
      "--\n",
      "Training Step: 345  | total loss: \u001b[1m\u001b[32m0.55877\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 345 | loss: 0.55877 - acc: 0.7497 -- iter: 891/891\n",
      "--\n",
      "Training Step: 346  | total loss: \u001b[1m\u001b[32m0.55103\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 346 | loss: 0.55103 - acc: 0.7549 -- iter: 891/891\n",
      "--\n",
      "Training Step: 347  | total loss: \u001b[1m\u001b[32m0.54358\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 347 | loss: 0.54358 - acc: 0.7577 -- iter: 891/891\n",
      "--\n",
      "Training Step: 348  | total loss: \u001b[1m\u001b[32m0.53688\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 348 | loss: 0.53688 - acc: 0.7601 -- iter: 891/891\n",
      "--\n",
      "Training Step: 349  | total loss: \u001b[1m\u001b[32m0.53115\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 349 | loss: 0.53115 - acc: 0.7620 -- iter: 891/891\n",
      "--\n",
      "Training Step: 350  | total loss: \u001b[1m\u001b[32m0.52640\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 350 | loss: 0.52640 - acc: 0.7653 -- iter: 891/891\n",
      "--\n",
      "Training Step: 351  | total loss: \u001b[1m\u001b[32m0.52237\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 351 | loss: 0.52237 - acc: 0.7683 -- iter: 891/891\n",
      "--\n",
      "Training Step: 352  | total loss: \u001b[1m\u001b[32m0.51850\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 352 | loss: 0.51850 - acc: 0.7712 -- iter: 891/891\n",
      "--\n",
      "Training Step: 353  | total loss: \u001b[1m\u001b[32m0.51419\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 353 | loss: 0.51419 - acc: 0.7742 -- iter: 891/891\n",
      "--\n",
      "Training Step: 354  | total loss: \u001b[1m\u001b[32m0.56086\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 354 | loss: 0.56086 - acc: 0.7532 -- iter: 891/891\n",
      "--\n",
      "Training Step: 355  | total loss: \u001b[1m\u001b[32m0.55011\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 355 | loss: 0.55011 - acc: 0.7597 -- iter: 891/891\n",
      "--\n",
      "Training Step: 356  | total loss: \u001b[1m\u001b[32m0.54021\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 356 | loss: 0.54021 - acc: 0.7650 -- iter: 891/891\n",
      "--\n",
      "Training Step: 357  | total loss: \u001b[1m\u001b[32m0.53196\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 357 | loss: 0.53196 - acc: 0.7696 -- iter: 891/891\n",
      "--\n",
      "Training Step: 358  | total loss: \u001b[1m\u001b[32m0.52556\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 358 | loss: 0.52556 - acc: 0.7737 -- iter: 891/891\n",
      "--\n",
      "Training Step: 359  | total loss: \u001b[1m\u001b[32m0.52031\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 359 | loss: 0.52031 - acc: 0.7768 -- iter: 891/891\n",
      "--\n",
      "Training Step: 360  | total loss: \u001b[1m\u001b[32m0.51516\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 360 | loss: 0.51516 - acc: 0.7797 -- iter: 891/891\n",
      "--\n",
      "Training Step: 361  | total loss: \u001b[1m\u001b[32m0.50941\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 361 | loss: 0.50941 - acc: 0.7826 -- iter: 891/891\n",
      "--\n",
      "Training Step: 362  | total loss: \u001b[1m\u001b[32m0.50330\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 362 | loss: 0.50330 - acc: 0.7855 -- iter: 891/891\n",
      "--\n",
      "Training Step: 363  | total loss: \u001b[1m\u001b[32m0.49761\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 363 | loss: 0.49761 - acc: 0.7887 -- iter: 891/891\n",
      "--\n",
      "Training Step: 364  | total loss: \u001b[1m\u001b[32m0.49277\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 364 | loss: 0.49277 - acc: 0.7910 -- iter: 891/891\n",
      "--\n",
      "Training Step: 365  | total loss: \u001b[1m\u001b[32m0.48882\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 365 | loss: 0.48882 - acc: 0.7926 -- iter: 891/891\n",
      "--\n",
      "Training Step: 366  | total loss: \u001b[1m\u001b[32m0.48560\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 366 | loss: 0.48560 - acc: 0.7938 -- iter: 891/891\n",
      "--\n",
      "Training Step: 367  | total loss: \u001b[1m\u001b[32m0.48263\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 367 | loss: 0.48263 - acc: 0.7944 -- iter: 891/891\n",
      "--\n",
      "Training Step: 368  | total loss: \u001b[1m\u001b[32m0.54221\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 368 | loss: 0.54221 - acc: 0.7678 -- iter: 891/891\n",
      "--\n",
      "Training Step: 369  | total loss: \u001b[1m\u001b[32m0.53253\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 369 | loss: 0.53253 - acc: 0.7723 -- iter: 891/891\n",
      "--\n",
      "Training Step: 370  | total loss: \u001b[1m\u001b[32m0.57191\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 370 | loss: 0.57191 - acc: 0.7502 -- iter: 891/891\n",
      "--\n",
      "Training Step: 371  | total loss: \u001b[1m\u001b[32m0.56064\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 371 | loss: 0.56064 - acc: 0.7559 -- iter: 891/891\n",
      "--\n",
      "Training Step: 372  | total loss: \u001b[1m\u001b[32m0.55202\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 372 | loss: 0.55202 - acc: 0.7604 -- iter: 891/891\n",
      "--\n",
      "Training Step: 373  | total loss: \u001b[1m\u001b[32m0.54510\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 373 | loss: 0.54510 - acc: 0.7641 -- iter: 891/891\n",
      "--\n",
      "Training Step: 374  | total loss: \u001b[1m\u001b[32m0.57180\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 374 | loss: 0.57180 - acc: 0.7411 -- iter: 891/891\n",
      "--\n",
      "Training Step: 375  | total loss: \u001b[1m\u001b[32m0.56319\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 375 | loss: 0.56319 - acc: 0.7473 -- iter: 891/891\n",
      "--\n",
      "Training Step: 376  | total loss: \u001b[1m\u001b[32m0.55531\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 376 | loss: 0.55531 - acc: 0.7532 -- iter: 891/891\n",
      "--\n",
      "Training Step: 377  | total loss: \u001b[1m\u001b[32m0.54766\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 377 | loss: 0.54766 - acc: 0.7590 -- iter: 891/891\n",
      "--\n",
      "Training Step: 378  | total loss: \u001b[1m\u001b[32m0.54008\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 378 | loss: 0.54008 - acc: 0.7638 -- iter: 891/891\n",
      "--\n",
      "Training Step: 379  | total loss: \u001b[1m\u001b[32m0.53272\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 379 | loss: 0.53272 - acc: 0.7688 -- iter: 891/891\n",
      "--\n",
      "Training Step: 380  | total loss: \u001b[1m\u001b[32m0.56497\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 380 | loss: 0.56497 - acc: 0.7466 -- iter: 891/891\n",
      "--\n",
      "Training Step: 381  | total loss: \u001b[1m\u001b[32m0.55511\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 381 | loss: 0.55511 - acc: 0.7527 -- iter: 891/891\n",
      "--\n",
      "Training Step: 382  | total loss: \u001b[1m\u001b[32m0.54637\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 382 | loss: 0.54637 - acc: 0.7580 -- iter: 891/891\n",
      "--\n",
      "Training Step: 383  | total loss: \u001b[1m\u001b[32m0.53847\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 383 | loss: 0.53847 - acc: 0.7629 -- iter: 891/891\n",
      "--\n",
      "Training Step: 384  | total loss: \u001b[1m\u001b[32m0.53121\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 384 | loss: 0.53121 - acc: 0.7670 -- iter: 891/891\n",
      "--\n",
      "Training Step: 385  | total loss: \u001b[1m\u001b[32m0.52446\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 385 | loss: 0.52446 - acc: 0.7708 -- iter: 891/891\n",
      "--\n",
      "Training Step: 386  | total loss: \u001b[1m\u001b[32m0.51812\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 386 | loss: 0.51812 - acc: 0.7744 -- iter: 891/891\n",
      "--\n",
      "Training Step: 387  | total loss: \u001b[1m\u001b[32m0.51206\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 387 | loss: 0.51206 - acc: 0.7779 -- iter: 891/891\n",
      "--\n",
      "Training Step: 388  | total loss: \u001b[1m\u001b[32m0.50619\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 388 | loss: 0.50619 - acc: 0.7807 -- iter: 891/891\n",
      "--\n",
      "Training Step: 389  | total loss: \u001b[1m\u001b[32m0.50056\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 389 | loss: 0.50056 - acc: 0.7833 -- iter: 891/891\n",
      "--\n",
      "Training Step: 390  | total loss: \u001b[1m\u001b[32m0.49518\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 390 | loss: 0.49518 - acc: 0.7860 -- iter: 891/891\n",
      "--\n",
      "Training Step: 391  | total loss: \u001b[1m\u001b[32m0.49011\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 391 | loss: 0.49011 - acc: 0.7887 -- iter: 891/891\n",
      "--\n",
      "Training Step: 392  | total loss: \u001b[1m\u001b[32m0.53449\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 392 | loss: 0.53449 - acc: 0.7638 -- iter: 891/891\n",
      "--\n",
      "Training Step: 393  | total loss: \u001b[1m\u001b[32m0.52622\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 393 | loss: 0.52622 - acc: 0.7684 -- iter: 891/891\n",
      "--\n",
      "Training Step: 394  | total loss: \u001b[1m\u001b[32m0.55922\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 394 | loss: 0.55922 - acc: 0.7465 -- iter: 891/891\n",
      "--\n",
      "Training Step: 395  | total loss: \u001b[1m\u001b[32m0.55118\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 395 | loss: 0.55118 - acc: 0.7522 -- iter: 891/891\n",
      "--\n",
      "Training Step: 396  | total loss: \u001b[1m\u001b[32m0.54512\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 396 | loss: 0.54512 - acc: 0.7570 -- iter: 891/891\n",
      "--\n",
      "Training Step: 397  | total loss: \u001b[1m\u001b[32m0.53933\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 397 | loss: 0.53933 - acc: 0.7615 -- iter: 891/891\n",
      "--\n",
      "Training Step: 398  | total loss: \u001b[1m\u001b[32m0.56430\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 398 | loss: 0.56430 - acc: 0.7419 -- iter: 891/891\n",
      "--\n",
      "Training Step: 399  | total loss: \u001b[1m\u001b[32m0.55531\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 399 | loss: 0.55531 - acc: 0.7489 -- iter: 891/891\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 400  | total loss: \u001b[1m\u001b[32m0.57771\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 400 | loss: 0.57771 - acc: 0.7288 -- iter: 891/891\n",
      "--\n",
      "Training Step: 401  | total loss: \u001b[1m\u001b[32m0.56809\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 401 | loss: 0.56809 - acc: 0.7373 -- iter: 891/891\n",
      "--\n",
      "Training Step: 402  | total loss: \u001b[1m\u001b[32m0.58553\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 402 | loss: 0.58553 - acc: 0.7202 -- iter: 891/891\n",
      "--\n",
      "Training Step: 403  | total loss: \u001b[1m\u001b[32m0.57673\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 403 | loss: 0.57673 - acc: 0.7289 -- iter: 891/891\n",
      "--\n",
      "Training Step: 404  | total loss: \u001b[1m\u001b[32m0.56950\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 404 | loss: 0.56950 - acc: 0.7364 -- iter: 891/891\n",
      "--\n",
      "Training Step: 405  | total loss: \u001b[1m\u001b[32m0.56318\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 405 | loss: 0.56318 - acc: 0.7432 -- iter: 891/891\n",
      "--\n",
      "Training Step: 406  | total loss: \u001b[1m\u001b[32m0.58048\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 406 | loss: 0.58048 - acc: 0.7252 -- iter: 891/891\n",
      "--\n",
      "Training Step: 407  | total loss: \u001b[1m\u001b[32m0.57318\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 407 | loss: 0.57318 - acc: 0.7326 -- iter: 891/891\n",
      "--\n",
      "Training Step: 408  | total loss: \u001b[1m\u001b[32m0.56654\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 408 | loss: 0.56654 - acc: 0.7394 -- iter: 891/891\n",
      "--\n",
      "Training Step: 409  | total loss: \u001b[1m\u001b[32m0.56016\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 409 | loss: 0.56016 - acc: 0.7452 -- iter: 891/891\n",
      "--\n",
      "Training Step: 410  | total loss: \u001b[1m\u001b[32m0.58026\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 410 | loss: 0.58026 - acc: 0.7257 -- iter: 891/891\n",
      "--\n",
      "Training Step: 411  | total loss: \u001b[1m\u001b[32m0.57194\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 411 | loss: 0.57194 - acc: 0.7311 -- iter: 891/891\n",
      "--\n",
      "Training Step: 412  | total loss: \u001b[1m\u001b[32m0.56421\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 412 | loss: 0.56421 - acc: 0.7364 -- iter: 891/891\n",
      "--\n",
      "Training Step: 413  | total loss: \u001b[1m\u001b[32m0.55678\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 413 | loss: 0.55678 - acc: 0.7425 -- iter: 891/891\n",
      "--\n",
      "Training Step: 414  | total loss: \u001b[1m\u001b[32m0.54946\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 414 | loss: 0.54946 - acc: 0.7485 -- iter: 891/891\n",
      "--\n",
      "Training Step: 415  | total loss: \u001b[1m\u001b[32m0.54215\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 415 | loss: 0.54215 - acc: 0.7540 -- iter: 891/891\n",
      "--\n",
      "Training Step: 416  | total loss: \u001b[1m\u001b[32m0.53488\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 416 | loss: 0.53488 - acc: 0.7594 -- iter: 891/891\n",
      "--\n",
      "Training Step: 417  | total loss: \u001b[1m\u001b[32m0.52773\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 417 | loss: 0.52773 - acc: 0.7649 -- iter: 891/891\n",
      "--\n",
      "Training Step: 418  | total loss: \u001b[1m\u001b[32m0.56460\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 418 | loss: 0.56460 - acc: 0.7455 -- iter: 891/891\n",
      "--\n",
      "Training Step: 419  | total loss: \u001b[1m\u001b[32m0.55363\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 419 | loss: 0.55363 - acc: 0.7519 -- iter: 891/891\n",
      "--\n",
      "Training Step: 420  | total loss: \u001b[1m\u001b[32m0.58942\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 420 | loss: 0.58942 - acc: 0.7291 -- iter: 891/891\n",
      "--\n",
      "Training Step: 421  | total loss: \u001b[1m\u001b[32m0.57553\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 421 | loss: 0.57553 - acc: 0.7376 -- iter: 891/891\n",
      "--\n",
      "Training Step: 422  | total loss: \u001b[1m\u001b[32m0.56337\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 422 | loss: 0.56337 - acc: 0.7451 -- iter: 891/891\n",
      "--\n",
      "Training Step: 423  | total loss: \u001b[1m\u001b[32m0.55283\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 423 | loss: 0.55283 - acc: 0.7518 -- iter: 891/891\n",
      "--\n",
      "Training Step: 424  | total loss: \u001b[1m\u001b[32m0.58423\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 424 | loss: 0.58423 - acc: 0.7309 -- iter: 891/891\n",
      "--\n",
      "Training Step: 425  | total loss: \u001b[1m\u001b[32m0.57290\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 425 | loss: 0.57290 - acc: 0.7385 -- iter: 891/891\n",
      "--\n",
      "Training Step: 426  | total loss: \u001b[1m\u001b[32m0.59356\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 426 | loss: 0.59356 - acc: 0.7217 -- iter: 891/891\n",
      "--\n",
      "Training Step: 427  | total loss: \u001b[1m\u001b[32m0.58278\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 427 | loss: 0.58278 - acc: 0.7300 -- iter: 891/891\n",
      "--\n",
      "Training Step: 428  | total loss: \u001b[1m\u001b[32m0.60206\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 428 | loss: 0.60206 - acc: 0.7102 -- iter: 891/891\n",
      "--\n",
      "Training Step: 429  | total loss: \u001b[1m\u001b[32m0.59156\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 429 | loss: 0.59156 - acc: 0.7198 -- iter: 891/891\n",
      "--\n",
      "Training Step: 430  | total loss: \u001b[1m\u001b[32m0.60736\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 430 | loss: 0.60736 - acc: 0.7025 -- iter: 891/891\n",
      "--\n",
      "Training Step: 431  | total loss: \u001b[1m\u001b[32m0.59717\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 431 | loss: 0.59717 - acc: 0.7128 -- iter: 891/891\n",
      "--\n",
      "Training Step: 432  | total loss: \u001b[1m\u001b[32m0.58806\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 432 | loss: 0.58806 - acc: 0.7225 -- iter: 891/891\n",
      "--\n",
      "Training Step: 433  | total loss: \u001b[1m\u001b[32m0.57941\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 433 | loss: 0.57941 - acc: 0.7313 -- iter: 891/891\n",
      "--\n",
      "Training Step: 434  | total loss: \u001b[1m\u001b[32m0.57089\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 434 | loss: 0.57089 - acc: 0.7392 -- iter: 891/891\n",
      "--\n",
      "Training Step: 435  | total loss: \u001b[1m\u001b[32m0.56243\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 435 | loss: 0.56243 - acc: 0.7458 -- iter: 891/891\n",
      "--\n",
      "Training Step: 436  | total loss: \u001b[1m\u001b[32m0.55417\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 436 | loss: 0.55417 - acc: 0.7504 -- iter: 891/891\n",
      "--\n",
      "Training Step: 437  | total loss: \u001b[1m\u001b[32m0.54635\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 437 | loss: 0.54635 - acc: 0.7534 -- iter: 891/891\n",
      "--\n",
      "Training Step: 438  | total loss: \u001b[1m\u001b[32m0.58184\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 438 | loss: 0.58184 - acc: 0.7321 -- iter: 891/891\n",
      "--\n",
      "Training Step: 439  | total loss: \u001b[1m\u001b[32m0.57109\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 439 | loss: 0.57109 - acc: 0.7376 -- iter: 891/891\n",
      "--\n",
      "Training Step: 440  | total loss: \u001b[1m\u001b[32m0.60689\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 440 | loss: 0.60689 - acc: 0.7163 -- iter: 891/891\n",
      "--\n",
      "Training Step: 441  | total loss: \u001b[1m\u001b[32m0.59363\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 441 | loss: 0.59363 - acc: 0.7244 -- iter: 891/891\n",
      "--\n",
      "Training Step: 442  | total loss: \u001b[1m\u001b[32m0.61936\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 442 | loss: 0.61936 - acc: 0.7059 -- iter: 891/891\n",
      "--\n",
      "Training Step: 443  | total loss: \u001b[1m\u001b[32m0.60517\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 443 | loss: 0.60517 - acc: 0.7163 -- iter: 891/891\n",
      "--\n",
      "Training Step: 444  | total loss: \u001b[1m\u001b[32m0.62118\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 444 | loss: 0.62118 - acc: 0.7017 -- iter: 891/891\n",
      "--\n",
      "Training Step: 445  | total loss: \u001b[1m\u001b[32m0.60849\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 445 | loss: 0.60849 - acc: 0.7120 -- iter: 891/891\n",
      "--\n",
      "Training Step: 446  | total loss: \u001b[1m\u001b[32m0.62045\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 446 | loss: 0.62045 - acc: 0.6952 -- iter: 891/891\n",
      "--\n",
      "Training Step: 447  | total loss: \u001b[1m\u001b[32m0.61090\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 447 | loss: 0.61090 - acc: 0.7052 -- iter: 891/891\n",
      "--\n",
      "Training Step: 448  | total loss: \u001b[1m\u001b[32m0.62009\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 448 | loss: 0.62009 - acc: 0.6894 -- iter: 891/891\n",
      "--\n",
      "Training Step: 449  | total loss: \u001b[1m\u001b[32m0.61296\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 449 | loss: 0.61296 - acc: 0.6998 -- iter: 891/891\n",
      "--\n",
      "Training Step: 450  | total loss: \u001b[1m\u001b[32m0.60703\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 450 | loss: 0.60703 - acc: 0.7089 -- iter: 891/891\n",
      "--\n",
      "Training Step: 451  | total loss: \u001b[1m\u001b[32m0.60146\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 451 | loss: 0.60146 - acc: 0.7172 -- iter: 891/891\n",
      "--\n",
      "Training Step: 452  | total loss: \u001b[1m\u001b[32m0.61212\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 452 | loss: 0.61212 - acc: 0.6987 -- iter: 891/891\n",
      "--\n",
      "Training Step: 453  | total loss: \u001b[1m\u001b[32m0.60470\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 453 | loss: 0.60470 - acc: 0.7093 -- iter: 891/891\n",
      "--\n",
      "Training Step: 454  | total loss: \u001b[1m\u001b[32m0.59699\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 454 | loss: 0.59699 - acc: 0.7195 -- iter: 891/891\n",
      "--\n",
      "Training Step: 455  | total loss: \u001b[1m\u001b[32m0.58860\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 455 | loss: 0.58860 - acc: 0.7288 -- iter: 891/891\n",
      "--\n",
      "Training Step: 456  | total loss: \u001b[1m\u001b[32m0.57942\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 456 | loss: 0.57942 - acc: 0.7365 -- iter: 891/891\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 457  | total loss: \u001b[1m\u001b[32m0.56970\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 457 | loss: 0.56970 - acc: 0.7437 -- iter: 891/891\n",
      "--\n",
      "Training Step: 458  | total loss: \u001b[1m\u001b[32m0.55991\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 458 | loss: 0.55991 - acc: 0.7483 -- iter: 891/891\n",
      "--\n",
      "Training Step: 459  | total loss: \u001b[1m\u001b[32m0.55062\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 459 | loss: 0.55062 - acc: 0.7529 -- iter: 891/891\n",
      "--\n",
      "Training Step: 460  | total loss: \u001b[1m\u001b[32m0.54231\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 460 | loss: 0.54231 - acc: 0.7582 -- iter: 891/891\n",
      "--\n",
      "Training Step: 461  | total loss: \u001b[1m\u001b[32m0.53526\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 461 | loss: 0.53526 - acc: 0.7629 -- iter: 891/891\n",
      "--\n",
      "Training Step: 462  | total loss: \u001b[1m\u001b[32m0.52936\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 462 | loss: 0.52936 - acc: 0.7662 -- iter: 891/891\n",
      "--\n",
      "Training Step: 463  | total loss: \u001b[1m\u001b[32m0.52408\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 463 | loss: 0.52408 - acc: 0.7695 -- iter: 891/891\n",
      "--\n",
      "Training Step: 464  | total loss: \u001b[1m\u001b[32m0.57628\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 464 | loss: 0.57628 - acc: 0.7475 -- iter: 891/891\n",
      "--\n",
      "Training Step: 465  | total loss: \u001b[1m\u001b[32m0.56453\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 465 | loss: 0.56453 - acc: 0.7537 -- iter: 891/891\n",
      "--\n",
      "Training Step: 466  | total loss: \u001b[1m\u001b[32m0.61139\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 466 | loss: 0.61139 - acc: 0.7312 -- iter: 891/891\n",
      "--\n",
      "Training Step: 467  | total loss: \u001b[1m\u001b[32m0.59521\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 467 | loss: 0.59521 - acc: 0.7392 -- iter: 891/891\n",
      "--\n",
      "Training Step: 468  | total loss: \u001b[1m\u001b[32m0.61821\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 468 | loss: 0.61821 - acc: 0.7213 -- iter: 891/891\n",
      "--\n",
      "Training Step: 469  | total loss: \u001b[1m\u001b[32m0.60585\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 469 | loss: 0.60585 - acc: 0.7294 -- iter: 891/891\n",
      "--\n",
      "Training Step: 470  | total loss: \u001b[1m\u001b[32m0.59744\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 470 | loss: 0.59744 - acc: 0.7354 -- iter: 891/891\n",
      "--\n",
      "Training Step: 471  | total loss: \u001b[1m\u001b[32m0.59117\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 471 | loss: 0.59117 - acc: 0.7401 -- iter: 891/891\n",
      "--\n",
      "Training Step: 472  | total loss: \u001b[1m\u001b[32m0.58529\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 472 | loss: 0.58529 - acc: 0.7453 -- iter: 891/891\n",
      "--\n",
      "Training Step: 473  | total loss: \u001b[1m\u001b[32m0.57857\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 473 | loss: 0.57857 - acc: 0.7508 -- iter: 891/891\n",
      "--\n",
      "Training Step: 474  | total loss: \u001b[1m\u001b[32m0.59622\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 474 | loss: 0.59622 - acc: 0.7278 -- iter: 891/891\n",
      "--\n",
      "Training Step: 475  | total loss: \u001b[1m\u001b[32m0.58599\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 475 | loss: 0.58599 - acc: 0.7365 -- iter: 891/891\n",
      "--\n",
      "Training Step: 476  | total loss: \u001b[1m\u001b[32m0.57614\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 476 | loss: 0.57614 - acc: 0.7442 -- iter: 891/891\n",
      "--\n",
      "Training Step: 477  | total loss: \u001b[1m\u001b[32m0.56658\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 477 | loss: 0.56658 - acc: 0.7500 -- iter: 891/891\n",
      "--\n",
      "Training Step: 478  | total loss: \u001b[1m\u001b[32m0.55740\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 478 | loss: 0.55740 - acc: 0.7557 -- iter: 891/891\n",
      "--\n",
      "Training Step: 479  | total loss: \u001b[1m\u001b[32m0.54879\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 479 | loss: 0.54879 - acc: 0.7605 -- iter: 891/891\n",
      "--\n",
      "Training Step: 480  | total loss: \u001b[1m\u001b[32m0.54098\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 480 | loss: 0.54098 - acc: 0.7650 -- iter: 891/891\n",
      "--\n",
      "Training Step: 481  | total loss: \u001b[1m\u001b[32m0.53413\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 481 | loss: 0.53413 - acc: 0.7688 -- iter: 891/891\n",
      "--\n",
      "Training Step: 482  | total loss: \u001b[1m\u001b[32m0.52820\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 482 | loss: 0.52820 - acc: 0.7716 -- iter: 891/891\n",
      "--\n",
      "Training Step: 483  | total loss: \u001b[1m\u001b[32m0.52295\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 483 | loss: 0.52295 - acc: 0.7741 -- iter: 891/891\n",
      "--\n",
      "Training Step: 484  | total loss: \u001b[1m\u001b[32m0.51807\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 484 | loss: 0.51807 - acc: 0.7770 -- iter: 891/891\n",
      "--\n",
      "Training Step: 485  | total loss: \u001b[1m\u001b[32m0.51329\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 485 | loss: 0.51329 - acc: 0.7802 -- iter: 891/891\n",
      "--\n",
      "Training Step: 486  | total loss: \u001b[1m\u001b[32m0.50833\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 486 | loss: 0.50833 - acc: 0.7829 -- iter: 891/891\n",
      "--\n",
      "Training Step: 487  | total loss: \u001b[1m\u001b[32m0.50308\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 487 | loss: 0.50308 - acc: 0.7855 -- iter: 891/891\n",
      "--\n",
      "Training Step: 488  | total loss: \u001b[1m\u001b[32m0.54953\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 488 | loss: 0.54953 - acc: 0.7598 -- iter: 891/891\n",
      "--\n",
      "Training Step: 489  | total loss: \u001b[1m\u001b[32m0.53954\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 489 | loss: 0.53954 - acc: 0.7650 -- iter: 891/891\n",
      "--\n",
      "Training Step: 490  | total loss: \u001b[1m\u001b[32m0.53167\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 490 | loss: 0.53167 - acc: 0.7695 -- iter: 891/891\n",
      "--\n",
      "Training Step: 491  | total loss: \u001b[1m\u001b[32m0.52571\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 491 | loss: 0.52571 - acc: 0.7731 -- iter: 891/891\n",
      "--\n",
      "Training Step: 492  | total loss: \u001b[1m\u001b[32m0.55501\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 492 | loss: 0.55501 - acc: 0.7491 -- iter: 891/891\n",
      "--\n",
      "Training Step: 493  | total loss: \u001b[1m\u001b[32m0.54793\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 493 | loss: 0.54793 - acc: 0.7549 -- iter: 891/891\n",
      "--\n",
      "Training Step: 494  | total loss: \u001b[1m\u001b[32m0.54142\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 494 | loss: 0.54142 - acc: 0.7600 -- iter: 891/891\n",
      "--\n",
      "Training Step: 495  | total loss: \u001b[1m\u001b[32m0.53457\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 495 | loss: 0.53457 - acc: 0.7648 -- iter: 891/891\n",
      "--\n",
      "Training Step: 496  | total loss: \u001b[1m\u001b[32m0.52736\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 496 | loss: 0.52736 - acc: 0.7696 -- iter: 891/891\n",
      "--\n",
      "Training Step: 497  | total loss: \u001b[1m\u001b[32m0.52026\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 497 | loss: 0.52026 - acc: 0.7741 -- iter: 891/891\n",
      "--\n",
      "Training Step: 498  | total loss: \u001b[1m\u001b[32m0.55579\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 498 | loss: 0.55579 - acc: 0.7501 -- iter: 891/891\n",
      "--\n",
      "Training Step: 499  | total loss: \u001b[1m\u001b[32m0.54595\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 499 | loss: 0.54595 - acc: 0.7566 -- iter: 891/891\n",
      "--\n",
      "Training Step: 500  | total loss: \u001b[1m\u001b[32m0.57806\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 500 | loss: 0.57806 - acc: 0.7359 -- iter: 891/891\n",
      "--\n",
      "Training Step: 501  | total loss: \u001b[1m\u001b[32m0.56642\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 501 | loss: 0.56642 - acc: 0.7434 -- iter: 891/891\n",
      "--\n",
      "Training Step: 502  | total loss: \u001b[1m\u001b[32m0.59139\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 502 | loss: 0.59139 - acc: 0.7239 -- iter: 891/891\n",
      "--\n",
      "Training Step: 503  | total loss: \u001b[1m\u001b[32m0.57983\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 503 | loss: 0.57983 - acc: 0.7322 -- iter: 891/891\n",
      "--\n",
      "Training Step: 504  | total loss: \u001b[1m\u001b[32m0.57032\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 504 | loss: 0.57032 - acc: 0.7399 -- iter: 891/891\n",
      "--\n",
      "Training Step: 505  | total loss: \u001b[1m\u001b[32m0.56227\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 505 | loss: 0.56227 - acc: 0.7470 -- iter: 891/891\n",
      "--\n",
      "Training Step: 506  | total loss: \u001b[1m\u001b[32m0.55518\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 506 | loss: 0.55518 - acc: 0.7539 -- iter: 891/891\n",
      "--\n",
      "Training Step: 507  | total loss: \u001b[1m\u001b[32m0.54859\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 507 | loss: 0.54859 - acc: 0.7600 -- iter: 891/891\n",
      "--\n",
      "Training Step: 508  | total loss: \u001b[1m\u001b[32m0.54207\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 508 | loss: 0.54207 - acc: 0.7654 -- iter: 891/891\n",
      "--\n",
      "Training Step: 509  | total loss: \u001b[1m\u001b[32m0.53531\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 509 | loss: 0.53531 - acc: 0.7704 -- iter: 891/891\n",
      "--\n",
      "Training Step: 510  | total loss: \u001b[1m\u001b[32m0.52827\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 510 | loss: 0.52827 - acc: 0.7750 -- iter: 891/891\n",
      "--\n",
      "Training Step: 511  | total loss: \u001b[1m\u001b[32m0.52123\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 511 | loss: 0.52123 - acc: 0.7783 -- iter: 891/891\n",
      "--\n",
      "Training Step: 512  | total loss: \u001b[1m\u001b[32m0.55909\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 512 | loss: 0.55909 - acc: 0.7543 -- iter: 891/891\n",
      "--\n",
      "Training Step: 513  | total loss: \u001b[1m\u001b[32m0.54847\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 513 | loss: 0.54847 - acc: 0.7592 -- iter: 891/891\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 514  | total loss: \u001b[1m\u001b[32m0.58462\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 514 | loss: 0.58462 - acc: 0.7378 -- iter: 891/891\n",
      "--\n",
      "Training Step: 515  | total loss: \u001b[1m\u001b[32m0.57101\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 515 | loss: 0.57101 - acc: 0.7450 -- iter: 891/891\n",
      "--\n",
      "Training Step: 516  | total loss: \u001b[1m\u001b[32m0.60532\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 516 | loss: 0.60532 - acc: 0.7236 -- iter: 891/891\n",
      "--\n",
      "Training Step: 517  | total loss: \u001b[1m\u001b[32m0.59022\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 517 | loss: 0.59022 - acc: 0.7325 -- iter: 891/891\n",
      "--\n",
      "Training Step: 518  | total loss: \u001b[1m\u001b[32m0.57730\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 518 | loss: 0.57730 - acc: 0.7402 -- iter: 891/891\n",
      "--\n",
      "Training Step: 519  | total loss: \u001b[1m\u001b[32m0.56598\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 519 | loss: 0.56598 - acc: 0.7467 -- iter: 891/891\n",
      "--\n",
      "Training Step: 520  | total loss: \u001b[1m\u001b[32m0.55579\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 520 | loss: 0.55579 - acc: 0.7524 -- iter: 891/891\n",
      "--\n",
      "Training Step: 521  | total loss: \u001b[1m\u001b[32m0.54626\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 521 | loss: 0.54626 - acc: 0.7574 -- iter: 891/891\n",
      "--\n",
      "Training Step: 522  | total loss: \u001b[1m\u001b[32m0.53696\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 522 | loss: 0.53696 - acc: 0.7622 -- iter: 891/891\n",
      "--\n",
      "Training Step: 523  | total loss: \u001b[1m\u001b[32m0.52801\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 523 | loss: 0.52801 - acc: 0.7670 -- iter: 891/891\n",
      "--\n",
      "Training Step: 524  | total loss: \u001b[1m\u001b[32m0.51985\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 524 | loss: 0.51985 - acc: 0.7716 -- iter: 891/891\n",
      "--\n",
      "Training Step: 525  | total loss: \u001b[1m\u001b[32m0.51261\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 525 | loss: 0.51261 - acc: 0.7759 -- iter: 891/891\n",
      "--\n",
      "Training Step: 526  | total loss: \u001b[1m\u001b[32m0.50618\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 526 | loss: 0.50618 - acc: 0.7791 -- iter: 891/891\n",
      "--\n",
      "Training Step: 527  | total loss: \u001b[1m\u001b[32m0.50058\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 527 | loss: 0.50058 - acc: 0.7814 -- iter: 891/891\n",
      "--\n",
      "Training Step: 528  | total loss: \u001b[1m\u001b[32m0.49567\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 528 | loss: 0.49567 - acc: 0.7827 -- iter: 891/891\n",
      "--\n",
      "Training Step: 529  | total loss: \u001b[1m\u001b[32m0.49106\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 529 | loss: 0.49106 - acc: 0.7842 -- iter: 891/891\n",
      "--\n",
      "Training Step: 530  | total loss: \u001b[1m\u001b[32m0.54219\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 530 | loss: 0.54219 - acc: 0.7619 -- iter: 891/891\n",
      "--\n",
      "Training Step: 531  | total loss: \u001b[1m\u001b[32m0.53238\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 531 | loss: 0.53238 - acc: 0.7665 -- iter: 891/891\n",
      "--\n",
      "Training Step: 532  | total loss: \u001b[1m\u001b[32m0.57790\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 532 | loss: 0.57790 - acc: 0.7407 -- iter: 891/891\n",
      "--\n",
      "Training Step: 533  | total loss: \u001b[1m\u001b[32m0.56609\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 533 | loss: 0.56609 - acc: 0.7472 -- iter: 891/891\n",
      "--\n",
      "Training Step: 534  | total loss: \u001b[1m\u001b[32m0.59017\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 534 | loss: 0.59017 - acc: 0.7278 -- iter: 891/891\n",
      "--\n",
      "Training Step: 535  | total loss: \u001b[1m\u001b[32m0.58102\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 535 | loss: 0.58102 - acc: 0.7347 -- iter: 891/891\n",
      "--\n",
      "Training Step: 536  | total loss: \u001b[1m\u001b[32m0.59773\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 536 | loss: 0.59773 - acc: 0.7152 -- iter: 891/891\n",
      "--\n",
      "Training Step: 537  | total loss: \u001b[1m\u001b[32m0.59085\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 537 | loss: 0.59085 - acc: 0.7224 -- iter: 891/891\n",
      "--\n",
      "Training Step: 538  | total loss: \u001b[1m\u001b[32m0.60477\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 538 | loss: 0.60477 - acc: 0.7044 -- iter: 891/891\n",
      "--\n",
      "Training Step: 539  | total loss: \u001b[1m\u001b[32m0.59858\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 539 | loss: 0.59858 - acc: 0.7125 -- iter: 891/891\n",
      "--\n",
      "Training Step: 540  | total loss: \u001b[1m\u001b[32m0.60985\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 540 | loss: 0.60985 - acc: 0.6953 -- iter: 891/891\n",
      "--\n",
      "Training Step: 541  | total loss: \u001b[1m\u001b[32m0.60370\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 541 | loss: 0.60370 - acc: 0.7041 -- iter: 891/891\n",
      "--\n",
      "Training Step: 542  | total loss: \u001b[1m\u001b[32m0.59804\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 542 | loss: 0.59804 - acc: 0.7117 -- iter: 891/891\n",
      "--\n",
      "Training Step: 543  | total loss: \u001b[1m\u001b[32m0.59238\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 543 | loss: 0.59238 - acc: 0.7188 -- iter: 891/891\n",
      "--\n",
      "Training Step: 544  | total loss: \u001b[1m\u001b[32m0.58642\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 544 | loss: 0.58642 - acc: 0.7257 -- iter: 891/891\n",
      "--\n",
      "Training Step: 545  | total loss: \u001b[1m\u001b[32m0.57999\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 545 | loss: 0.57999 - acc: 0.7302 -- iter: 891/891\n",
      "--\n",
      "Training Step: 546  | total loss: \u001b[1m\u001b[32m0.57311\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 546 | loss: 0.57311 - acc: 0.7341 -- iter: 891/891\n",
      "--\n",
      "Training Step: 547  | total loss: \u001b[1m\u001b[32m0.56591\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 547 | loss: 0.56591 - acc: 0.7378 -- iter: 891/891\n",
      "--\n",
      "Training Step: 548  | total loss: \u001b[1m\u001b[32m0.58724\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 548 | loss: 0.58724 - acc: 0.7212 -- iter: 891/891\n",
      "--\n",
      "Training Step: 549  | total loss: \u001b[1m\u001b[32m0.57769\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 549 | loss: 0.57769 - acc: 0.7269 -- iter: 891/891\n",
      "--\n",
      "Training Step: 550  | total loss: \u001b[1m\u001b[32m0.60132\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 550 | loss: 0.60132 - acc: 0.7117 -- iter: 891/891\n",
      "--\n",
      "Training Step: 551  | total loss: \u001b[1m\u001b[32m0.59031\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 551 | loss: 0.59031 - acc: 0.7185 -- iter: 891/891\n",
      "--\n",
      "Training Step: 552  | total loss: \u001b[1m\u001b[32m0.61454\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 552 | loss: 0.61454 - acc: 0.7005 -- iter: 891/891\n",
      "--\n",
      "Training Step: 553  | total loss: \u001b[1m\u001b[32m0.60264\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 553 | loss: 0.60264 - acc: 0.7098 -- iter: 891/891\n",
      "--\n",
      "Training Step: 554  | total loss: \u001b[1m\u001b[32m0.61595\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 554 | loss: 0.61595 - acc: 0.6966 -- iter: 891/891\n",
      "--\n",
      "Training Step: 555  | total loss: \u001b[1m\u001b[32m0.60468\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 555 | loss: 0.60468 - acc: 0.7071 -- iter: 891/891\n",
      "--\n",
      "Training Step: 556  | total loss: \u001b[1m\u001b[32m0.61981\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 556 | loss: 0.61981 - acc: 0.6915 -- iter: 891/891\n",
      "--\n",
      "Training Step: 557  | total loss: \u001b[1m\u001b[32m0.60952\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 557 | loss: 0.60952 - acc: 0.7007 -- iter: 891/891\n",
      "--\n",
      "Training Step: 558  | total loss: \u001b[1m\u001b[32m0.62039\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 558 | loss: 0.62039 - acc: 0.6864 -- iter: 891/891\n",
      "--\n",
      "Training Step: 559  | total loss: \u001b[1m\u001b[32m0.61181\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 559 | loss: 0.61181 - acc: 0.6954 -- iter: 891/891\n",
      "--\n",
      "Training Step: 560  | total loss: \u001b[1m\u001b[32m0.60474\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 560 | loss: 0.60474 - acc: 0.7027 -- iter: 891/891\n",
      "--\n",
      "Training Step: 561  | total loss: \u001b[1m\u001b[32m0.59857\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 561 | loss: 0.59857 - acc: 0.7084 -- iter: 891/891\n",
      "--\n",
      "Training Step: 562  | total loss: \u001b[1m\u001b[32m0.60922\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 562 | loss: 0.60922 - acc: 0.6948 -- iter: 891/891\n",
      "--\n",
      "Training Step: 563  | total loss: \u001b[1m\u001b[32m0.60236\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 563 | loss: 0.60236 - acc: 0.7021 -- iter: 891/891\n",
      "--\n",
      "Training Step: 564  | total loss: \u001b[1m\u001b[32m0.59577\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 564 | loss: 0.59577 - acc: 0.7106 -- iter: 891/891\n",
      "--\n",
      "Training Step: 565  | total loss: \u001b[1m\u001b[32m0.58899\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 565 | loss: 0.58899 - acc: 0.7194 -- iter: 891/891\n",
      "--\n",
      "Training Step: 566  | total loss: \u001b[1m\u001b[32m0.60182\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 566 | loss: 0.60182 - acc: 0.7012 -- iter: 891/891\n",
      "--\n",
      "Training Step: 567  | total loss: \u001b[1m\u001b[32m0.59271\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 567 | loss: 0.59271 - acc: 0.7117 -- iter: 891/891\n",
      "--\n",
      "Training Step: 568  | total loss: \u001b[1m\u001b[32m0.60835\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 568 | loss: 0.60835 - acc: 0.6952 -- iter: 891/891\n",
      "--\n",
      "Training Step: 569  | total loss: \u001b[1m\u001b[32m0.59757\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 569 | loss: 0.59757 - acc: 0.7071 -- iter: 891/891\n",
      "--\n",
      "Training Step: 570  | total loss: \u001b[1m\u001b[32m0.58735\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 570 | loss: 0.58735 - acc: 0.7177 -- iter: 891/891\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 571  | total loss: \u001b[1m\u001b[32m0.57735\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 571 | loss: 0.57735 - acc: 0.7276 -- iter: 891/891\n",
      "--\n",
      "Training Step: 572  | total loss: \u001b[1m\u001b[32m0.59467\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 572 | loss: 0.59467 - acc: 0.7113 -- iter: 891/891\n",
      "--\n",
      "Training Step: 573  | total loss: \u001b[1m\u001b[32m0.58280\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 573 | loss: 0.58280 - acc: 0.7218 -- iter: 891/891\n",
      "--\n",
      "Training Step: 574  | total loss: \u001b[1m\u001b[32m0.57173\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 574 | loss: 0.57173 - acc: 0.7311 -- iter: 891/891\n",
      "--\n",
      "Training Step: 575  | total loss: \u001b[1m\u001b[32m0.56125\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 575 | loss: 0.56125 - acc: 0.7396 -- iter: 891/891\n",
      "--\n",
      "Training Step: 576  | total loss: \u001b[1m\u001b[32m0.58976\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 576 | loss: 0.58976 - acc: 0.7201 -- iter: 891/891\n",
      "--\n",
      "Training Step: 577  | total loss: \u001b[1m\u001b[32m0.57687\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 577 | loss: 0.57687 - acc: 0.7297 -- iter: 891/891\n",
      "--\n",
      "Training Step: 578  | total loss: \u001b[1m\u001b[32m0.60174\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 578 | loss: 0.60174 - acc: 0.7111 -- iter: 891/891\n",
      "--\n",
      "Training Step: 579  | total loss: \u001b[1m\u001b[32m0.58798\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 579 | loss: 0.58798 - acc: 0.7210 -- iter: 891/891\n",
      "--\n",
      "Training Step: 580  | total loss: \u001b[1m\u001b[32m0.57594\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 580 | loss: 0.57594 - acc: 0.7299 -- iter: 891/891\n",
      "--\n",
      "Training Step: 581  | total loss: \u001b[1m\u001b[32m0.56520\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 581 | loss: 0.56520 - acc: 0.7381 -- iter: 891/891\n",
      "--\n",
      "Training Step: 582  | total loss: \u001b[1m\u001b[32m0.59079\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 582 | loss: 0.59079 - acc: 0.7164 -- iter: 891/891\n",
      "--\n",
      "Training Step: 583  | total loss: \u001b[1m\u001b[32m0.57901\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 583 | loss: 0.57901 - acc: 0.7262 -- iter: 891/891\n",
      "--\n",
      "Training Step: 584  | total loss: \u001b[1m\u001b[32m0.56864\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 584 | loss: 0.56864 - acc: 0.7349 -- iter: 891/891\n",
      "--\n",
      "Training Step: 585  | total loss: \u001b[1m\u001b[32m0.55901\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 585 | loss: 0.55901 - acc: 0.7427 -- iter: 891/891\n",
      "--\n",
      "Training Step: 586  | total loss: \u001b[1m\u001b[32m0.54968\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 586 | loss: 0.54968 - acc: 0.7496 -- iter: 891/891\n",
      "--\n",
      "Training Step: 587  | total loss: \u001b[1m\u001b[32m0.54053\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 587 | loss: 0.54053 - acc: 0.7555 -- iter: 891/891\n",
      "--\n",
      "Training Step: 588  | total loss: \u001b[1m\u001b[32m0.53172\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 588 | loss: 0.53172 - acc: 0.7612 -- iter: 891/891\n",
      "--\n",
      "Training Step: 589  | total loss: \u001b[1m\u001b[32m0.52353\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 589 | loss: 0.52353 - acc: 0.7664 -- iter: 891/891\n",
      "--\n",
      "Training Step: 590  | total loss: \u001b[1m\u001b[32m0.56305\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 590 | loss: 0.56305 - acc: 0.7448 -- iter: 891/891\n",
      "--\n",
      "Training Step: 591  | total loss: \u001b[1m\u001b[32m0.55175\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 591 | loss: 0.55175 - acc: 0.7515 -- iter: 891/891\n",
      "--\n",
      "Training Step: 592  | total loss: \u001b[1m\u001b[32m0.54151\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 592 | loss: 0.54151 - acc: 0.7573 -- iter: 891/891\n",
      "--\n",
      "Training Step: 593  | total loss: \u001b[1m\u001b[32m0.53220\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 593 | loss: 0.53220 - acc: 0.7621 -- iter: 891/891\n",
      "--\n",
      "Training Step: 594  | total loss: \u001b[1m\u001b[32m0.52371\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 594 | loss: 0.52371 - acc: 0.7664 -- iter: 891/891\n",
      "--\n",
      "Training Step: 595  | total loss: \u001b[1m\u001b[32m0.51593\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 595 | loss: 0.51593 - acc: 0.7704 -- iter: 891/891\n",
      "--\n",
      "Training Step: 596  | total loss: \u001b[1m\u001b[32m0.50886\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 596 | loss: 0.50886 - acc: 0.7742 -- iter: 891/891\n",
      "--\n",
      "Training Step: 597  | total loss: \u001b[1m\u001b[32m0.50247\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 597 | loss: 0.50247 - acc: 0.7777 -- iter: 891/891\n",
      "--\n",
      "Training Step: 598  | total loss: \u001b[1m\u001b[32m0.53993\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 598 | loss: 0.53993 - acc: 0.7574 -- iter: 891/891\n",
      "--\n",
      "Training Step: 599  | total loss: \u001b[1m\u001b[32m0.53081\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 599 | loss: 0.53081 - acc: 0.7620 -- iter: 891/891\n",
      "--\n",
      "Training Step: 600  | total loss: \u001b[1m\u001b[32m0.52306\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 600 | loss: 0.52306 - acc: 0.7662 -- iter: 891/891\n",
      "--\n",
      "Training Step: 601  | total loss: \u001b[1m\u001b[32m0.51627\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 601 | loss: 0.51627 - acc: 0.7699 -- iter: 891/891\n",
      "--\n",
      "Training Step: 602  | total loss: \u001b[1m\u001b[32m0.55518\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 602 | loss: 0.55518 - acc: 0.7452 -- iter: 891/891\n",
      "--\n",
      "Training Step: 603  | total loss: \u001b[1m\u001b[32m0.54558\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 603 | loss: 0.54558 - acc: 0.7511 -- iter: 891/891\n",
      "--\n",
      "Training Step: 604  | total loss: \u001b[1m\u001b[32m0.53712\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 604 | loss: 0.53712 - acc: 0.7562 -- iter: 891/891\n",
      "--\n",
      "Training Step: 605  | total loss: \u001b[1m\u001b[32m0.52931\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 605 | loss: 0.52931 - acc: 0.7611 -- iter: 891/891\n",
      "--\n",
      "Training Step: 606  | total loss: \u001b[1m\u001b[32m0.52191\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 606 | loss: 0.52191 - acc: 0.7654 -- iter: 891/891\n",
      "--\n",
      "Training Step: 607  | total loss: \u001b[1m\u001b[32m0.51489\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 607 | loss: 0.51489 - acc: 0.7700 -- iter: 891/891\n",
      "--\n",
      "Training Step: 608  | total loss: \u001b[1m\u001b[32m0.50838\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 608 | loss: 0.50838 - acc: 0.7741 -- iter: 891/891\n",
      "--\n",
      "Training Step: 609  | total loss: \u001b[1m\u001b[32m0.50248\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 609 | loss: 0.50248 - acc: 0.7771 -- iter: 891/891\n",
      "--\n",
      "Training Step: 610  | total loss: \u001b[1m\u001b[32m0.54618\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 610 | loss: 0.54618 - acc: 0.7543 -- iter: 891/891\n",
      "--\n",
      "Training Step: 611  | total loss: \u001b[1m\u001b[32m0.53659\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 611 | loss: 0.53659 - acc: 0.7593 -- iter: 891/891\n",
      "--\n",
      "Training Step: 612  | total loss: \u001b[1m\u001b[32m0.57367\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 612 | loss: 0.57367 - acc: 0.7387 -- iter: 891/891\n",
      "--\n",
      "Training Step: 613  | total loss: \u001b[1m\u001b[32m0.56140\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 613 | loss: 0.56140 - acc: 0.7456 -- iter: 891/891\n",
      "--\n",
      "Training Step: 614  | total loss: \u001b[1m\u001b[32m0.59193\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 614 | loss: 0.59193 - acc: 0.7250 -- iter: 891/891\n",
      "--\n",
      "Training Step: 615  | total loss: \u001b[1m\u001b[32m0.57904\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 615 | loss: 0.57904 - acc: 0.7333 -- iter: 891/891\n",
      "--\n",
      "Training Step: 616  | total loss: \u001b[1m\u001b[32m0.59848\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 616 | loss: 0.59848 - acc: 0.7144 -- iter: 891/891\n",
      "--\n",
      "Training Step: 617  | total loss: \u001b[1m\u001b[32m0.58781\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 617 | loss: 0.58781 - acc: 0.7237 -- iter: 891/891\n",
      "--\n",
      "Training Step: 618  | total loss: \u001b[1m\u001b[32m0.57939\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 618 | loss: 0.57939 - acc: 0.7315 -- iter: 891/891\n",
      "--\n",
      "Training Step: 619  | total loss: \u001b[1m\u001b[32m0.57218\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 619 | loss: 0.57218 - acc: 0.7386 -- iter: 891/891\n",
      "--\n",
      "Training Step: 620  | total loss: \u001b[1m\u001b[32m0.56535\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 620 | loss: 0.56535 - acc: 0.7453 -- iter: 891/891\n",
      "--\n",
      "Training Step: 621  | total loss: \u001b[1m\u001b[32m0.55830\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 621 | loss: 0.55830 - acc: 0.7518 -- iter: 891/891\n",
      "--\n",
      "Training Step: 622  | total loss: \u001b[1m\u001b[32m0.55078\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 622 | loss: 0.55078 - acc: 0.7577 -- iter: 891/891\n",
      "--\n",
      "Training Step: 623  | total loss: \u001b[1m\u001b[32m0.54288\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 623 | loss: 0.54288 - acc: 0.7633 -- iter: 891/891\n",
      "--\n",
      "Training Step: 624  | total loss: \u001b[1m\u001b[32m0.53493\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 624 | loss: 0.53493 - acc: 0.7686 -- iter: 891/891\n",
      "--\n",
      "Training Step: 625  | total loss: \u001b[1m\u001b[32m0.52734\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 625 | loss: 0.52734 - acc: 0.7731 -- iter: 891/891\n",
      "--\n",
      "Training Step: 626  | total loss: \u001b[1m\u001b[32m0.52042\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 626 | loss: 0.52042 - acc: 0.7769 -- iter: 891/891\n",
      "--\n",
      "Training Step: 627  | total loss: \u001b[1m\u001b[32m0.51438\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 627 | loss: 0.51438 - acc: 0.7800 -- iter: 891/891\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 628  | total loss: \u001b[1m\u001b[32m0.56304\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 628 | loss: 0.56304 - acc: 0.7563 -- iter: 891/891\n",
      "--\n",
      "Training Step: 629  | total loss: \u001b[1m\u001b[32m0.55303\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 629 | loss: 0.55303 - acc: 0.7597 -- iter: 891/891\n",
      "--\n",
      "Training Step: 630  | total loss: \u001b[1m\u001b[32m0.59606\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 630 | loss: 0.59606 - acc: 0.7378 -- iter: 891/891\n",
      "--\n",
      "Training Step: 631  | total loss: \u001b[1m\u001b[32m0.58177\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 631 | loss: 0.58177 - acc: 0.7446 -- iter: 891/891\n",
      "--\n",
      "Training Step: 632  | total loss: \u001b[1m\u001b[32m0.61961\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 632 | loss: 0.61961 - acc: 0.7220 -- iter: 891/891\n",
      "--\n",
      "Training Step: 633  | total loss: \u001b[1m\u001b[32m0.60287\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 633 | loss: 0.60287 - acc: 0.7311 -- iter: 891/891\n",
      "--\n",
      "Training Step: 634  | total loss: \u001b[1m\u001b[32m0.58886\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 634 | loss: 0.58886 - acc: 0.7390 -- iter: 891/891\n",
      "--\n",
      "Training Step: 635  | total loss: \u001b[1m\u001b[32m0.57721\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 635 | loss: 0.57721 - acc: 0.7464 -- iter: 891/891\n",
      "--\n",
      "Training Step: 636  | total loss: \u001b[1m\u001b[32m0.56719\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 636 | loss: 0.56719 - acc: 0.7530 -- iter: 891/891\n",
      "--\n",
      "Training Step: 637  | total loss: \u001b[1m\u001b[32m0.55814\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 637 | loss: 0.55814 - acc: 0.7588 -- iter: 891/891\n",
      "--\n",
      "Training Step: 638  | total loss: \u001b[1m\u001b[32m0.54953\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 638 | loss: 0.54953 - acc: 0.7633 -- iter: 891/891\n",
      "--\n",
      "Training Step: 639  | total loss: \u001b[1m\u001b[32m0.54094\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 639 | loss: 0.54094 - acc: 0.7676 -- iter: 891/891\n",
      "--\n",
      "Training Step: 640  | total loss: \u001b[1m\u001b[32m0.53233\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 640 | loss: 0.53233 - acc: 0.7721 -- iter: 891/891\n",
      "--\n",
      "Training Step: 641  | total loss: \u001b[1m\u001b[32m0.52411\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 641 | loss: 0.52411 - acc: 0.7763 -- iter: 891/891\n",
      "--\n",
      "Training Step: 642  | total loss: \u001b[1m\u001b[32m0.51665\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 642 | loss: 0.51665 - acc: 0.7800 -- iter: 891/891\n",
      "--\n",
      "Training Step: 643  | total loss: \u001b[1m\u001b[32m0.51001\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 643 | loss: 0.51001 - acc: 0.7831 -- iter: 891/891\n",
      "--\n",
      "Training Step: 644  | total loss: \u001b[1m\u001b[32m0.50417\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 644 | loss: 0.50417 - acc: 0.7858 -- iter: 891/891\n",
      "--\n",
      "Training Step: 645  | total loss: \u001b[1m\u001b[32m0.49914\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 645 | loss: 0.49914 - acc: 0.7869 -- iter: 891/891\n",
      "--\n",
      "Training Step: 646  | total loss: \u001b[1m\u001b[32m0.49470\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 646 | loss: 0.49470 - acc: 0.7876 -- iter: 891/891\n",
      "--\n",
      "Training Step: 647  | total loss: \u001b[1m\u001b[32m0.49045\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 647 | loss: 0.49045 - acc: 0.7886 -- iter: 891/891\n",
      "--\n",
      "Training Step: 648  | total loss: \u001b[1m\u001b[32m0.48623\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 648 | loss: 0.48623 - acc: 0.7905 -- iter: 891/891\n",
      "--\n",
      "Training Step: 649  | total loss: \u001b[1m\u001b[32m0.48217\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 649 | loss: 0.48217 - acc: 0.7924 -- iter: 891/891\n",
      "--\n",
      "Training Step: 650  | total loss: \u001b[1m\u001b[32m0.47832\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 650 | loss: 0.47832 - acc: 0.7943 -- iter: 891/891\n",
      "--\n",
      "Training Step: 651  | total loss: \u001b[1m\u001b[32m0.47482\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 651 | loss: 0.47482 - acc: 0.7956 -- iter: 891/891\n",
      "--\n",
      "Training Step: 652  | total loss: \u001b[1m\u001b[32m0.47193\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 652 | loss: 0.47193 - acc: 0.7962 -- iter: 891/891\n",
      "--\n",
      "Training Step: 653  | total loss: \u001b[1m\u001b[32m0.46956\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 653 | loss: 0.46956 - acc: 0.7966 -- iter: 891/891\n",
      "--\n",
      "Training Step: 654  | total loss: \u001b[1m\u001b[32m0.51936\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 654 | loss: 0.51936 - acc: 0.7691 -- iter: 891/891\n",
      "--\n",
      "Training Step: 655  | total loss: \u001b[1m\u001b[32m0.51271\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 655 | loss: 0.51271 - acc: 0.7726 -- iter: 891/891\n",
      "--\n",
      "Training Step: 656  | total loss: \u001b[1m\u001b[32m0.50690\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 656 | loss: 0.50690 - acc: 0.7756 -- iter: 891/891\n",
      "--\n",
      "Training Step: 657  | total loss: \u001b[1m\u001b[32m0.50143\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 657 | loss: 0.50143 - acc: 0.7782 -- iter: 891/891\n",
      "--\n",
      "Training Step: 658  | total loss: \u001b[1m\u001b[32m0.49620\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 658 | loss: 0.49620 - acc: 0.7808 -- iter: 891/891\n",
      "--\n",
      "Training Step: 659  | total loss: \u001b[1m\u001b[32m0.49127\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 659 | loss: 0.49127 - acc: 0.7832 -- iter: 891/891\n",
      "--\n",
      "Training Step: 660  | total loss: \u001b[1m\u001b[32m0.48674\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 660 | loss: 0.48674 - acc: 0.7851 -- iter: 891/891\n",
      "--\n",
      "Training Step: 661  | total loss: \u001b[1m\u001b[32m0.48277\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 661 | loss: 0.48277 - acc: 0.7871 -- iter: 891/891\n",
      "--\n",
      "Training Step: 662  | total loss: \u001b[1m\u001b[32m0.47936\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 662 | loss: 0.47936 - acc: 0.7889 -- iter: 891/891\n",
      "--\n",
      "Training Step: 663  | total loss: \u001b[1m\u001b[32m0.47641\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 663 | loss: 0.47641 - acc: 0.7907 -- iter: 891/891\n",
      "--\n",
      "Training Step: 664  | total loss: \u001b[1m\u001b[32m0.47377\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 664 | loss: 0.47377 - acc: 0.7921 -- iter: 891/891\n",
      "--\n",
      "Training Step: 665  | total loss: \u001b[1m\u001b[32m0.47137\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 665 | loss: 0.47137 - acc: 0.7932 -- iter: 891/891\n",
      "--\n",
      "Training Step: 666  | total loss: \u001b[1m\u001b[32m0.46910\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 666 | loss: 0.46910 - acc: 0.7941 -- iter: 891/891\n",
      "--\n",
      "Training Step: 667  | total loss: \u001b[1m\u001b[32m0.46685\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 667 | loss: 0.46685 - acc: 0.7949 -- iter: 891/891\n",
      "--\n",
      "Training Step: 668  | total loss: \u001b[1m\u001b[32m0.46460\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 668 | loss: 0.46460 - acc: 0.7963 -- iter: 891/891\n",
      "--\n",
      "Training Step: 669  | total loss: \u001b[1m\u001b[32m0.46244\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 669 | loss: 0.46244 - acc: 0.7977 -- iter: 891/891\n",
      "--\n",
      "Training Step: 670  | total loss: \u001b[1m\u001b[32m0.51199\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 670 | loss: 0.51199 - acc: 0.7712 -- iter: 891/891\n",
      "--\n",
      "Training Step: 671  | total loss: \u001b[1m\u001b[32m0.50540\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 671 | loss: 0.50540 - acc: 0.7748 -- iter: 891/891\n",
      "--\n",
      "Training Step: 672  | total loss: \u001b[1m\u001b[32m0.50009\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 672 | loss: 0.50009 - acc: 0.7775 -- iter: 891/891\n",
      "--\n",
      "Training Step: 673  | total loss: \u001b[1m\u001b[32m0.49574\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 673 | loss: 0.49574 - acc: 0.7799 -- iter: 891/891\n",
      "--\n",
      "Training Step: 674  | total loss: \u001b[1m\u001b[32m0.49178\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 674 | loss: 0.49178 - acc: 0.7820 -- iter: 891/891\n",
      "--\n",
      "Training Step: 675  | total loss: \u001b[1m\u001b[32m0.48776\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 675 | loss: 0.48776 - acc: 0.7840 -- iter: 891/891\n",
      "--\n",
      "Training Step: 676  | total loss: \u001b[1m\u001b[32m0.48360\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 676 | loss: 0.48360 - acc: 0.7864 -- iter: 891/891\n",
      "--\n",
      "Training Step: 677  | total loss: \u001b[1m\u001b[32m0.47957\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 677 | loss: 0.47957 - acc: 0.7888 -- iter: 891/891\n",
      "--\n",
      "Training Step: 678  | total loss: \u001b[1m\u001b[32m0.47593\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 678 | loss: 0.47593 - acc: 0.7910 -- iter: 891/891\n",
      "--\n",
      "Training Step: 679  | total loss: \u001b[1m\u001b[32m0.47279\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 679 | loss: 0.47279 - acc: 0.7927 -- iter: 891/891\n",
      "--\n",
      "Training Step: 680  | total loss: \u001b[1m\u001b[32m0.52812\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 680 | loss: 0.52812 - acc: 0.7655 -- iter: 891/891\n",
      "--\n",
      "Training Step: 681  | total loss: \u001b[1m\u001b[32m0.51990\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 681 | loss: 0.51990 - acc: 0.7688 -- iter: 891/891\n",
      "--\n",
      "Training Step: 682  | total loss: \u001b[1m\u001b[32m0.51239\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 682 | loss: 0.51239 - acc: 0.7723 -- iter: 891/891\n",
      "--\n",
      "Training Step: 683  | total loss: \u001b[1m\u001b[32m0.50551\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 683 | loss: 0.50551 - acc: 0.7759 -- iter: 891/891\n",
      "--\n",
      "Training Step: 684  | total loss: \u001b[1m\u001b[32m0.49929\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 684 | loss: 0.49929 - acc: 0.7796 -- iter: 891/891\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 685  | total loss: \u001b[1m\u001b[32m0.49375\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 685 | loss: 0.49375 - acc: 0.7829 -- iter: 891/891\n",
      "--\n",
      "Training Step: 686  | total loss: \u001b[1m\u001b[32m0.53162\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 686 | loss: 0.53162 - acc: 0.7607 -- iter: 891/891\n",
      "--\n",
      "Training Step: 687  | total loss: \u001b[1m\u001b[32m0.52349\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 687 | loss: 0.52349 - acc: 0.7653 -- iter: 891/891\n",
      "--\n",
      "Training Step: 688  | total loss: \u001b[1m\u001b[32m0.55492\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 688 | loss: 0.55492 - acc: 0.7423 -- iter: 891/891\n",
      "--\n",
      "Training Step: 689  | total loss: \u001b[1m\u001b[32m0.54697\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 689 | loss: 0.54697 - acc: 0.7480 -- iter: 891/891\n",
      "--\n",
      "Training Step: 690  | total loss: \u001b[1m\u001b[32m0.56747\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 690 | loss: 0.56747 - acc: 0.7298 -- iter: 891/891\n",
      "--\n",
      "Training Step: 691  | total loss: \u001b[1m\u001b[32m0.56103\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 691 | loss: 0.56103 - acc: 0.7365 -- iter: 891/891\n",
      "--\n",
      "Training Step: 692  | total loss: \u001b[1m\u001b[32m0.57923\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 692 | loss: 0.57923 - acc: 0.7169 -- iter: 891/891\n",
      "--\n",
      "Training Step: 693  | total loss: \u001b[1m\u001b[32m0.57334\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 693 | loss: 0.57334 - acc: 0.7245 -- iter: 891/891\n",
      "--\n",
      "Training Step: 694  | total loss: \u001b[1m\u001b[32m0.59048\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 694 | loss: 0.59048 - acc: 0.7052 -- iter: 891/891\n",
      "--\n",
      "Training Step: 695  | total loss: \u001b[1m\u001b[32m0.58436\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 695 | loss: 0.58436 - acc: 0.7147 -- iter: 891/891\n",
      "--\n",
      "Training Step: 696  | total loss: \u001b[1m\u001b[32m0.59425\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 696 | loss: 0.59425 - acc: 0.7021 -- iter: 891/891\n",
      "--\n",
      "Training Step: 697  | total loss: \u001b[1m\u001b[32m0.58817\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 697 | loss: 0.58817 - acc: 0.7121 -- iter: 891/891\n",
      "--\n",
      "Training Step: 698  | total loss: \u001b[1m\u001b[32m0.60088\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 698 | loss: 0.60088 - acc: 0.6947 -- iter: 891/891\n",
      "--\n",
      "Training Step: 699  | total loss: \u001b[1m\u001b[32m0.59413\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 699 | loss: 0.59413 - acc: 0.7053 -- iter: 891/891\n",
      "--\n",
      "Training Step: 700  | total loss: \u001b[1m\u001b[32m0.58779\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 700 | loss: 0.58779 - acc: 0.7145 -- iter: 891/891\n",
      "--\n",
      "Training Step: 701  | total loss: \u001b[1m\u001b[32m0.58148\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 701 | loss: 0.58148 - acc: 0.7220 -- iter: 891/891\n",
      "--\n",
      "Training Step: 702  | total loss: \u001b[1m\u001b[32m0.59513\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 702 | loss: 0.59513 - acc: 0.7062 -- iter: 891/891\n",
      "--\n",
      "Training Step: 703  | total loss: \u001b[1m\u001b[32m0.58698\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 703 | loss: 0.58698 - acc: 0.7123 -- iter: 891/891\n",
      "--\n",
      "Training Step: 704  | total loss: \u001b[1m\u001b[32m0.57913\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 704 | loss: 0.57913 - acc: 0.7182 -- iter: 891/891\n",
      "--\n",
      "Training Step: 705  | total loss: \u001b[1m\u001b[32m0.57141\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 705 | loss: 0.57141 - acc: 0.7237 -- iter: 891/891\n",
      "--\n",
      "Training Step: 706  | total loss: \u001b[1m\u001b[32m0.58943\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 706 | loss: 0.58943 - acc: 0.7090 -- iter: 891/891\n",
      "--\n",
      "Training Step: 707  | total loss: \u001b[1m\u001b[32m0.57978\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 707 | loss: 0.57978 - acc: 0.7157 -- iter: 891/891\n",
      "--\n",
      "Training Step: 708  | total loss: \u001b[1m\u001b[32m0.57077\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 708 | loss: 0.57077 - acc: 0.7221 -- iter: 891/891\n",
      "--\n",
      "Training Step: 709  | total loss: \u001b[1m\u001b[32m0.56226\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 709 | loss: 0.56226 - acc: 0.7287 -- iter: 891/891\n",
      "--\n",
      "Training Step: 710  | total loss: \u001b[1m\u001b[32m0.55412\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 710 | loss: 0.55412 - acc: 0.7351 -- iter: 891/891\n",
      "--\n",
      "Training Step: 711  | total loss: \u001b[1m\u001b[32m0.54629\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 711 | loss: 0.54629 - acc: 0.7406 -- iter: 891/891\n",
      "--\n",
      "Training Step: 712  | total loss: \u001b[1m\u001b[32m0.53874\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 712 | loss: 0.53874 - acc: 0.7470 -- iter: 891/891\n",
      "--\n",
      "Training Step: 713  | total loss: \u001b[1m\u001b[32m0.53152\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 713 | loss: 0.53152 - acc: 0.7526 -- iter: 891/891\n",
      "--\n",
      "Training Step: 714  | total loss: \u001b[1m\u001b[32m0.52462\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 714 | loss: 0.52462 - acc: 0.7578 -- iter: 891/891\n",
      "--\n",
      "Training Step: 715  | total loss: \u001b[1m\u001b[32m0.51806\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 715 | loss: 0.51806 - acc: 0.7631 -- iter: 891/891\n",
      "--\n",
      "Training Step: 716  | total loss: \u001b[1m\u001b[32m0.51179\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 716 | loss: 0.51179 - acc: 0.7682 -- iter: 891/891\n",
      "--\n",
      "Training Step: 717  | total loss: \u001b[1m\u001b[32m0.50582\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 717 | loss: 0.50582 - acc: 0.7721 -- iter: 891/891\n",
      "--\n",
      "Training Step: 718  | total loss: \u001b[1m\u001b[32m0.50013\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 718 | loss: 0.50013 - acc: 0.7755 -- iter: 891/891\n",
      "--\n",
      "Training Step: 719  | total loss: \u001b[1m\u001b[32m0.49477\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 719 | loss: 0.49477 - acc: 0.7789 -- iter: 891/891\n",
      "--\n",
      "Training Step: 720  | total loss: \u001b[1m\u001b[32m0.48989\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 720 | loss: 0.48989 - acc: 0.7819 -- iter: 891/891\n",
      "--\n",
      "Training Step: 721  | total loss: \u001b[1m\u001b[32m0.48568\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 721 | loss: 0.48568 - acc: 0.7846 -- iter: 891/891\n",
      "--\n",
      "Training Step: 722  | total loss: \u001b[1m\u001b[32m0.53521\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 722 | loss: 0.53521 - acc: 0.7606 -- iter: 891/891\n",
      "--\n",
      "Training Step: 723  | total loss: \u001b[1m\u001b[32m0.52758\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 723 | loss: 0.52758 - acc: 0.7654 -- iter: 891/891\n",
      "--\n",
      "Training Step: 724  | total loss: \u001b[1m\u001b[32m0.52111\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 724 | loss: 0.52111 - acc: 0.7691 -- iter: 891/891\n",
      "--\n",
      "Training Step: 725  | total loss: \u001b[1m\u001b[32m0.51496\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 725 | loss: 0.51496 - acc: 0.7726 -- iter: 891/891\n",
      "--\n",
      "Training Step: 726  | total loss: \u001b[1m\u001b[32m0.50861\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 726 | loss: 0.50861 - acc: 0.7758 -- iter: 891/891\n",
      "--\n",
      "Training Step: 727  | total loss: \u001b[1m\u001b[32m0.50221\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 727 | loss: 0.50221 - acc: 0.7783 -- iter: 891/891\n",
      "--\n",
      "Training Step: 728  | total loss: \u001b[1m\u001b[32m0.49627\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 728 | loss: 0.49627 - acc: 0.7817 -- iter: 891/891\n",
      "--\n",
      "Training Step: 729  | total loss: \u001b[1m\u001b[32m0.49112\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 729 | loss: 0.49112 - acc: 0.7840 -- iter: 891/891\n",
      "--\n",
      "Training Step: 730  | total loss: \u001b[1m\u001b[32m0.54150\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 730 | loss: 0.54150 - acc: 0.7591 -- iter: 891/891\n",
      "--\n",
      "Training Step: 731  | total loss: \u001b[1m\u001b[32m0.53210\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 731 | loss: 0.53210 - acc: 0.7637 -- iter: 891/891\n",
      "--\n",
      "Training Step: 732  | total loss: \u001b[1m\u001b[32m0.52373\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 732 | loss: 0.52373 - acc: 0.7675 -- iter: 891/891\n",
      "--\n",
      "Training Step: 733  | total loss: \u001b[1m\u001b[32m0.51626\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 733 | loss: 0.51626 - acc: 0.7709 -- iter: 891/891\n",
      "--\n",
      "Training Step: 734  | total loss: \u001b[1m\u001b[32m0.55743\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 734 | loss: 0.55743 - acc: 0.7452 -- iter: 891/891\n",
      "--\n",
      "Training Step: 735  | total loss: \u001b[1m\u001b[32m0.54708\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 735 | loss: 0.54708 - acc: 0.7512 -- iter: 891/891\n",
      "--\n",
      "Training Step: 736  | total loss: \u001b[1m\u001b[32m0.57555\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 736 | loss: 0.57555 - acc: 0.7292 -- iter: 891/891\n",
      "--\n",
      "Training Step: 737  | total loss: \u001b[1m\u001b[32m0.56564\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 737 | loss: 0.56564 - acc: 0.7370 -- iter: 891/891\n",
      "--\n",
      "Training Step: 738  | total loss: \u001b[1m\u001b[32m0.58736\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 738 | loss: 0.58736 - acc: 0.7156 -- iter: 891/891\n",
      "--\n",
      "Training Step: 739  | total loss: \u001b[1m\u001b[32m0.57971\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 739 | loss: 0.57971 - acc: 0.7240 -- iter: 891/891\n",
      "--\n",
      "Training Step: 740  | total loss: \u001b[1m\u001b[32m0.59275\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 740 | loss: 0.59275 - acc: 0.7080 -- iter: 891/891\n",
      "--\n",
      "Training Step: 741  | total loss: \u001b[1m\u001b[32m0.58713\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 741 | loss: 0.58713 - acc: 0.7164 -- iter: 891/891\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 742  | total loss: \u001b[1m\u001b[32m0.58247\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 742 | loss: 0.58247 - acc: 0.7237 -- iter: 891/891\n",
      "--\n",
      "Training Step: 743  | total loss: \u001b[1m\u001b[32m0.57792\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 743 | loss: 0.57792 - acc: 0.7305 -- iter: 891/891\n",
      "--\n",
      "Training Step: 744  | total loss: \u001b[1m\u001b[32m0.59030\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 744 | loss: 0.59030 - acc: 0.7142 -- iter: 891/891\n",
      "--\n",
      "Training Step: 745  | total loss: \u001b[1m\u001b[32m0.58358\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 745 | loss: 0.58358 - acc: 0.7222 -- iter: 891/891\n",
      "--\n",
      "Training Step: 746  | total loss: \u001b[1m\u001b[32m0.57664\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 746 | loss: 0.57664 - acc: 0.7297 -- iter: 891/891\n",
      "--\n",
      "Training Step: 747  | total loss: \u001b[1m\u001b[32m0.56926\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 747 | loss: 0.56926 - acc: 0.7366 -- iter: 891/891\n",
      "--\n",
      "Training Step: 748  | total loss: \u001b[1m\u001b[32m0.56144\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 748 | loss: 0.56144 - acc: 0.7426 -- iter: 891/891\n",
      "--\n",
      "Training Step: 749  | total loss: \u001b[1m\u001b[32m0.55339\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 749 | loss: 0.55339 - acc: 0.7481 -- iter: 891/891\n",
      "--\n",
      "Training Step: 750  | total loss: \u001b[1m\u001b[32m0.54540\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 750 | loss: 0.54540 - acc: 0.7536 -- iter: 891/891\n",
      "--\n",
      "Training Step: 751  | total loss: \u001b[1m\u001b[32m0.53783\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 751 | loss: 0.53783 - acc: 0.7586 -- iter: 891/891\n",
      "--\n",
      "Training Step: 752  | total loss: \u001b[1m\u001b[32m0.53094\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 752 | loss: 0.53094 - acc: 0.7630 -- iter: 891/891\n",
      "--\n",
      "Training Step: 753  | total loss: \u001b[1m\u001b[32m0.52491\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 753 | loss: 0.52491 - acc: 0.7662 -- iter: 891/891\n",
      "--\n",
      "Training Step: 754  | total loss: \u001b[1m\u001b[32m0.51968\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 754 | loss: 0.51968 - acc: 0.7695 -- iter: 891/891\n",
      "--\n",
      "Training Step: 755  | total loss: \u001b[1m\u001b[32m0.51503\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 755 | loss: 0.51503 - acc: 0.7724 -- iter: 891/891\n",
      "--\n",
      "Training Step: 756  | total loss: \u001b[1m\u001b[32m0.51059\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 756 | loss: 0.51059 - acc: 0.7757 -- iter: 891/891\n",
      "--\n",
      "Training Step: 757  | total loss: \u001b[1m\u001b[32m0.50602\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 757 | loss: 0.50602 - acc: 0.7786 -- iter: 891/891\n",
      "--\n",
      "Training Step: 758  | total loss: \u001b[1m\u001b[32m0.50113\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 758 | loss: 0.50113 - acc: 0.7812 -- iter: 891/891\n",
      "--\n",
      "Training Step: 759  | total loss: \u001b[1m\u001b[32m0.49597\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 759 | loss: 0.49597 - acc: 0.7842 -- iter: 891/891\n",
      "--\n",
      "Training Step: 760  | total loss: \u001b[1m\u001b[32m0.54452\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 760 | loss: 0.54452 - acc: 0.7572 -- iter: 891/891\n",
      "--\n",
      "Training Step: 761  | total loss: \u001b[1m\u001b[32m0.53495\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 761 | loss: 0.53495 - acc: 0.7625 -- iter: 891/891\n",
      "--\n",
      "Training Step: 762  | total loss: \u001b[1m\u001b[32m0.52784\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 762 | loss: 0.52784 - acc: 0.7667 -- iter: 891/891\n",
      "--\n",
      "Training Step: 763  | total loss: \u001b[1m\u001b[32m0.52253\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 763 | loss: 0.52253 - acc: 0.7705 -- iter: 891/891\n",
      "--\n",
      "Training Step: 764  | total loss: \u001b[1m\u001b[32m0.51773\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 764 | loss: 0.51773 - acc: 0.7738 -- iter: 891/891\n",
      "--\n",
      "Training Step: 765  | total loss: \u001b[1m\u001b[32m0.51241\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 765 | loss: 0.51241 - acc: 0.7771 -- iter: 891/891\n",
      "--\n",
      "Training Step: 766  | total loss: \u001b[1m\u001b[32m0.50643\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 766 | loss: 0.50643 - acc: 0.7800 -- iter: 891/891\n",
      "--\n",
      "Training Step: 767  | total loss: \u001b[1m\u001b[32m0.50035\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 767 | loss: 0.50035 - acc: 0.7834 -- iter: 891/891\n",
      "--\n",
      "Training Step: 768  | total loss: \u001b[1m\u001b[32m0.49481\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 768 | loss: 0.49481 - acc: 0.7862 -- iter: 891/891\n",
      "--\n",
      "Training Step: 769  | total loss: \u001b[1m\u001b[32m0.49010\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 769 | loss: 0.49010 - acc: 0.7888 -- iter: 891/891\n",
      "--\n",
      "Training Step: 770  | total loss: \u001b[1m\u001b[32m0.54142\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 770 | loss: 0.54142 - acc: 0.7636 -- iter: 891/891\n",
      "--\n",
      "Training Step: 771  | total loss: \u001b[1m\u001b[32m0.53237\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 771 | loss: 0.53237 - acc: 0.7673 -- iter: 891/891\n",
      "--\n",
      "Training Step: 772  | total loss: \u001b[1m\u001b[32m0.52414\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 772 | loss: 0.52414 - acc: 0.7703 -- iter: 891/891\n",
      "--\n",
      "Training Step: 773  | total loss: \u001b[1m\u001b[32m0.51657\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 773 | loss: 0.51657 - acc: 0.7738 -- iter: 891/891\n",
      "--\n",
      "Training Step: 774  | total loss: \u001b[1m\u001b[32m0.55190\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 774 | loss: 0.55190 - acc: 0.7522 -- iter: 891/891\n",
      "--\n",
      "Training Step: 775  | total loss: \u001b[1m\u001b[32m0.54162\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 775 | loss: 0.54162 - acc: 0.7582 -- iter: 891/891\n",
      "--\n",
      "Training Step: 776  | total loss: \u001b[1m\u001b[32m0.53294\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 776 | loss: 0.53294 - acc: 0.7633 -- iter: 891/891\n",
      "--\n",
      "Training Step: 777  | total loss: \u001b[1m\u001b[32m0.52572\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 777 | loss: 0.52572 - acc: 0.7680 -- iter: 891/891\n",
      "--\n",
      "Training Step: 778  | total loss: \u001b[1m\u001b[32m0.51952\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 778 | loss: 0.51952 - acc: 0.7721 -- iter: 891/891\n",
      "--\n",
      "Training Step: 779  | total loss: \u001b[1m\u001b[32m0.51381\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 779 | loss: 0.51381 - acc: 0.7757 -- iter: 891/891\n",
      "--\n",
      "Training Step: 780  | total loss: \u001b[1m\u001b[32m0.54706\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 780 | loss: 0.54706 - acc: 0.7516 -- iter: 891/891\n",
      "--\n",
      "Training Step: 781  | total loss: \u001b[1m\u001b[32m0.53851\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 781 | loss: 0.53851 - acc: 0.7573 -- iter: 891/891\n",
      "--\n",
      "Training Step: 782  | total loss: \u001b[1m\u001b[32m0.53082\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 782 | loss: 0.53082 - acc: 0.7626 -- iter: 891/891\n",
      "--\n",
      "Training Step: 783  | total loss: \u001b[1m\u001b[32m0.52357\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 783 | loss: 0.52357 - acc: 0.7676 -- iter: 891/891\n",
      "--\n",
      "Training Step: 784  | total loss: \u001b[1m\u001b[32m0.55295\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 784 | loss: 0.55295 - acc: 0.7461 -- iter: 891/891\n",
      "--\n",
      "Training Step: 785  | total loss: \u001b[1m\u001b[32m0.54332\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 785 | loss: 0.54332 - acc: 0.7525 -- iter: 891/891\n",
      "--\n",
      "Training Step: 786  | total loss: \u001b[1m\u001b[32m0.56902\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 786 | loss: 0.56902 - acc: 0.7325 -- iter: 891/891\n",
      "--\n",
      "Training Step: 787  | total loss: \u001b[1m\u001b[32m0.55867\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 787 | loss: 0.55867 - acc: 0.7401 -- iter: 891/891\n",
      "--\n",
      "Training Step: 788  | total loss: \u001b[1m\u001b[32m0.54983\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 788 | loss: 0.54983 - acc: 0.7468 -- iter: 891/891\n",
      "--\n",
      "Training Step: 789  | total loss: \u001b[1m\u001b[32m0.54191\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 789 | loss: 0.54191 - acc: 0.7530 -- iter: 891/891\n",
      "--\n",
      "Training Step: 790  | total loss: \u001b[1m\u001b[32m0.56763\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 790 | loss: 0.56763 - acc: 0.7304 -- iter: 891/891\n",
      "--\n",
      "Training Step: 791  | total loss: \u001b[1m\u001b[32m0.55813\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 791 | loss: 0.55813 - acc: 0.7384 -- iter: 891/891\n",
      "--\n",
      "Training Step: 792  | total loss: \u001b[1m\u001b[32m0.54967\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 792 | loss: 0.54967 - acc: 0.7456 -- iter: 891/891\n",
      "--\n",
      "Training Step: 793  | total loss: \u001b[1m\u001b[32m0.54177\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 793 | loss: 0.54177 - acc: 0.7522 -- iter: 891/891\n",
      "--\n",
      "Training Step: 794  | total loss: \u001b[1m\u001b[32m0.53411\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 794 | loss: 0.53411 - acc: 0.7580 -- iter: 891/891\n",
      "--\n",
      "Training Step: 795  | total loss: \u001b[1m\u001b[32m0.52661\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 795 | loss: 0.52661 - acc: 0.7633 -- iter: 891/891\n",
      "--\n",
      "Training Step: 796  | total loss: \u001b[1m\u001b[32m0.51934\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 796 | loss: 0.51934 - acc: 0.7678 -- iter: 891/891\n",
      "--\n",
      "Training Step: 797  | total loss: \u001b[1m\u001b[32m0.51246\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 797 | loss: 0.51246 - acc: 0.7718 -- iter: 891/891\n",
      "--\n",
      "Training Step: 798  | total loss: \u001b[1m\u001b[32m0.55988\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 798 | loss: 0.55988 - acc: 0.7449 -- iter: 891/891\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 799  | total loss: \u001b[1m\u001b[32m0.54874\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 799 | loss: 0.54874 - acc: 0.7515 -- iter: 891/891\n",
      "--\n",
      "Training Step: 800  | total loss: \u001b[1m\u001b[32m0.53861\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 800 | loss: 0.53861 - acc: 0.7572 -- iter: 891/891\n",
      "--\n",
      "Training Step: 801  | total loss: \u001b[1m\u001b[32m0.52936\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 801 | loss: 0.52936 - acc: 0.7624 -- iter: 891/891\n",
      "--\n",
      "Training Step: 802  | total loss: \u001b[1m\u001b[32m0.52092\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 802 | loss: 0.52092 - acc: 0.7670 -- iter: 891/891\n",
      "--\n",
      "Training Step: 803  | total loss: \u001b[1m\u001b[32m0.51321\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 803 | loss: 0.51321 - acc: 0.7715 -- iter: 891/891\n",
      "--\n",
      "Training Step: 804  | total loss: \u001b[1m\u001b[32m0.50622\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 804 | loss: 0.50622 - acc: 0.7755 -- iter: 891/891\n",
      "--\n",
      "Training Step: 805  | total loss: \u001b[1m\u001b[32m0.49991\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 805 | loss: 0.49991 - acc: 0.7791 -- iter: 891/891\n",
      "--\n",
      "Training Step: 806  | total loss: \u001b[1m\u001b[32m0.54794\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 806 | loss: 0.54794 - acc: 0.7524 -- iter: 891/891\n",
      "--\n",
      "Training Step: 807  | total loss: \u001b[1m\u001b[32m0.53785\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 807 | loss: 0.53785 - acc: 0.7573 -- iter: 891/891\n",
      "--\n",
      "Training Step: 808  | total loss: \u001b[1m\u001b[32m0.52921\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 808 | loss: 0.52921 - acc: 0.7617 -- iter: 891/891\n",
      "--\n",
      "Training Step: 809  | total loss: \u001b[1m\u001b[32m0.52159\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 809 | loss: 0.52159 - acc: 0.7658 -- iter: 891/891\n",
      "--\n",
      "Training Step: 810  | total loss: \u001b[1m\u001b[32m0.51457\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 810 | loss: 0.51457 - acc: 0.7694 -- iter: 891/891\n",
      "--\n",
      "Training Step: 811  | total loss: \u001b[1m\u001b[32m0.50789\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 811 | loss: 0.50789 - acc: 0.7730 -- iter: 891/891\n",
      "--\n",
      "Training Step: 812  | total loss: \u001b[1m\u001b[32m0.50158\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 812 | loss: 0.50158 - acc: 0.7763 -- iter: 891/891\n",
      "--\n",
      "Training Step: 813  | total loss: \u001b[1m\u001b[32m0.49577\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 813 | loss: 0.49577 - acc: 0.7799 -- iter: 891/891\n",
      "--\n",
      "Training Step: 814  | total loss: \u001b[1m\u001b[32m0.54050\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 814 | loss: 0.54050 - acc: 0.7565 -- iter: 891/891\n",
      "--\n",
      "Training Step: 815  | total loss: \u001b[1m\u001b[32m0.53089\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 815 | loss: 0.53089 - acc: 0.7615 -- iter: 891/891\n",
      "--\n",
      "Training Step: 816  | total loss: \u001b[1m\u001b[32m0.57041\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 816 | loss: 0.57041 - acc: 0.7388 -- iter: 891/891\n",
      "--\n",
      "Training Step: 817  | total loss: \u001b[1m\u001b[32m0.55810\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 817 | loss: 0.55810 - acc: 0.7455 -- iter: 891/891\n",
      "--\n",
      "Training Step: 818  | total loss: \u001b[1m\u001b[32m0.58613\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 818 | loss: 0.58613 - acc: 0.7266 -- iter: 891/891\n",
      "--\n",
      "Training Step: 819  | total loss: \u001b[1m\u001b[32m0.57374\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 819 | loss: 0.57374 - acc: 0.7346 -- iter: 891/891\n",
      "--\n",
      "Training Step: 820  | total loss: \u001b[1m\u001b[32m0.56361\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 820 | loss: 0.56361 - acc: 0.7419 -- iter: 891/891\n",
      "--\n",
      "Training Step: 821  | total loss: \u001b[1m\u001b[32m0.55504\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 821 | loss: 0.55504 - acc: 0.7484 -- iter: 891/891\n",
      "--\n",
      "Training Step: 822  | total loss: \u001b[1m\u001b[32m0.54732\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 822 | loss: 0.54732 - acc: 0.7541 -- iter: 891/891\n",
      "--\n",
      "Training Step: 823  | total loss: \u001b[1m\u001b[32m0.53992\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 823 | loss: 0.53992 - acc: 0.7592 -- iter: 891/891\n",
      "--\n",
      "Training Step: 824  | total loss: \u001b[1m\u001b[32m0.53256\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 824 | loss: 0.53256 - acc: 0.7642 -- iter: 891/891\n",
      "--\n",
      "Training Step: 825  | total loss: \u001b[1m\u001b[32m0.52523\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 825 | loss: 0.52523 - acc: 0.7690 -- iter: 891/891\n",
      "--\n",
      "Training Step: 826  | total loss: \u001b[1m\u001b[32m0.55836\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 826 | loss: 0.55836 - acc: 0.7449 -- iter: 891/891\n",
      "--\n",
      "Training Step: 827  | total loss: \u001b[1m\u001b[32m0.54787\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 827 | loss: 0.54787 - acc: 0.7511 -- iter: 891/891\n",
      "--\n",
      "Training Step: 828  | total loss: \u001b[1m\u001b[32m0.53834\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 828 | loss: 0.53834 - acc: 0.7570 -- iter: 891/891\n",
      "--\n",
      "Training Step: 829  | total loss: \u001b[1m\u001b[32m0.52963\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 829 | loss: 0.52963 - acc: 0.7620 -- iter: 891/891\n",
      "--\n",
      "Training Step: 830  | total loss: \u001b[1m\u001b[32m0.52165\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 830 | loss: 0.52165 - acc: 0.7667 -- iter: 891/891\n",
      "--\n",
      "Training Step: 831  | total loss: \u001b[1m\u001b[32m0.51431\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 831 | loss: 0.51431 - acc: 0.7707 -- iter: 891/891\n",
      "--\n",
      "Training Step: 832  | total loss: \u001b[1m\u001b[32m0.50757\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 832 | loss: 0.50757 - acc: 0.7743 -- iter: 891/891\n",
      "--\n",
      "Training Step: 833  | total loss: \u001b[1m\u001b[32m0.50136\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 833 | loss: 0.50136 - acc: 0.7775 -- iter: 891/891\n",
      "--\n",
      "Training Step: 834  | total loss: \u001b[1m\u001b[32m0.49562\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 834 | loss: 0.49562 - acc: 0.7806 -- iter: 891/891\n",
      "--\n",
      "Training Step: 835  | total loss: \u001b[1m\u001b[32m0.49035\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 835 | loss: 0.49035 - acc: 0.7832 -- iter: 891/891\n",
      "--\n",
      "Training Step: 836  | total loss: \u001b[1m\u001b[32m0.48552\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 836 | loss: 0.48552 - acc: 0.7859 -- iter: 891/891\n",
      "--\n",
      "Training Step: 837  | total loss: \u001b[1m\u001b[32m0.48115\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 837 | loss: 0.48115 - acc: 0.7881 -- iter: 891/891\n",
      "--\n",
      "Training Step: 838  | total loss: \u001b[1m\u001b[32m0.53188\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 838 | loss: 0.53188 - acc: 0.7624 -- iter: 891/891\n",
      "--\n",
      "Training Step: 839  | total loss: \u001b[1m\u001b[32m0.52318\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 839 | loss: 0.52318 - acc: 0.7665 -- iter: 891/891\n",
      "--\n",
      "Training Step: 840  | total loss: \u001b[1m\u001b[32m0.51577\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 840 | loss: 0.51577 - acc: 0.7701 -- iter: 891/891\n",
      "--\n",
      "Training Step: 841  | total loss: \u001b[1m\u001b[32m0.50926\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 841 | loss: 0.50926 - acc: 0.7735 -- iter: 891/891\n",
      "--\n",
      "Training Step: 842  | total loss: \u001b[1m\u001b[32m0.54877\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 842 | loss: 0.54877 - acc: 0.7500 -- iter: 891/891\n",
      "--\n",
      "Training Step: 843  | total loss: \u001b[1m\u001b[32m0.53921\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 843 | loss: 0.53921 - acc: 0.7552 -- iter: 891/891\n",
      "--\n",
      "Training Step: 844  | total loss: \u001b[1m\u001b[32m0.53075\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 844 | loss: 0.53075 - acc: 0.7597 -- iter: 891/891\n",
      "--\n",
      "Training Step: 845  | total loss: \u001b[1m\u001b[32m0.52300\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 845 | loss: 0.52300 - acc: 0.7645 -- iter: 891/891\n",
      "--\n",
      "Training Step: 846  | total loss: \u001b[1m\u001b[32m0.51580\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 846 | loss: 0.51580 - acc: 0.7688 -- iter: 891/891\n",
      "--\n",
      "Training Step: 847  | total loss: \u001b[1m\u001b[32m0.50914\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 847 | loss: 0.50914 - acc: 0.7731 -- iter: 891/891\n",
      "--\n",
      "Training Step: 848  | total loss: \u001b[1m\u001b[32m0.50308\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 848 | loss: 0.50308 - acc: 0.7766 -- iter: 891/891\n",
      "--\n",
      "Training Step: 849  | total loss: \u001b[1m\u001b[32m0.49767\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 849 | loss: 0.49767 - acc: 0.7798 -- iter: 891/891\n",
      "--\n",
      "Training Step: 850  | total loss: \u001b[1m\u001b[32m0.49289\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 850 | loss: 0.49289 - acc: 0.7829 -- iter: 891/891\n",
      "--\n",
      "Training Step: 851  | total loss: \u001b[1m\u001b[32m0.48867\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 851 | loss: 0.48867 - acc: 0.7854 -- iter: 891/891\n",
      "--\n",
      "Training Step: 852  | total loss: \u001b[1m\u001b[32m0.48491\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 852 | loss: 0.48491 - acc: 0.7877 -- iter: 891/891\n",
      "--\n",
      "Training Step: 853  | total loss: \u001b[1m\u001b[32m0.48151\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 853 | loss: 0.48151 - acc: 0.7894 -- iter: 891/891\n",
      "--\n",
      "Training Step: 854  | total loss: \u001b[1m\u001b[32m0.47832\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 854 | loss: 0.47832 - acc: 0.7907 -- iter: 891/891\n",
      "--\n",
      "Training Step: 855  | total loss: \u001b[1m\u001b[32m0.47526\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 855 | loss: 0.47526 - acc: 0.7919 -- iter: 891/891\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 856  | total loss: \u001b[1m\u001b[32m0.47227\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 856 | loss: 0.47227 - acc: 0.7934 -- iter: 891/891\n",
      "--\n",
      "Training Step: 857  | total loss: \u001b[1m\u001b[32m0.46937\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 857 | loss: 0.46937 - acc: 0.7947 -- iter: 891/891\n",
      "--\n",
      "Training Step: 858  | total loss: \u001b[1m\u001b[32m0.51957\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 858 | loss: 0.51957 - acc: 0.7671 -- iter: 891/891\n",
      "--\n",
      "Training Step: 859  | total loss: \u001b[1m\u001b[32m0.51201\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 859 | loss: 0.51201 - acc: 0.7709 -- iter: 891/891\n",
      "--\n",
      "Training Step: 860  | total loss: \u001b[1m\u001b[32m0.50579\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 860 | loss: 0.50579 - acc: 0.7741 -- iter: 891/891\n",
      "--\n",
      "Training Step: 861  | total loss: \u001b[1m\u001b[32m0.50070\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 861 | loss: 0.50070 - acc: 0.7769 -- iter: 891/891\n",
      "--\n",
      "Training Step: 862  | total loss: \u001b[1m\u001b[32m0.53690\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 862 | loss: 0.53690 - acc: 0.7531 -- iter: 891/891\n",
      "--\n",
      "Training Step: 863  | total loss: \u001b[1m\u001b[32m0.52978\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 863 | loss: 0.52978 - acc: 0.7579 -- iter: 891/891\n",
      "--\n",
      "Training Step: 864  | total loss: \u001b[1m\u001b[32m0.52374\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 864 | loss: 0.52374 - acc: 0.7620 -- iter: 891/891\n",
      "--\n",
      "Training Step: 865  | total loss: \u001b[1m\u001b[32m0.51802\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 865 | loss: 0.51802 - acc: 0.7659 -- iter: 891/891\n",
      "--\n",
      "Training Step: 866  | total loss: \u001b[1m\u001b[32m0.51224\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 866 | loss: 0.51224 - acc: 0.7705 -- iter: 891/891\n",
      "--\n",
      "Training Step: 867  | total loss: \u001b[1m\u001b[32m0.50640\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 867 | loss: 0.50640 - acc: 0.7747 -- iter: 891/891\n",
      "--\n",
      "Training Step: 868  | total loss: \u001b[1m\u001b[32m0.50075\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 868 | loss: 0.50075 - acc: 0.7785 -- iter: 891/891\n",
      "--\n",
      "Training Step: 869  | total loss: \u001b[1m\u001b[32m0.49556\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 869 | loss: 0.49556 - acc: 0.7814 -- iter: 891/891\n",
      "--\n",
      "Training Step: 870  | total loss: \u001b[1m\u001b[32m0.54333\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 870 | loss: 0.54333 - acc: 0.7550 -- iter: 891/891\n",
      "--\n",
      "Training Step: 871  | total loss: \u001b[1m\u001b[32m0.53411\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 871 | loss: 0.53411 - acc: 0.7603 -- iter: 891/891\n",
      "--\n",
      "Training Step: 872  | total loss: \u001b[1m\u001b[32m0.56828\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 872 | loss: 0.56828 - acc: 0.7411 -- iter: 891/891\n",
      "--\n",
      "Training Step: 873  | total loss: \u001b[1m\u001b[32m0.55667\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 873 | loss: 0.55667 - acc: 0.7475 -- iter: 891/891\n",
      "--\n",
      "Training Step: 874  | total loss: \u001b[1m\u001b[32m0.58936\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 874 | loss: 0.58936 - acc: 0.7260 -- iter: 891/891\n",
      "--\n",
      "Training Step: 875  | total loss: \u001b[1m\u001b[32m0.57621\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 875 | loss: 0.57621 - acc: 0.7342 -- iter: 891/891\n",
      "--\n",
      "Training Step: 876  | total loss: \u001b[1m\u001b[32m0.59716\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 876 | loss: 0.59716 - acc: 0.7157 -- iter: 891/891\n",
      "--\n",
      "Training Step: 877  | total loss: \u001b[1m\u001b[32m0.58538\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 877 | loss: 0.58538 - acc: 0.7249 -- iter: 891/891\n",
      "--\n",
      "Training Step: 878  | total loss: \u001b[1m\u001b[32m0.57603\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 878 | loss: 0.57603 - acc: 0.7332 -- iter: 891/891\n",
      "--\n",
      "Training Step: 879  | total loss: \u001b[1m\u001b[32m0.56828\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 879 | loss: 0.56828 - acc: 0.7403 -- iter: 891/891\n",
      "--\n",
      "Training Step: 880  | total loss: \u001b[1m\u001b[32m0.58473\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 880 | loss: 0.58473 - acc: 0.7228 -- iter: 891/891\n",
      "--\n",
      "Training Step: 881  | total loss: \u001b[1m\u001b[32m0.57665\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 881 | loss: 0.57665 - acc: 0.7312 -- iter: 891/891\n",
      "--\n",
      "Training Step: 882  | total loss: \u001b[1m\u001b[32m0.59182\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 882 | loss: 0.59182 - acc: 0.7133 -- iter: 891/891\n",
      "--\n",
      "Training Step: 883  | total loss: \u001b[1m\u001b[32m0.58329\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 883 | loss: 0.58329 - acc: 0.7229 -- iter: 891/891\n",
      "--\n",
      "Training Step: 884  | total loss: \u001b[1m\u001b[32m0.57540\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 884 | loss: 0.57540 - acc: 0.7315 -- iter: 891/891\n",
      "--\n",
      "Training Step: 885  | total loss: \u001b[1m\u001b[32m0.56766\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 885 | loss: 0.56766 - acc: 0.7394 -- iter: 891/891\n",
      "--\n",
      "Training Step: 886  | total loss: \u001b[1m\u001b[32m0.55979\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 886 | loss: 0.55979 - acc: 0.7465 -- iter: 891/891\n",
      "--\n",
      "Training Step: 887  | total loss: \u001b[1m\u001b[32m0.55172\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 887 | loss: 0.55172 - acc: 0.7529 -- iter: 891/891\n",
      "--\n",
      "Training Step: 888  | total loss: \u001b[1m\u001b[32m0.54358\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 888 | loss: 0.54358 - acc: 0.7583 -- iter: 891/891\n",
      "--\n",
      "Training Step: 889  | total loss: \u001b[1m\u001b[32m0.53558\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 889 | loss: 0.53558 - acc: 0.7628 -- iter: 891/891\n",
      "--\n",
      "Training Step: 890  | total loss: \u001b[1m\u001b[32m0.52801\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 890 | loss: 0.52801 - acc: 0.7677 -- iter: 891/891\n",
      "--\n",
      "Training Step: 891  | total loss: \u001b[1m\u001b[32m0.52114\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 891 | loss: 0.52114 - acc: 0.7715 -- iter: 891/891\n",
      "--\n",
      "Training Step: 892  | total loss: \u001b[1m\u001b[32m0.56666\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 892 | loss: 0.56666 - acc: 0.7489 -- iter: 891/891\n",
      "--\n",
      "Training Step: 893  | total loss: \u001b[1m\u001b[32m0.55605\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 893 | loss: 0.55605 - acc: 0.7535 -- iter: 891/891\n",
      "--\n",
      "Training Step: 894  | total loss: \u001b[1m\u001b[32m0.60405\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 894 | loss: 0.60405 - acc: 0.7299 -- iter: 891/891\n",
      "--\n",
      "Training Step: 895  | total loss: \u001b[1m\u001b[32m0.58902\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 895 | loss: 0.58902 - acc: 0.7370 -- iter: 891/891\n",
      "--\n",
      "Training Step: 896  | total loss: \u001b[1m\u001b[32m0.62593\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 896 | loss: 0.62593 - acc: 0.7145 -- iter: 891/891\n",
      "--\n",
      "Training Step: 897  | total loss: \u001b[1m\u001b[32m0.60847\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 897 | loss: 0.60847 - acc: 0.7246 -- iter: 891/891\n",
      "--\n",
      "Training Step: 898  | total loss: \u001b[1m\u001b[32m0.59336\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 898 | loss: 0.59336 - acc: 0.7338 -- iter: 891/891\n",
      "--\n",
      "Training Step: 899  | total loss: \u001b[1m\u001b[32m0.58051\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 899 | loss: 0.58051 - acc: 0.7414 -- iter: 891/891\n",
      "--\n",
      "Training Step: 900  | total loss: \u001b[1m\u001b[32m0.56939\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 900 | loss: 0.56939 - acc: 0.7486 -- iter: 891/891\n",
      "--\n",
      "Training Step: 901  | total loss: \u001b[1m\u001b[32m0.55944\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 901 | loss: 0.55944 - acc: 0.7549 -- iter: 891/891\n",
      "--\n",
      "Training Step: 902  | total loss: \u001b[1m\u001b[32m0.55017\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 902 | loss: 0.55017 - acc: 0.7602 -- iter: 891/891\n",
      "--\n",
      "Training Step: 903  | total loss: \u001b[1m\u001b[32m0.54122\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 903 | loss: 0.54122 - acc: 0.7648 -- iter: 891/891\n",
      "--\n",
      "Training Step: 904  | total loss: \u001b[1m\u001b[32m0.57050\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 904 | loss: 0.57050 - acc: 0.7430 -- iter: 891/891\n",
      "--\n",
      "Training Step: 905  | total loss: \u001b[1m\u001b[32m0.55879\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 905 | loss: 0.55879 - acc: 0.7499 -- iter: 891/891\n",
      "--\n",
      "Training Step: 906  | total loss: \u001b[1m\u001b[32m0.54816\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 906 | loss: 0.54816 - acc: 0.7559 -- iter: 891/891\n",
      "--\n",
      "Training Step: 907  | total loss: \u001b[1m\u001b[32m0.53844\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 907 | loss: 0.53844 - acc: 0.7615 -- iter: 891/891\n",
      "--\n",
      "Training Step: 908  | total loss: \u001b[1m\u001b[32m0.52950\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 908 | loss: 0.52950 - acc: 0.7670 -- iter: 891/891\n",
      "--\n",
      "Training Step: 909  | total loss: \u001b[1m\u001b[32m0.52128\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 909 | loss: 0.52128 - acc: 0.7715 -- iter: 891/891\n",
      "--\n",
      "Training Step: 910  | total loss: \u001b[1m\u001b[32m0.55972\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 910 | loss: 0.55972 - acc: 0.7495 -- iter: 891/891\n",
      "--\n",
      "Training Step: 911  | total loss: \u001b[1m\u001b[32m0.54849\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 911 | loss: 0.54849 - acc: 0.7550 -- iter: 891/891\n",
      "--\n",
      "Training Step: 912  | total loss: \u001b[1m\u001b[32m0.58477\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 912 | loss: 0.58477 - acc: 0.7323 -- iter: 891/891\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 913  | total loss: \u001b[1m\u001b[32m0.57147\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 913 | loss: 0.57147 - acc: 0.7396 -- iter: 891/891\n",
      "--\n",
      "Training Step: 914  | total loss: \u001b[1m\u001b[32m0.55991\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 914 | loss: 0.55991 - acc: 0.7470 -- iter: 891/891\n",
      "--\n",
      "Training Step: 915  | total loss: \u001b[1m\u001b[32m0.54987\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 915 | loss: 0.54987 - acc: 0.7531 -- iter: 891/891\n",
      "--\n",
      "Training Step: 916  | total loss: \u001b[1m\u001b[32m0.54095\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 916 | loss: 0.54095 - acc: 0.7586 -- iter: 891/891\n",
      "--\n",
      "Training Step: 917  | total loss: \u001b[1m\u001b[32m0.53272\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 917 | loss: 0.53272 - acc: 0.7637 -- iter: 891/891\n",
      "--\n",
      "Training Step: 918  | total loss: \u001b[1m\u001b[32m0.52494\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 918 | loss: 0.52494 - acc: 0.7685 -- iter: 891/891\n",
      "--\n",
      "Training Step: 919  | total loss: \u001b[1m\u001b[32m0.51757\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 919 | loss: 0.51757 - acc: 0.7726 -- iter: 891/891\n",
      "--\n",
      "Training Step: 920  | total loss: \u001b[1m\u001b[32m0.55555\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 920 | loss: 0.55555 - acc: 0.7487 -- iter: 891/891\n",
      "--\n",
      "Training Step: 921  | total loss: \u001b[1m\u001b[32m0.54484\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 921 | loss: 0.54484 - acc: 0.7543 -- iter: 891/891\n",
      "--\n",
      "Training Step: 922  | total loss: \u001b[1m\u001b[32m0.53513\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 922 | loss: 0.53513 - acc: 0.7594 -- iter: 891/891\n",
      "--\n",
      "Training Step: 923  | total loss: \u001b[1m\u001b[32m0.52630\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 923 | loss: 0.52630 - acc: 0.7646 -- iter: 891/891\n",
      "--\n",
      "Training Step: 924  | total loss: \u001b[1m\u001b[32m0.51828\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 924 | loss: 0.51828 - acc: 0.7694 -- iter: 891/891\n",
      "--\n",
      "Training Step: 925  | total loss: \u001b[1m\u001b[32m0.51094\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 925 | loss: 0.51094 - acc: 0.7738 -- iter: 891/891\n",
      "--\n",
      "Training Step: 926  | total loss: \u001b[1m\u001b[32m0.50421\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 926 | loss: 0.50421 - acc: 0.7770 -- iter: 891/891\n",
      "--\n",
      "Training Step: 927  | total loss: \u001b[1m\u001b[32m0.49809\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 927 | loss: 0.49809 - acc: 0.7794 -- iter: 891/891\n",
      "--\n",
      "Training Step: 928  | total loss: \u001b[1m\u001b[32m0.49253\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 928 | loss: 0.49253 - acc: 0.7814 -- iter: 891/891\n",
      "--\n",
      "Training Step: 929  | total loss: \u001b[1m\u001b[32m0.48749\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 929 | loss: 0.48749 - acc: 0.7833 -- iter: 891/891\n",
      "--\n",
      "Training Step: 930  | total loss: \u001b[1m\u001b[32m0.53640\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 930 | loss: 0.53640 - acc: 0.7568 -- iter: 891/891\n",
      "--\n",
      "Training Step: 931  | total loss: \u001b[1m\u001b[32m0.52697\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 931 | loss: 0.52697 - acc: 0.7618 -- iter: 891/891\n",
      "--\n",
      "Training Step: 932  | total loss: \u001b[1m\u001b[32m0.51863\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 932 | loss: 0.51863 - acc: 0.7661 -- iter: 891/891\n",
      "--\n",
      "Training Step: 933  | total loss: \u001b[1m\u001b[32m0.51124\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 933 | loss: 0.51124 - acc: 0.7694 -- iter: 891/891\n",
      "--\n",
      "Training Step: 934  | total loss: \u001b[1m\u001b[32m0.55099\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 934 | loss: 0.55099 - acc: 0.7473 -- iter: 891/891\n",
      "--\n",
      "Training Step: 935  | total loss: \u001b[1m\u001b[32m0.54096\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 935 | loss: 0.54096 - acc: 0.7530 -- iter: 891/891\n",
      "--\n",
      "Training Step: 936  | total loss: \u001b[1m\u001b[32m0.53222\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 936 | loss: 0.53222 - acc: 0.7578 -- iter: 891/891\n",
      "--\n",
      "Training Step: 937  | total loss: \u001b[1m\u001b[32m0.52429\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 937 | loss: 0.52429 - acc: 0.7623 -- iter: 891/891\n",
      "--\n",
      "Training Step: 938  | total loss: \u001b[1m\u001b[32m0.55496\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 938 | loss: 0.55496 - acc: 0.7420 -- iter: 891/891\n",
      "--\n",
      "Training Step: 939  | total loss: \u001b[1m\u001b[32m0.54496\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 939 | loss: 0.54496 - acc: 0.7486 -- iter: 891/891\n",
      "--\n",
      "Training Step: 940  | total loss: \u001b[1m\u001b[32m0.53621\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 940 | loss: 0.53621 - acc: 0.7544 -- iter: 891/891\n",
      "--\n",
      "Training Step: 941  | total loss: \u001b[1m\u001b[32m0.52839\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 941 | loss: 0.52839 - acc: 0.7596 -- iter: 891/891\n",
      "--\n",
      "Training Step: 942  | total loss: \u001b[1m\u001b[32m0.52122\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 942 | loss: 0.52122 - acc: 0.7649 -- iter: 891/891\n",
      "--\n",
      "Training Step: 943  | total loss: \u001b[1m\u001b[32m0.51458\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 943 | loss: 0.51458 - acc: 0.7696 -- iter: 891/891\n",
      "--\n",
      "Training Step: 944  | total loss: \u001b[1m\u001b[32m0.54598\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 944 | loss: 0.54598 - acc: 0.7494 -- iter: 891/891\n",
      "--\n",
      "Training Step: 945  | total loss: \u001b[1m\u001b[32m0.53696\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 945 | loss: 0.53696 - acc: 0.7555 -- iter: 891/891\n",
      "--\n",
      "Training Step: 946  | total loss: \u001b[1m\u001b[32m0.52895\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 946 | loss: 0.52895 - acc: 0.7608 -- iter: 891/891\n",
      "--\n",
      "Training Step: 947  | total loss: \u001b[1m\u001b[32m0.52175\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 947 | loss: 0.52175 - acc: 0.7656 -- iter: 891/891\n",
      "--\n",
      "Training Step: 948  | total loss: \u001b[1m\u001b[32m0.51518\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 948 | loss: 0.51518 - acc: 0.7700 -- iter: 891/891\n",
      "--\n",
      "Training Step: 949  | total loss: \u001b[1m\u001b[32m0.50912\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 949 | loss: 0.50912 - acc: 0.7737 -- iter: 891/891\n",
      "--\n",
      "Training Step: 950  | total loss: \u001b[1m\u001b[32m0.50346\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 950 | loss: 0.50346 - acc: 0.7770 -- iter: 891/891\n",
      "--\n",
      "Training Step: 951  | total loss: \u001b[1m\u001b[32m0.49814\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 951 | loss: 0.49814 - acc: 0.7800 -- iter: 891/891\n",
      "--\n",
      "Training Step: 952  | total loss: \u001b[1m\u001b[32m0.53936\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 952 | loss: 0.53936 - acc: 0.7538 -- iter: 891/891\n",
      "--\n",
      "Training Step: 953  | total loss: \u001b[1m\u001b[32m0.53019\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 953 | loss: 0.53019 - acc: 0.7594 -- iter: 891/891\n",
      "--\n",
      "Training Step: 954  | total loss: \u001b[1m\u001b[32m0.52191\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 954 | loss: 0.52191 - acc: 0.7645 -- iter: 891/891\n",
      "--\n",
      "Training Step: 955  | total loss: \u001b[1m\u001b[32m0.51445\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 955 | loss: 0.51445 - acc: 0.7693 -- iter: 891/891\n",
      "--\n",
      "Training Step: 956  | total loss: \u001b[1m\u001b[32m0.54923\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 956 | loss: 0.54923 - acc: 0.7464 -- iter: 891/891\n",
      "--\n",
      "Training Step: 957  | total loss: \u001b[1m\u001b[32m0.53951\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 957 | loss: 0.53951 - acc: 0.7522 -- iter: 891/891\n",
      "--\n",
      "Training Step: 958  | total loss: \u001b[1m\u001b[32m0.53119\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 958 | loss: 0.53119 - acc: 0.7572 -- iter: 891/891\n",
      "--\n",
      "Training Step: 959  | total loss: \u001b[1m\u001b[32m0.52381\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 959 | loss: 0.52381 - acc: 0.7616 -- iter: 891/891\n",
      "--\n",
      "Training Step: 960  | total loss: \u001b[1m\u001b[32m0.55496\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 960 | loss: 0.55496 - acc: 0.7397 -- iter: 891/891\n",
      "--\n",
      "Training Step: 961  | total loss: \u001b[1m\u001b[32m0.54546\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 961 | loss: 0.54546 - acc: 0.7457 -- iter: 891/891\n",
      "--\n",
      "Training Step: 962  | total loss: \u001b[1m\u001b[32m0.56900\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 962 | loss: 0.56900 - acc: 0.7283 -- iter: 891/891\n",
      "--\n",
      "Training Step: 963  | total loss: \u001b[1m\u001b[32m0.55906\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 963 | loss: 0.55906 - acc: 0.7357 -- iter: 891/891\n",
      "--\n",
      "Training Step: 964  | total loss: \u001b[1m\u001b[32m0.55047\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 964 | loss: 0.55047 - acc: 0.7423 -- iter: 891/891\n",
      "--\n",
      "Training Step: 965  | total loss: \u001b[1m\u001b[32m0.54259\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 965 | loss: 0.54259 - acc: 0.7486 -- iter: 891/891\n",
      "--\n",
      "Training Step: 966  | total loss: \u001b[1m\u001b[32m0.53501\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 966 | loss: 0.53501 - acc: 0.7549 -- iter: 891/891\n",
      "--\n",
      "Training Step: 967  | total loss: \u001b[1m\u001b[32m0.52758\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 967 | loss: 0.52758 - acc: 0.7607 -- iter: 891/891\n",
      "--\n",
      "Training Step: 968  | total loss: \u001b[1m\u001b[32m0.52040\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 968 | loss: 0.52040 - acc: 0.7657 -- iter: 891/891\n",
      "--\n",
      "Training Step: 969  | total loss: \u001b[1m\u001b[32m0.51365\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 969 | loss: 0.51365 - acc: 0.7701 -- iter: 891/891\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 970  | total loss: \u001b[1m\u001b[32m0.50748\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 970 | loss: 0.50748 - acc: 0.7746 -- iter: 891/891\n",
      "--\n",
      "Training Step: 971  | total loss: \u001b[1m\u001b[32m0.50195\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 971 | loss: 0.50195 - acc: 0.7784 -- iter: 891/891\n",
      "--\n",
      "Training Step: 972  | total loss: \u001b[1m\u001b[32m0.49709\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 972 | loss: 0.49709 - acc: 0.7813 -- iter: 891/891\n",
      "--\n",
      "Training Step: 973  | total loss: \u001b[1m\u001b[32m0.49281\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 973 | loss: 0.49281 - acc: 0.7828 -- iter: 891/891\n",
      "--\n",
      "Training Step: 974  | total loss: \u001b[1m\u001b[32m0.48893\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 974 | loss: 0.48893 - acc: 0.7843 -- iter: 891/891\n",
      "--\n",
      "Training Step: 975  | total loss: \u001b[1m\u001b[32m0.48525\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 975 | loss: 0.48525 - acc: 0.7857 -- iter: 891/891\n",
      "--\n",
      "Training Step: 976  | total loss: \u001b[1m\u001b[32m0.48163\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 976 | loss: 0.48163 - acc: 0.7877 -- iter: 891/891\n",
      "--\n",
      "Training Step: 977  | total loss: \u001b[1m\u001b[32m0.47803\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 977 | loss: 0.47803 - acc: 0.7898 -- iter: 891/891\n",
      "--\n",
      "Training Step: 978  | total loss: \u001b[1m\u001b[32m0.47451\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 978 | loss: 0.47451 - acc: 0.7915 -- iter: 891/891\n",
      "--\n",
      "Training Step: 979  | total loss: \u001b[1m\u001b[32m0.47124\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 979 | loss: 0.47124 - acc: 0.7933 -- iter: 891/891\n",
      "--\n",
      "Training Step: 980  | total loss: \u001b[1m\u001b[32m0.46841\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 980 | loss: 0.46841 - acc: 0.7943 -- iter: 891/891\n",
      "--\n",
      "Training Step: 981  | total loss: \u001b[1m\u001b[32m0.46612\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 981 | loss: 0.46612 - acc: 0.7950 -- iter: 891/891\n",
      "--\n",
      "Training Step: 982  | total loss: \u001b[1m\u001b[32m0.51586\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 982 | loss: 0.51586 - acc: 0.7675 -- iter: 891/891\n",
      "--\n",
      "Training Step: 983  | total loss: \u001b[1m\u001b[32m0.50969\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 983 | loss: 0.50969 - acc: 0.7712 -- iter: 891/891\n",
      "--\n",
      "Training Step: 984  | total loss: \u001b[1m\u001b[32m0.50446\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 984 | loss: 0.50446 - acc: 0.7743 -- iter: 891/891\n",
      "--\n",
      "Training Step: 985  | total loss: \u001b[1m\u001b[32m0.49955\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 985 | loss: 0.49955 - acc: 0.7774 -- iter: 891/891\n",
      "--\n",
      "Training Step: 986  | total loss: \u001b[1m\u001b[32m0.49463\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 986 | loss: 0.49463 - acc: 0.7801 -- iter: 891/891\n",
      "--\n",
      "Training Step: 987  | total loss: \u001b[1m\u001b[32m0.48975\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 987 | loss: 0.48975 - acc: 0.7825 -- iter: 891/891\n",
      "--\n",
      "Training Step: 988  | total loss: \u001b[1m\u001b[32m0.48516\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 988 | loss: 0.48516 - acc: 0.7850 -- iter: 891/891\n",
      "--\n",
      "Training Step: 989  | total loss: \u001b[1m\u001b[32m0.48108\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 989 | loss: 0.48108 - acc: 0.7872 -- iter: 891/891\n",
      "--\n",
      "Training Step: 990  | total loss: \u001b[1m\u001b[32m0.47759\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 990 | loss: 0.47759 - acc: 0.7893 -- iter: 891/891\n",
      "--\n",
      "Training Step: 991  | total loss: \u001b[1m\u001b[32m0.47464\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 991 | loss: 0.47464 - acc: 0.7911 -- iter: 891/891\n",
      "--\n",
      "Training Step: 992  | total loss: \u001b[1m\u001b[32m0.47211\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 992 | loss: 0.47211 - acc: 0.7924 -- iter: 891/891\n",
      "--\n",
      "Training Step: 993  | total loss: \u001b[1m\u001b[32m0.46987\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 993 | loss: 0.46987 - acc: 0.7932 -- iter: 891/891\n",
      "--\n",
      "Training Step: 994  | total loss: \u001b[1m\u001b[32m0.52006\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 994 | loss: 0.52006 - acc: 0.7690 -- iter: 891/891\n",
      "--\n",
      "Training Step: 995  | total loss: \u001b[1m\u001b[32m0.51271\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 995 | loss: 0.51271 - acc: 0.7721 -- iter: 891/891\n",
      "--\n",
      "Training Step: 996  | total loss: \u001b[1m\u001b[32m0.50596\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 996 | loss: 0.50596 - acc: 0.7754 -- iter: 891/891\n",
      "--\n",
      "Training Step: 997  | total loss: \u001b[1m\u001b[32m0.49992\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 997 | loss: 0.49992 - acc: 0.7787 -- iter: 891/891\n",
      "--\n",
      "Training Step: 998  | total loss: \u001b[1m\u001b[32m0.54174\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 998 | loss: 0.54174 - acc: 0.7515 -- iter: 891/891\n",
      "--\n",
      "Training Step: 999  | total loss: \u001b[1m\u001b[32m0.53321\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 999 | loss: 0.53321 - acc: 0.7576 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1000  | total loss: \u001b[1m\u001b[32m0.52631\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1000 | loss: 0.52631 - acc: 0.7628 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1001  | total loss: \u001b[1m\u001b[32m0.52051\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1001 | loss: 0.52051 - acc: 0.7669 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1002  | total loss: \u001b[1m\u001b[32m0.51521\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1002 | loss: 0.51521 - acc: 0.7704 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1003  | total loss: \u001b[1m\u001b[32m0.50989\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1003 | loss: 0.50989 - acc: 0.7736 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1004  | total loss: \u001b[1m\u001b[32m0.50434\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1004 | loss: 0.50434 - acc: 0.7765 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1005  | total loss: \u001b[1m\u001b[32m0.49875\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1005 | loss: 0.49875 - acc: 0.7802 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1006  | total loss: \u001b[1m\u001b[32m0.49348\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1006 | loss: 0.49348 - acc: 0.7832 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1007  | total loss: \u001b[1m\u001b[32m0.48872\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1007 | loss: 0.48872 - acc: 0.7858 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1008  | total loss: \u001b[1m\u001b[32m0.53627\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1008 | loss: 0.53627 - acc: 0.7607 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1009  | total loss: \u001b[1m\u001b[32m0.52738\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1009 | loss: 0.52738 - acc: 0.7644 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1010  | total loss: \u001b[1m\u001b[32m0.57418\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1010 | loss: 0.57418 - acc: 0.7394 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1011  | total loss: \u001b[1m\u001b[32m0.56148\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1011 | loss: 0.56148 - acc: 0.7449 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1012  | total loss: \u001b[1m\u001b[32m0.59540\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1012 | loss: 0.59540 - acc: 0.7229 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1013  | total loss: \u001b[1m\u001b[32m0.58106\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1013 | loss: 0.58106 - acc: 0.7318 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1014  | total loss: \u001b[1m\u001b[32m0.56909\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1014 | loss: 0.56909 - acc: 0.7397 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1015  | total loss: \u001b[1m\u001b[32m0.55898\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1015 | loss: 0.55898 - acc: 0.7467 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1016  | total loss: \u001b[1m\u001b[32m0.58390\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1016 | loss: 0.58390 - acc: 0.7254 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1017  | total loss: \u001b[1m\u001b[32m0.57334\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1017 | loss: 0.57334 - acc: 0.7331 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1018  | total loss: \u001b[1m\u001b[32m0.59288\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1018 | loss: 0.59288 - acc: 0.7134 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1019  | total loss: \u001b[1m\u001b[32m0.58280\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1019 | loss: 0.58280 - acc: 0.7224 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1020  | total loss: \u001b[1m\u001b[32m0.57397\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1020 | loss: 0.57397 - acc: 0.7308 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1021  | total loss: \u001b[1m\u001b[32m0.56574\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1021 | loss: 0.56574 - acc: 0.7387 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1022  | total loss: \u001b[1m\u001b[32m0.55775\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1022 | loss: 0.55775 - acc: 0.7460 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1023  | total loss: \u001b[1m\u001b[32m0.54980\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1023 | loss: 0.54980 - acc: 0.7527 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1024  | total loss: \u001b[1m\u001b[32m0.54187\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1024 | loss: 0.54187 - acc: 0.7587 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1025  | total loss: \u001b[1m\u001b[32m0.53409\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1025 | loss: 0.53409 - acc: 0.7634 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1026  | total loss: \u001b[1m\u001b[32m0.52668\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1026 | loss: 0.52668 - acc: 0.7682 -- iter: 891/891\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1027  | total loss: \u001b[1m\u001b[32m0.51989\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1027 | loss: 0.51989 - acc: 0.7724 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1028  | total loss: \u001b[1m\u001b[32m0.56128\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1028 | loss: 0.56128 - acc: 0.7490 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1029  | total loss: \u001b[1m\u001b[32m0.55116\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1029 | loss: 0.55116 - acc: 0.7540 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1030  | total loss: \u001b[1m\u001b[32m0.54193\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1030 | loss: 0.54193 - acc: 0.7585 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1031  | total loss: \u001b[1m\u001b[32m0.53337\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1031 | loss: 0.53337 - acc: 0.7632 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1032  | total loss: \u001b[1m\u001b[32m0.57638\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1032 | loss: 0.57638 - acc: 0.7409 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1033  | total loss: \u001b[1m\u001b[32m0.56382\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1033 | loss: 0.56382 - acc: 0.7482 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1034  | total loss: \u001b[1m\u001b[32m0.55243\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1034 | loss: 0.55243 - acc: 0.7545 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1035  | total loss: \u001b[1m\u001b[32m0.54223\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1035 | loss: 0.54223 - acc: 0.7607 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1036  | total loss: \u001b[1m\u001b[32m0.57462\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1036 | loss: 0.57462 - acc: 0.7382 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1037  | total loss: \u001b[1m\u001b[32m0.56334\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1037 | loss: 0.56334 - acc: 0.7452 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1038  | total loss: \u001b[1m\u001b[32m0.55409\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1038 | loss: 0.55409 - acc: 0.7510 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1039  | total loss: \u001b[1m\u001b[32m0.54617\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1039 | loss: 0.54617 - acc: 0.7563 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1040  | total loss: \u001b[1m\u001b[32m0.57039\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1040 | loss: 0.57039 - acc: 0.7367 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1041  | total loss: \u001b[1m\u001b[32m0.56110\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1041 | loss: 0.56110 - acc: 0.7430 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1042  | total loss: \u001b[1m\u001b[32m0.58336\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1042 | loss: 0.58336 - acc: 0.7230 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1043  | total loss: \u001b[1m\u001b[32m0.57314\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1043 | loss: 0.57314 - acc: 0.7314 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1044  | total loss: \u001b[1m\u001b[32m0.56393\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1044 | loss: 0.56393 - acc: 0.7394 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1045  | total loss: \u001b[1m\u001b[32m0.55524\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1045 | loss: 0.55524 - acc: 0.7463 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1046  | total loss: \u001b[1m\u001b[32m0.57714\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1046 | loss: 0.57714 - acc: 0.7259 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1047  | total loss: \u001b[1m\u001b[32m0.56663\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1047 | loss: 0.56663 - acc: 0.7344 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1048  | total loss: \u001b[1m\u001b[32m0.55709\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1048 | loss: 0.55709 - acc: 0.7421 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1049  | total loss: \u001b[1m\u001b[32m0.54829\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1049 | loss: 0.54829 - acc: 0.7488 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1050  | total loss: \u001b[1m\u001b[32m0.57438\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1050 | loss: 0.57438 - acc: 0.7265 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1051  | total loss: \u001b[1m\u001b[32m0.56384\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1051 | loss: 0.56384 - acc: 0.7350 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1052  | total loss: \u001b[1m\u001b[32m0.58778\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1052 | loss: 0.58778 - acc: 0.7147 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1053  | total loss: \u001b[1m\u001b[32m0.57655\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1053 | loss: 0.57655 - acc: 0.7245 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1054  | total loss: \u001b[1m\u001b[32m0.59838\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1054 | loss: 0.59838 - acc: 0.7054 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1055  | total loss: \u001b[1m\u001b[32m0.58734\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1055 | loss: 0.58734 - acc: 0.7163 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1056  | total loss: \u001b[1m\u001b[32m0.57804\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1056 | loss: 0.57804 - acc: 0.7259 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1057  | total loss: \u001b[1m\u001b[32m0.56995\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1057 | loss: 0.56995 - acc: 0.7343 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1058  | total loss: \u001b[1m\u001b[32m0.56262\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1058 | loss: 0.56262 - acc: 0.7419 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1059  | total loss: \u001b[1m\u001b[32m0.55565\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1059 | loss: 0.55565 - acc: 0.7483 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1060  | total loss: \u001b[1m\u001b[32m0.54870\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1060 | loss: 0.54870 - acc: 0.7542 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1061  | total loss: \u001b[1m\u001b[32m0.54157\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1061 | loss: 0.54157 - acc: 0.7599 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1062  | total loss: \u001b[1m\u001b[32m0.56789\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1062 | loss: 0.56789 - acc: 0.7380 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1063  | total loss: \u001b[1m\u001b[32m0.55784\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1063 | loss: 0.55784 - acc: 0.7455 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1064  | total loss: \u001b[1m\u001b[32m0.54858\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1064 | loss: 0.54858 - acc: 0.7520 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1065  | total loss: \u001b[1m\u001b[32m0.53986\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1065 | loss: 0.53986 - acc: 0.7577 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1066  | total loss: \u001b[1m\u001b[32m0.53153\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1066 | loss: 0.53153 - acc: 0.7632 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1067  | total loss: \u001b[1m\u001b[32m0.52365\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1067 | loss: 0.52365 - acc: 0.7685 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1068  | total loss: \u001b[1m\u001b[32m0.56109\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1068 | loss: 0.56109 - acc: 0.7463 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1069  | total loss: \u001b[1m\u001b[32m0.54992\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1069 | loss: 0.54992 - acc: 0.7535 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1070  | total loss: \u001b[1m\u001b[32m0.53971\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1070 | loss: 0.53971 - acc: 0.7597 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1071  | total loss: \u001b[1m\u001b[32m0.53041\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1071 | loss: 0.53041 - acc: 0.7650 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1072  | total loss: \u001b[1m\u001b[32m0.56698\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1072 | loss: 0.56698 - acc: 0.7419 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1073  | total loss: \u001b[1m\u001b[32m0.55535\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1073 | loss: 0.55535 - acc: 0.7482 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1074  | total loss: \u001b[1m\u001b[32m0.54520\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1074 | loss: 0.54520 - acc: 0.7537 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1075  | total loss: \u001b[1m\u001b[32m0.53620\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1075 | loss: 0.53620 - acc: 0.7591 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1076  | total loss: \u001b[1m\u001b[32m0.52803\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1076 | loss: 0.52803 - acc: 0.7644 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1077  | total loss: \u001b[1m\u001b[32m0.52036\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1077 | loss: 0.52036 - acc: 0.7691 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1078  | total loss: \u001b[1m\u001b[32m0.51300\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1078 | loss: 0.51300 - acc: 0.7732 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1079  | total loss: \u001b[1m\u001b[32m0.50613\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1079 | loss: 0.50613 - acc: 0.7764 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1080  | total loss: \u001b[1m\u001b[32m0.54928\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1080 | loss: 0.54928 - acc: 0.7519 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1081  | total loss: \u001b[1m\u001b[32m0.53875\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1081 | loss: 0.53875 - acc: 0.7572 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1082  | total loss: \u001b[1m\u001b[32m0.52925\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1082 | loss: 0.52925 - acc: 0.7622 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1083  | total loss: \u001b[1m\u001b[32m0.52072\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1083 | loss: 0.52072 - acc: 0.7668 -- iter: 891/891\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1084  | total loss: \u001b[1m\u001b[32m0.51304\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1084 | loss: 0.51304 - acc: 0.7712 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1085  | total loss: \u001b[1m\u001b[32m0.50610\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1085 | loss: 0.50610 - acc: 0.7752 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1086  | total loss: \u001b[1m\u001b[32m0.49981\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1086 | loss: 0.49981 - acc: 0.7784 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1087  | total loss: \u001b[1m\u001b[32m0.49413\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1087 | loss: 0.49413 - acc: 0.7806 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1088  | total loss: \u001b[1m\u001b[32m0.54106\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1088 | loss: 0.54106 - acc: 0.7533 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1089  | total loss: \u001b[1m\u001b[32m0.53141\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1089 | loss: 0.53141 - acc: 0.7583 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1090  | total loss: \u001b[1m\u001b[32m0.52299\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1090 | loss: 0.52299 - acc: 0.7627 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1091  | total loss: \u001b[1m\u001b[32m0.51563\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1091 | loss: 0.51563 - acc: 0.7668 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1092  | total loss: \u001b[1m\u001b[32m0.55145\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1092 | loss: 0.55145 - acc: 0.7435 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1093  | total loss: \u001b[1m\u001b[32m0.54201\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1093 | loss: 0.54201 - acc: 0.7498 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1094  | total loss: \u001b[1m\u001b[32m0.57162\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1094 | loss: 0.57162 - acc: 0.7288 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1095  | total loss: \u001b[1m\u001b[32m0.56186\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1095 | loss: 0.56186 - acc: 0.7361 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1096  | total loss: \u001b[1m\u001b[32m0.58558\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1096 | loss: 0.58558 - acc: 0.7143 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1097  | total loss: \u001b[1m\u001b[32m0.57673\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1097 | loss: 0.57673 - acc: 0.7226 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1098  | total loss: \u001b[1m\u001b[32m0.56948\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1098 | loss: 0.56948 - acc: 0.7301 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1099  | total loss: \u001b[1m\u001b[32m0.56298\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1099 | loss: 0.56298 - acc: 0.7372 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1100  | total loss: \u001b[1m\u001b[32m0.58067\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1100 | loss: 0.58067 - acc: 0.7169 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1101  | total loss: \u001b[1m\u001b[32m0.57283\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1101 | loss: 0.57283 - acc: 0.7258 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1102  | total loss: \u001b[1m\u001b[32m0.59000\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1102 | loss: 0.59000 - acc: 0.7074 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1103  | total loss: \u001b[1m\u001b[32m0.58126\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1103 | loss: 0.58126 - acc: 0.7174 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1104  | total loss: \u001b[1m\u001b[32m0.59492\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1104 | loss: 0.59492 - acc: 0.7030 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1105  | total loss: \u001b[1m\u001b[32m0.58591\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1105 | loss: 0.58591 - acc: 0.7136 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1106  | total loss: \u001b[1m\u001b[32m0.57774\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1106 | loss: 0.57774 - acc: 0.7228 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1107  | total loss: \u001b[1m\u001b[32m0.56999\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1107 | loss: 0.56999 - acc: 0.7309 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1108  | total loss: \u001b[1m\u001b[32m0.58907\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1108 | loss: 0.58907 - acc: 0.7111 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1109  | total loss: \u001b[1m\u001b[32m0.57961\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1109 | loss: 0.57961 - acc: 0.7200 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1110  | total loss: \u001b[1m\u001b[32m0.57087\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1110 | loss: 0.57087 - acc: 0.7280 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1111  | total loss: \u001b[1m\u001b[32m0.56261\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1111 | loss: 0.56261 - acc: 0.7338 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1112  | total loss: \u001b[1m\u001b[32m0.58455\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1112 | loss: 0.58455 - acc: 0.7151 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1113  | total loss: \u001b[1m\u001b[32m0.57437\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1113 | loss: 0.57437 - acc: 0.7243 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1114  | total loss: \u001b[1m\u001b[32m0.56504\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1114 | loss: 0.56504 - acc: 0.7324 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1115  | total loss: \u001b[1m\u001b[32m0.55632\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1115 | loss: 0.55632 - acc: 0.7395 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1116  | total loss: \u001b[1m\u001b[32m0.54802\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1116 | loss: 0.54802 - acc: 0.7470 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1117  | total loss: \u001b[1m\u001b[32m0.54002\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1117 | loss: 0.54002 - acc: 0.7539 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1118  | total loss: \u001b[1m\u001b[32m0.53227\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1118 | loss: 0.53227 - acc: 0.7602 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1119  | total loss: \u001b[1m\u001b[32m0.52480\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1119 | loss: 0.52480 - acc: 0.7659 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1120  | total loss: \u001b[1m\u001b[32m0.51766\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1120 | loss: 0.51766 - acc: 0.7707 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1121  | total loss: \u001b[1m\u001b[32m0.51092\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1121 | loss: 0.51092 - acc: 0.7753 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1122  | total loss: \u001b[1m\u001b[32m0.50464\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1122 | loss: 0.50464 - acc: 0.7790 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1123  | total loss: \u001b[1m\u001b[32m0.49886\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1123 | loss: 0.49886 - acc: 0.7820 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1124  | total loss: \u001b[1m\u001b[32m0.55149\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1124 | loss: 0.55149 - acc: 0.7559 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1125  | total loss: \u001b[1m\u001b[32m0.54077\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1125 | loss: 0.54077 - acc: 0.7610 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1126  | total loss: \u001b[1m\u001b[32m0.58521\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1126 | loss: 0.58521 - acc: 0.7374 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1127  | total loss: \u001b[1m\u001b[32m0.57174\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1127 | loss: 0.57174 - acc: 0.7447 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1128  | total loss: \u001b[1m\u001b[32m0.56039\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1128 | loss: 0.56039 - acc: 0.7511 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1129  | total loss: \u001b[1m\u001b[32m0.55041\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1129 | loss: 0.55041 - acc: 0.7568 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1130  | total loss: \u001b[1m\u001b[32m0.54124\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1130 | loss: 0.54124 - acc: 0.7616 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1131  | total loss: \u001b[1m\u001b[32m0.53248\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1131 | loss: 0.53248 - acc: 0.7658 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1132  | total loss: \u001b[1m\u001b[32m0.52396\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1132 | loss: 0.52396 - acc: 0.7696 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1133  | total loss: \u001b[1m\u001b[32m0.51595\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1133 | loss: 0.51595 - acc: 0.7737 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1134  | total loss: \u001b[1m\u001b[32m0.50883\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1134 | loss: 0.50883 - acc: 0.7777 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1135  | total loss: \u001b[1m\u001b[32m0.50259\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1135 | loss: 0.50259 - acc: 0.7809 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1136  | total loss: \u001b[1m\u001b[32m0.55106\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1136 | loss: 0.55106 - acc: 0.7562 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1137  | total loss: \u001b[1m\u001b[32m0.54070\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1137 | loss: 0.54070 - acc: 0.7610 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1138  | total loss: \u001b[1m\u001b[32m0.53139\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1138 | loss: 0.53139 - acc: 0.7648 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1139  | total loss: \u001b[1m\u001b[32m0.52297\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1139 | loss: 0.52297 - acc: 0.7684 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1140  | total loss: \u001b[1m\u001b[32m0.55658\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1140 | loss: 0.55658 - acc: 0.7460 -- iter: 891/891\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1141  | total loss: \u001b[1m\u001b[32m0.54582\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1141 | loss: 0.54582 - acc: 0.7523 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1142  | total loss: \u001b[1m\u001b[32m0.53662\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1142 | loss: 0.53662 - acc: 0.7581 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1143  | total loss: \u001b[1m\u001b[32m0.52878\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1143 | loss: 0.52878 - acc: 0.7636 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1144  | total loss: \u001b[1m\u001b[32m0.55800\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1144 | loss: 0.55800 - acc: 0.7400 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1145  | total loss: \u001b[1m\u001b[32m0.54925\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1145 | loss: 0.54925 - acc: 0.7467 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1146  | total loss: \u001b[1m\u001b[32m0.57304\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1146 | loss: 0.57304 - acc: 0.7253 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1147  | total loss: \u001b[1m\u001b[32m0.56471\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1147 | loss: 0.56471 - acc: 0.7327 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1148  | total loss: \u001b[1m\u001b[32m0.55781\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1148 | loss: 0.55781 - acc: 0.7397 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1149  | total loss: \u001b[1m\u001b[32m0.55159\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1149 | loss: 0.55159 - acc: 0.7461 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1150  | total loss: \u001b[1m\u001b[32m0.54549\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1150 | loss: 0.54549 - acc: 0.7523 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1151  | total loss: \u001b[1m\u001b[32m0.53918\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1151 | loss: 0.53918 - acc: 0.7580 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1152  | total loss: \u001b[1m\u001b[32m0.53257\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1152 | loss: 0.53257 - acc: 0.7633 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1153  | total loss: \u001b[1m\u001b[32m0.52582\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1153 | loss: 0.52582 - acc: 0.7683 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1154  | total loss: \u001b[1m\u001b[32m0.56218\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1154 | loss: 0.56218 - acc: 0.7418 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1155  | total loss: \u001b[1m\u001b[32m0.55190\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1155 | loss: 0.55190 - acc: 0.7488 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1156  | total loss: \u001b[1m\u001b[32m0.58484\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1156 | loss: 0.58484 - acc: 0.7266 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1157  | total loss: \u001b[1m\u001b[32m0.57241\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1157 | loss: 0.57241 - acc: 0.7348 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1158  | total loss: \u001b[1m\u001b[32m0.59737\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1158 | loss: 0.59737 - acc: 0.7160 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1159  | total loss: \u001b[1m\u001b[32m0.58409\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1159 | loss: 0.58409 - acc: 0.7255 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1160  | total loss: \u001b[1m\u001b[32m0.60639\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1160 | loss: 0.60639 - acc: 0.7078 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1161  | total loss: \u001b[1m\u001b[32m0.59353\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1161 | loss: 0.59353 - acc: 0.7185 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1162  | total loss: \u001b[1m\u001b[32m0.58284\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1162 | loss: 0.58284 - acc: 0.7282 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1163  | total loss: \u001b[1m\u001b[32m0.57375\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1163 | loss: 0.57375 - acc: 0.7367 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1164  | total loss: \u001b[1m\u001b[32m0.56564\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1164 | loss: 0.56564 - acc: 0.7442 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1165  | total loss: \u001b[1m\u001b[32m0.55799\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1165 | loss: 0.55799 - acc: 0.7511 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1166  | total loss: \u001b[1m\u001b[32m0.57439\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1166 | loss: 0.57439 - acc: 0.7344 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1167  | total loss: \u001b[1m\u001b[32m0.56508\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1167 | loss: 0.56508 - acc: 0.7422 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1168  | total loss: \u001b[1m\u001b[32m0.55630\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1168 | loss: 0.55630 - acc: 0.7491 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1169  | total loss: \u001b[1m\u001b[32m0.54774\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1169 | loss: 0.54774 - acc: 0.7557 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1170  | total loss: \u001b[1m\u001b[32m0.53931\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1170 | loss: 0.53931 - acc: 0.7617 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1171  | total loss: \u001b[1m\u001b[32m0.53111\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1171 | loss: 0.53111 - acc: 0.7674 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1172  | total loss: \u001b[1m\u001b[32m0.56122\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1172 | loss: 0.56122 - acc: 0.7472 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1173  | total loss: \u001b[1m\u001b[32m0.55033\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1173 | loss: 0.55033 - acc: 0.7535 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1174  | total loss: \u001b[1m\u001b[32m0.58079\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1174 | loss: 0.58079 - acc: 0.7330 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1175  | total loss: \u001b[1m\u001b[32m0.56810\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1175 | loss: 0.56810 - acc: 0.7404 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1176  | total loss: \u001b[1m\u001b[32m0.55679\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1176 | loss: 0.55679 - acc: 0.7471 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1177  | total loss: \u001b[1m\u001b[32m0.54662\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1177 | loss: 0.54662 - acc: 0.7539 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1178  | total loss: \u001b[1m\u001b[32m0.57189\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1178 | loss: 0.57189 - acc: 0.7360 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1179  | total loss: \u001b[1m\u001b[32m0.56087\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1179 | loss: 0.56087 - acc: 0.7438 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1180  | total loss: \u001b[1m\u001b[32m0.55137\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1180 | loss: 0.55137 - acc: 0.7509 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1181  | total loss: \u001b[1m\u001b[32m0.54291\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1181 | loss: 0.54291 - acc: 0.7571 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1182  | total loss: \u001b[1m\u001b[32m0.53509\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1182 | loss: 0.53509 - acc: 0.7622 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1183  | total loss: \u001b[1m\u001b[32m0.52763\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1183 | loss: 0.52763 - acc: 0.7665 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1184  | total loss: \u001b[1m\u001b[32m0.52035\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1184 | loss: 0.52035 - acc: 0.7708 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1185  | total loss: \u001b[1m\u001b[32m0.51330\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1185 | loss: 0.51330 - acc: 0.7750 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1186  | total loss: \u001b[1m\u001b[32m0.50670\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1186 | loss: 0.50670 - acc: 0.7791 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1187  | total loss: \u001b[1m\u001b[32m0.50071\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1187 | loss: 0.50071 - acc: 0.7822 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1188  | total loss: \u001b[1m\u001b[32m0.49536\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1188 | loss: 0.49536 - acc: 0.7851 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1189  | total loss: \u001b[1m\u001b[32m0.49062\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1189 | loss: 0.49062 - acc: 0.7876 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1190  | total loss: \u001b[1m\u001b[32m0.53980\u001b[0m\u001b[0m | time: 0.012s\n",
      "| Adam | epoch: 1190 | loss: 0.53980 - acc: 0.7631 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1191  | total loss: \u001b[1m\u001b[32m0.53056\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1191 | loss: 0.53056 - acc: 0.7664 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1192  | total loss: \u001b[1m\u001b[32m0.52197\u001b[0m\u001b[0m | time: 0.011s\n",
      "| Adam | epoch: 1192 | loss: 0.52197 - acc: 0.7696 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1193  | total loss: \u001b[1m\u001b[32m0.51403\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1193 | loss: 0.51403 - acc: 0.7731 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1194  | total loss: \u001b[1m\u001b[32m0.55236\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1194 | loss: 0.55236 - acc: 0.7518 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1195  | total loss: \u001b[1m\u001b[32m0.54207\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1195 | loss: 0.54207 - acc: 0.7576 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1196  | total loss: \u001b[1m\u001b[32m0.53342\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1196 | loss: 0.53342 - acc: 0.7625 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1197  | total loss: \u001b[1m\u001b[32m0.52604\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1197 | loss: 0.52604 - acc: 0.7664 -- iter: 891/891\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1198  | total loss: \u001b[1m\u001b[32m0.51956\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1198 | loss: 0.51956 - acc: 0.7700 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1199  | total loss: \u001b[1m\u001b[32m0.51340\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1199 | loss: 0.51340 - acc: 0.7731 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1200  | total loss: \u001b[1m\u001b[32m0.54342\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1200 | loss: 0.54342 - acc: 0.7519 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1201  | total loss: \u001b[1m\u001b[32m0.53448\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1201 | loss: 0.53448 - acc: 0.7578 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1202  | total loss: \u001b[1m\u001b[32m0.56451\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1202 | loss: 0.56451 - acc: 0.7371 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1203  | total loss: \u001b[1m\u001b[32m0.55431\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1203 | loss: 0.55431 - acc: 0.7448 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1204  | total loss: \u001b[1m\u001b[32m0.54561\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1204 | loss: 0.54561 - acc: 0.7513 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1205  | total loss: \u001b[1m\u001b[32m0.53798\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1205 | loss: 0.53798 - acc: 0.7571 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1206  | total loss: \u001b[1m\u001b[32m0.56650\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1206 | loss: 0.56650 - acc: 0.7328 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1207  | total loss: \u001b[1m\u001b[32m0.55731\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1207 | loss: 0.55731 - acc: 0.7408 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1208  | total loss: \u001b[1m\u001b[32m0.57871\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1208 | loss: 0.57871 - acc: 0.7209 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1209  | total loss: \u001b[1m\u001b[32m0.56935\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1209 | loss: 0.56935 - acc: 0.7296 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1210  | total loss: \u001b[1m\u001b[32m0.56134\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1210 | loss: 0.56134 - acc: 0.7374 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1211  | total loss: \u001b[1m\u001b[32m0.55419\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1211 | loss: 0.55419 - acc: 0.7445 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1212  | total loss: \u001b[1m\u001b[32m0.54747\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1212 | loss: 0.54747 - acc: 0.7510 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1213  | total loss: \u001b[1m\u001b[32m0.54090\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1213 | loss: 0.54090 - acc: 0.7571 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1214  | total loss: \u001b[1m\u001b[32m0.53435\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1214 | loss: 0.53435 - acc: 0.7625 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1215  | total loss: \u001b[1m\u001b[32m0.52781\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1215 | loss: 0.52781 - acc: 0.7673 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1216  | total loss: \u001b[1m\u001b[32m0.52136\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1216 | loss: 0.52136 - acc: 0.7717 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1217  | total loss: \u001b[1m\u001b[32m0.51515\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1217 | loss: 0.51515 - acc: 0.7757 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1218  | total loss: \u001b[1m\u001b[32m0.55584\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1218 | loss: 0.55584 - acc: 0.7509 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1219  | total loss: \u001b[1m\u001b[32m0.54578\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1219 | loss: 0.54578 - acc: 0.7567 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1220  | total loss: \u001b[1m\u001b[32m0.53654\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1220 | loss: 0.53654 - acc: 0.7617 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1221  | total loss: \u001b[1m\u001b[32m0.52797\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1221 | loss: 0.52797 - acc: 0.7665 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1222  | total loss: \u001b[1m\u001b[32m0.52002\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1222 | loss: 0.52002 - acc: 0.7710 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1223  | total loss: \u001b[1m\u001b[32m0.51264\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1223 | loss: 0.51264 - acc: 0.7751 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1224  | total loss: \u001b[1m\u001b[32m0.55536\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1224 | loss: 0.55536 - acc: 0.7499 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1225  | total loss: \u001b[1m\u001b[32m0.54424\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1225 | loss: 0.54424 - acc: 0.7561 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1226  | total loss: \u001b[1m\u001b[32m0.53452\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1226 | loss: 0.53452 - acc: 0.7610 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1227  | total loss: \u001b[1m\u001b[32m0.52614\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1227 | loss: 0.52614 - acc: 0.7655 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1228  | total loss: \u001b[1m\u001b[32m0.55680\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1228 | loss: 0.55680 - acc: 0.7461 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1229  | total loss: \u001b[1m\u001b[32m0.54728\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1229 | loss: 0.54728 - acc: 0.7521 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1230  | total loss: \u001b[1m\u001b[32m0.53911\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1230 | loss: 0.53911 - acc: 0.7571 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1231  | total loss: \u001b[1m\u001b[32m0.53157\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1231 | loss: 0.53157 - acc: 0.7619 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1232  | total loss: \u001b[1m\u001b[32m0.56058\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1232 | loss: 0.56058 - acc: 0.7415 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1233  | total loss: \u001b[1m\u001b[32m0.55053\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1233 | loss: 0.55053 - acc: 0.7477 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1234  | total loss: \u001b[1m\u001b[32m0.54140\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1234 | loss: 0.54140 - acc: 0.7535 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1235  | total loss: \u001b[1m\u001b[32m0.53293\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1235 | loss: 0.53293 - acc: 0.7591 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1236  | total loss: \u001b[1m\u001b[32m0.56467\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1236 | loss: 0.56467 - acc: 0.7358 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1237  | total loss: \u001b[1m\u001b[32m0.55388\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1237 | loss: 0.55388 - acc: 0.7436 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1238  | total loss: \u001b[1m\u001b[32m0.58250\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1238 | loss: 0.58250 - acc: 0.7201 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1239  | total loss: \u001b[1m\u001b[32m0.57090\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1239 | loss: 0.57090 - acc: 0.7292 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1240  | total loss: \u001b[1m\u001b[32m0.58980\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1240 | loss: 0.58980 - acc: 0.7114 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1241  | total loss: \u001b[1m\u001b[32m0.57923\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1241 | loss: 0.57923 - acc: 0.7212 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1242  | total loss: \u001b[1m\u001b[32m0.59589\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1242 | loss: 0.59589 - acc: 0.7039 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1243  | total loss: \u001b[1m\u001b[32m0.58683\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1243 | loss: 0.58683 - acc: 0.7142 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1244  | total loss: \u001b[1m\u001b[32m0.57947\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1244 | loss: 0.57947 - acc: 0.7236 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1245  | total loss: \u001b[1m\u001b[32m0.57310\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1245 | loss: 0.57310 - acc: 0.7316 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1246  | total loss: \u001b[1m\u001b[32m0.58776\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1246 | loss: 0.58776 - acc: 0.7151 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1247  | total loss: \u001b[1m\u001b[32m0.58048\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1247 | loss: 0.58048 - acc: 0.7235 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1248  | total loss: \u001b[1m\u001b[32m0.57364\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1248 | loss: 0.57364 - acc: 0.7314 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1249  | total loss: \u001b[1m\u001b[32m0.56686\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1249 | loss: 0.56686 - acc: 0.7386 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1250  | total loss: \u001b[1m\u001b[32m0.58433\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1250 | loss: 0.58433 - acc: 0.7199 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1251  | total loss: \u001b[1m\u001b[32m0.57536\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1251 | loss: 0.57536 - acc: 0.7284 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1252  | total loss: \u001b[1m\u001b[32m0.59364\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1252 | loss: 0.59364 - acc: 0.7092 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1253  | total loss: \u001b[1m\u001b[32m0.58333\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1253 | loss: 0.58333 - acc: 0.7196 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1254  | total loss: \u001b[1m\u001b[32m0.59875\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1254 | loss: 0.59875 - acc: 0.7025 -- iter: 891/891\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1255  | total loss: \u001b[1m\u001b[32m0.58789\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1255 | loss: 0.58789 - acc: 0.7136 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1256  | total loss: \u001b[1m\u001b[32m0.60402\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1256 | loss: 0.60402 - acc: 0.6966 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1257  | total loss: \u001b[1m\u001b[32m0.59291\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1257 | loss: 0.59291 - acc: 0.7078 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1258  | total loss: \u001b[1m\u001b[32m0.60660\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1258 | loss: 0.60660 - acc: 0.6931 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1259  | total loss: \u001b[1m\u001b[32m0.59583\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1259 | loss: 0.59583 - acc: 0.7047 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1260  | total loss: \u001b[1m\u001b[32m0.61153\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1260 | loss: 0.61153 - acc: 0.6876 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1261  | total loss: \u001b[1m\u001b[32m0.60103\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1261 | loss: 0.60103 - acc: 0.6997 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1262  | total loss: \u001b[1m\u001b[32m0.61475\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1262 | loss: 0.61475 - acc: 0.6837 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1263  | total loss: \u001b[1m\u001b[32m0.60482\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1263 | loss: 0.60482 - acc: 0.6966 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1264  | total loss: \u001b[1m\u001b[32m0.61410\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1264 | loss: 0.61410 - acc: 0.6834 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1265  | total loss: \u001b[1m\u001b[32m0.60487\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1265 | loss: 0.60487 - acc: 0.6963 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1266  | total loss: \u001b[1m\u001b[32m0.61579\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1266 | loss: 0.61579 - acc: 0.6819 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1267  | total loss: \u001b[1m\u001b[32m0.60664\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1267 | loss: 0.60664 - acc: 0.6939 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1268  | total loss: \u001b[1m\u001b[32m0.61688\u001b[0m\u001b[0m | time: 0.011s\n",
      "| Adam | epoch: 1268 | loss: 0.61688 - acc: 0.6800 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1269  | total loss: \u001b[1m\u001b[32m0.60760\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1269 | loss: 0.60760 - acc: 0.6926 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1270  | total loss: \u001b[1m\u001b[32m0.59898\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1270 | loss: 0.59898 - acc: 0.7040 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1271  | total loss: \u001b[1m\u001b[32m0.59058\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1271 | loss: 0.59058 - acc: 0.7144 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1272  | total loss: \u001b[1m\u001b[32m0.60387\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1272 | loss: 0.60387 - acc: 0.6989 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1273  | total loss: \u001b[1m\u001b[32m0.59363\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1273 | loss: 0.59363 - acc: 0.7100 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1274  | total loss: \u001b[1m\u001b[32m0.60895\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1274 | loss: 0.60895 - acc: 0.6934 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1275  | total loss: \u001b[1m\u001b[32m0.59748\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1275 | loss: 0.59748 - acc: 0.7057 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1276  | total loss: \u001b[1m\u001b[32m0.61077\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1276 | loss: 0.61077 - acc: 0.6908 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1277  | total loss: \u001b[1m\u001b[32m0.59885\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1277 | loss: 0.59885 - acc: 0.7034 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1278  | total loss: \u001b[1m\u001b[32m0.58788\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1278 | loss: 0.58788 - acc: 0.7145 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1279  | total loss: \u001b[1m\u001b[32m0.57758\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1279 | loss: 0.57758 - acc: 0.7245 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1280  | total loss: \u001b[1m\u001b[32m0.56779\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1280 | loss: 0.56779 - acc: 0.7334 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1281  | total loss: \u001b[1m\u001b[32m0.55841\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1281 | loss: 0.55841 - acc: 0.7412 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1282  | total loss: \u001b[1m\u001b[32m0.54939\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1282 | loss: 0.54939 - acc: 0.7485 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1283  | total loss: \u001b[1m\u001b[32m0.54071\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1283 | loss: 0.54071 - acc: 0.7548 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1284  | total loss: \u001b[1m\u001b[32m0.56991\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1284 | loss: 0.56991 - acc: 0.7354 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1285  | total loss: \u001b[1m\u001b[32m0.55862\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1285 | loss: 0.55862 - acc: 0.7429 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1286  | total loss: \u001b[1m\u001b[32m0.59429\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1286 | loss: 0.59429 - acc: 0.7204 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1287  | total loss: \u001b[1m\u001b[32m0.58051\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1287 | loss: 0.58051 - acc: 0.7286 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1288  | total loss: \u001b[1m\u001b[32m0.56801\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1288 | loss: 0.56801 - acc: 0.7364 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1289  | total loss: \u001b[1m\u001b[32m0.55668\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1289 | loss: 0.55668 - acc: 0.7444 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1290  | total loss: \u001b[1m\u001b[32m0.58437\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1290 | loss: 0.58437 - acc: 0.7241 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1291  | total loss: \u001b[1m\u001b[32m0.57230\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1291 | loss: 0.57230 - acc: 0.7332 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1292  | total loss: \u001b[1m\u001b[32m0.59430\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1292 | loss: 0.59430 - acc: 0.7163 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1293  | total loss: \u001b[1m\u001b[32m0.58278\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1293 | loss: 0.58278 - acc: 0.7260 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1294  | total loss: \u001b[1m\u001b[32m0.60413\u001b[0m\u001b[0m | time: 0.016s\n",
      "| Adam | epoch: 1294 | loss: 0.60413 - acc: 0.7051 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1295  | total loss: \u001b[1m\u001b[32m0.59399\u001b[0m\u001b[0m | time: 0.018s\n",
      "| Adam | epoch: 1295 | loss: 0.59399 - acc: 0.7149 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1296  | total loss: \u001b[1m\u001b[32m0.58561\u001b[0m\u001b[0m | time: 0.014s\n",
      "| Adam | epoch: 1296 | loss: 0.58561 - acc: 0.7238 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1297  | total loss: \u001b[1m\u001b[32m0.57809\u001b[0m\u001b[0m | time: 0.012s\n",
      "| Adam | epoch: 1297 | loss: 0.57809 - acc: 0.7319 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1298  | total loss: \u001b[1m\u001b[32m0.59482\u001b[0m\u001b[0m | time: 0.016s\n",
      "| Adam | epoch: 1298 | loss: 0.59482 - acc: 0.7125 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1299  | total loss: \u001b[1m\u001b[32m0.58588\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1299 | loss: 0.58588 - acc: 0.7218 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1300  | total loss: \u001b[1m\u001b[32m0.57744\u001b[0m\u001b[0m | time: 0.014s\n",
      "| Adam | epoch: 1300 | loss: 0.57744 - acc: 0.7308 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1301  | total loss: \u001b[1m\u001b[32m0.56916\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1301 | loss: 0.56916 - acc: 0.7393 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1302  | total loss: \u001b[1m\u001b[32m0.56089\u001b[0m\u001b[0m | time: 0.011s\n",
      "| Adam | epoch: 1302 | loss: 0.56089 - acc: 0.7465 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1303  | total loss: \u001b[1m\u001b[32m0.55264\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1303 | loss: 0.55264 - acc: 0.7526 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1304  | total loss: \u001b[1m\u001b[32m0.57919\u001b[0m\u001b[0m | time: 0.011s\n",
      "| Adam | epoch: 1304 | loss: 0.57919 - acc: 0.7311 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1305  | total loss: \u001b[1m\u001b[32m0.56846\u001b[0m\u001b[0m | time: 0.011s\n",
      "| Adam | epoch: 1305 | loss: 0.56846 - acc: 0.7385 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1306  | total loss: \u001b[1m\u001b[32m0.59460\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1306 | loss: 0.59460 - acc: 0.7169 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1307  | total loss: \u001b[1m\u001b[32m0.58253\u001b[0m\u001b[0m | time: 0.011s\n",
      "| Adam | epoch: 1307 | loss: 0.58253 - acc: 0.7255 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1308  | total loss: \u001b[1m\u001b[32m0.57177\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1308 | loss: 0.57177 - acc: 0.7332 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1309  | total loss: \u001b[1m\u001b[32m0.56201\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1309 | loss: 0.56201 - acc: 0.7402 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1310  | total loss: \u001b[1m\u001b[32m0.55304\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1310 | loss: 0.55304 - acc: 0.7467 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1311  | total loss: \u001b[1m\u001b[32m0.54473\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1311 | loss: 0.54473 - acc: 0.7525 -- iter: 891/891\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1312  | total loss: \u001b[1m\u001b[32m0.53696\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1312 | loss: 0.53696 - acc: 0.7576 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1313  | total loss: \u001b[1m\u001b[32m0.52961\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1313 | loss: 0.52961 - acc: 0.7625 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1314  | total loss: \u001b[1m\u001b[32m0.55919\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1314 | loss: 0.55919 - acc: 0.7414 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1315  | total loss: \u001b[1m\u001b[32m0.54910\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1315 | loss: 0.54910 - acc: 0.7485 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1316  | total loss: \u001b[1m\u001b[32m0.57817\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1316 | loss: 0.57817 - acc: 0.7258 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1317  | total loss: \u001b[1m\u001b[32m0.56662\u001b[0m\u001b[0m | time: 0.011s\n",
      "| Adam | epoch: 1317 | loss: 0.56662 - acc: 0.7335 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1318  | total loss: \u001b[1m\u001b[32m0.59078\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1318 | loss: 0.59078 - acc: 0.7126 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1319  | total loss: \u001b[1m\u001b[32m0.57949\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1319 | loss: 0.57949 - acc: 0.7223 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1320  | total loss: \u001b[1m\u001b[32m0.59744\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1320 | loss: 0.59744 - acc: 0.7036 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1321  | total loss: \u001b[1m\u001b[32m0.58801\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1321 | loss: 0.58801 - acc: 0.7138 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1322  | total loss: \u001b[1m\u001b[32m0.60217\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1322 | loss: 0.60217 - acc: 0.6988 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1323  | total loss: \u001b[1m\u001b[32m0.59419\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1323 | loss: 0.59419 - acc: 0.7086 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1324  | total loss: \u001b[1m\u001b[32m0.60967\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1324 | loss: 0.60967 - acc: 0.6902 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1325  | total loss: \u001b[1m\u001b[32m0.60179\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1325 | loss: 0.60179 - acc: 0.7008 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1326  | total loss: \u001b[1m\u001b[32m0.61228\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1326 | loss: 0.61228 - acc: 0.6878 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1327  | total loss: \u001b[1m\u001b[32m0.60427\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1327 | loss: 0.60427 - acc: 0.6997 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1328  | total loss: \u001b[1m\u001b[32m0.59674\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1328 | loss: 0.59674 - acc: 0.7104 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1329  | total loss: \u001b[1m\u001b[32m0.58924\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1329 | loss: 0.58924 - acc: 0.7207 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1330  | total loss: \u001b[1m\u001b[32m0.60402\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1330 | loss: 0.60402 - acc: 0.7024 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1331  | total loss: \u001b[1m\u001b[32m0.59444\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1331 | loss: 0.59444 - acc: 0.7134 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1332  | total loss: \u001b[1m\u001b[32m0.60711\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1332 | loss: 0.60711 - acc: 0.6983 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1333  | total loss: \u001b[1m\u001b[32m0.59653\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1333 | loss: 0.59653 - acc: 0.7076 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1334  | total loss: \u001b[1m\u001b[32m0.59947\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1334 | loss: 0.59947 - acc: 0.7048 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1335  | total loss: \u001b[1m\u001b[32m0.58914\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1335 | loss: 0.58914 - acc: 0.7131 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1336  | total loss: \u001b[1m\u001b[32m0.60576\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1336 | loss: 0.60576 - acc: 0.6977 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1337  | total loss: \u001b[1m\u001b[32m0.59457\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1337 | loss: 0.59457 - acc: 0.7072 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1338  | total loss: \u001b[1m\u001b[32m0.61026\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1338 | loss: 0.61026 - acc: 0.6915 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1339  | total loss: \u001b[1m\u001b[32m0.59886\u001b[0m\u001b[0m | time: 0.011s\n",
      "| Adam | epoch: 1339 | loss: 0.59886 - acc: 0.7031 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1340  | total loss: \u001b[1m\u001b[32m0.58869\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1340 | loss: 0.58869 - acc: 0.7138 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1341  | total loss: \u001b[1m\u001b[32m0.57937\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1341 | loss: 0.57937 - acc: 0.7234 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1342  | total loss: \u001b[1m\u001b[32m0.59681\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1342 | loss: 0.59681 - acc: 0.7047 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1343  | total loss: \u001b[1m\u001b[32m0.58639\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1343 | loss: 0.58639 - acc: 0.7151 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1344  | total loss: \u001b[1m\u001b[32m0.60145\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1344 | loss: 0.60145 - acc: 0.6994 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1345  | total loss: \u001b[1m\u001b[32m0.59072\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1345 | loss: 0.59072 - acc: 0.7104 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1346  | total loss: \u001b[1m\u001b[32m0.58110\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1346 | loss: 0.58110 - acc: 0.7202 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1347  | total loss: \u001b[1m\u001b[32m0.57219\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1347 | loss: 0.57219 - acc: 0.7292 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1348  | total loss: \u001b[1m\u001b[32m0.56368\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1348 | loss: 0.56368 - acc: 0.7374 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1349  | total loss: \u001b[1m\u001b[32m0.55534\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1349 | loss: 0.55534 - acc: 0.7451 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1350  | total loss: \u001b[1m\u001b[32m0.57939\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1350 | loss: 0.57939 - acc: 0.7245 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1351  | total loss: \u001b[1m\u001b[32m0.56854\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1351 | loss: 0.56854 - acc: 0.7341 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1352  | total loss: \u001b[1m\u001b[32m0.58997\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1352 | loss: 0.58997 - acc: 0.7164 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1353  | total loss: \u001b[1m\u001b[32m0.57802\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1353 | loss: 0.57802 - acc: 0.7261 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1354  | total loss: \u001b[1m\u001b[32m0.56729\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1354 | loss: 0.56729 - acc: 0.7350 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1355  | total loss: \u001b[1m\u001b[32m0.55744\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1355 | loss: 0.55744 - acc: 0.7427 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1356  | total loss: \u001b[1m\u001b[32m0.54823\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1356 | loss: 0.54823 - acc: 0.7499 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1357  | total loss: \u001b[1m\u001b[32m0.53951\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1357 | loss: 0.53951 - acc: 0.7564 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1358  | total loss: \u001b[1m\u001b[32m0.53121\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1358 | loss: 0.53121 - acc: 0.7624 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1359  | total loss: \u001b[1m\u001b[32m0.52331\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1359 | loss: 0.52331 - acc: 0.7676 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1360  | total loss: \u001b[1m\u001b[32m0.55994\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1360 | loss: 0.55994 - acc: 0.7438 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1361  | total loss: \u001b[1m\u001b[32m0.54882\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1361 | loss: 0.54882 - acc: 0.7504 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1362  | total loss: \u001b[1m\u001b[32m0.58156\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1362 | loss: 0.58156 - acc: 0.7279 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1363  | total loss: \u001b[1m\u001b[32m0.56844\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1363 | loss: 0.56844 - acc: 0.7353 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1364  | total loss: \u001b[1m\u001b[32m0.59810\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1364 | loss: 0.59810 - acc: 0.7151 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1365  | total loss: \u001b[1m\u001b[32m0.58452\u001b[0m\u001b[0m | time: 0.011s\n",
      "| Adam | epoch: 1365 | loss: 0.58452 - acc: 0.7243 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1366  | total loss: \u001b[1m\u001b[32m0.57319\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1366 | loss: 0.57319 - acc: 0.7324 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1367  | total loss: \u001b[1m\u001b[32m0.56342\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1367 | loss: 0.56342 - acc: 0.7397 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1368  | total loss: \u001b[1m\u001b[32m0.55452\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1368 | loss: 0.55452 - acc: 0.7463 -- iter: 891/891\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1369  | total loss: \u001b[1m\u001b[32m0.54603\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1369 | loss: 0.54603 - acc: 0.7523 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1370  | total loss: \u001b[1m\u001b[32m0.53772\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1370 | loss: 0.53772 - acc: 0.7577 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1371  | total loss: \u001b[1m\u001b[32m0.52962\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1371 | loss: 0.52962 - acc: 0.7630 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1372  | total loss: \u001b[1m\u001b[32m0.52189\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1372 | loss: 0.52189 - acc: 0.7677 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1373  | total loss: \u001b[1m\u001b[32m0.51474\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1373 | loss: 0.51474 - acc: 0.7722 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1374  | total loss: \u001b[1m\u001b[32m0.50833\u001b[0m\u001b[0m | time: 0.011s\n",
      "| Adam | epoch: 1374 | loss: 0.50833 - acc: 0.7761 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1375  | total loss: \u001b[1m\u001b[32m0.50266\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1375 | loss: 0.50266 - acc: 0.7800 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1376  | total loss: \u001b[1m\u001b[32m0.49766\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1376 | loss: 0.49766 - acc: 0.7828 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1377  | total loss: \u001b[1m\u001b[32m0.49323\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1377 | loss: 0.49323 - acc: 0.7844 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1378  | total loss: \u001b[1m\u001b[32m0.54119\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1378 | loss: 0.54119 - acc: 0.7607 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1379  | total loss: \u001b[1m\u001b[32m0.53201\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1379 | loss: 0.53201 - acc: 0.7644 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1380  | total loss: \u001b[1m\u001b[32m0.52337\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1380 | loss: 0.52337 - acc: 0.7679 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1381  | total loss: \u001b[1m\u001b[32m0.51538\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1381 | loss: 0.51538 - acc: 0.7718 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1382  | total loss: \u001b[1m\u001b[32m0.54835\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1382 | loss: 0.54835 - acc: 0.7531 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1383  | total loss: \u001b[1m\u001b[32m0.53871\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1383 | loss: 0.53871 - acc: 0.7590 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1384  | total loss: \u001b[1m\u001b[32m0.53097\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1384 | loss: 0.53097 - acc: 0.7637 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1385  | total loss: \u001b[1m\u001b[32m0.52470\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1385 | loss: 0.52470 - acc: 0.7673 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1386  | total loss: \u001b[1m\u001b[32m0.51924\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1386 | loss: 0.51924 - acc: 0.7707 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1387  | total loss: \u001b[1m\u001b[32m0.51397\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1387 | loss: 0.51397 - acc: 0.7738 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1388  | total loss: \u001b[1m\u001b[32m0.50855\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1388 | loss: 0.50855 - acc: 0.7766 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1389  | total loss: \u001b[1m\u001b[32m0.50302\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1389 | loss: 0.50302 - acc: 0.7804 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1390  | total loss: \u001b[1m\u001b[32m0.54344\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1390 | loss: 0.54344 - acc: 0.7538 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1391  | total loss: \u001b[1m\u001b[32m0.53418\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1391 | loss: 0.53418 - acc: 0.7596 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1392  | total loss: \u001b[1m\u001b[32m0.52581\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1392 | loss: 0.52581 - acc: 0.7646 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1393  | total loss: \u001b[1m\u001b[32m0.51817\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1393 | loss: 0.51817 - acc: 0.7689 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1394  | total loss: \u001b[1m\u001b[32m0.55248\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1394 | loss: 0.55248 - acc: 0.7472 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1395  | total loss: \u001b[1m\u001b[32m0.54247\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1395 | loss: 0.54247 - acc: 0.7523 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1396  | total loss: \u001b[1m\u001b[32m0.53359\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1396 | loss: 0.53359 - acc: 0.7570 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1397  | total loss: \u001b[1m\u001b[32m0.52548\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1397 | loss: 0.52548 - acc: 0.7622 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1398  | total loss: \u001b[1m\u001b[32m0.51811\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1398 | loss: 0.51811 - acc: 0.7670 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1399  | total loss: \u001b[1m\u001b[32m0.51152\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1399 | loss: 0.51152 - acc: 0.7715 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1400  | total loss: \u001b[1m\u001b[32m0.50549\u001b[0m\u001b[0m | time: 0.011s\n",
      "| Adam | epoch: 1400 | loss: 0.50549 - acc: 0.7754 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1401  | total loss: \u001b[1m\u001b[32m0.49977\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1401 | loss: 0.49977 - acc: 0.7791 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1402  | total loss: \u001b[1m\u001b[32m0.53827\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1402 | loss: 0.53827 - acc: 0.7562 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1403  | total loss: \u001b[1m\u001b[32m0.52930\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1403 | loss: 0.52930 - acc: 0.7609 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1404  | total loss: \u001b[1m\u001b[32m0.52135\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1404 | loss: 0.52135 - acc: 0.7649 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1405  | total loss: \u001b[1m\u001b[32m0.51406\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1405 | loss: 0.51406 - acc: 0.7689 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1406  | total loss: \u001b[1m\u001b[32m0.55219\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1406 | loss: 0.55219 - acc: 0.7457 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1407  | total loss: \u001b[1m\u001b[32m0.54233\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1407 | loss: 0.54233 - acc: 0.7520 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1408  | total loss: \u001b[1m\u001b[32m0.53375\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1408 | loss: 0.53375 - acc: 0.7580 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1409  | total loss: \u001b[1m\u001b[32m0.52595\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1409 | loss: 0.52595 - acc: 0.7631 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1410  | total loss: \u001b[1m\u001b[32m0.55406\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1410 | loss: 0.55406 - acc: 0.7430 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1411  | total loss: \u001b[1m\u001b[32m0.54456\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1411 | loss: 0.54456 - acc: 0.7491 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1412  | total loss: \u001b[1m\u001b[32m0.57255\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1412 | loss: 0.57255 - acc: 0.7266 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1413  | total loss: \u001b[1m\u001b[32m0.56201\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1413 | loss: 0.56201 - acc: 0.7343 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1414  | total loss: \u001b[1m\u001b[32m0.55291\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1414 | loss: 0.55291 - acc: 0.7418 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1415  | total loss: \u001b[1m\u001b[32m0.54477\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1415 | loss: 0.54477 - acc: 0.7484 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1416  | total loss: \u001b[1m\u001b[32m0.57021\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1416 | loss: 0.57021 - acc: 0.7268 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1417  | total loss: \u001b[1m\u001b[32m0.56042\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1417 | loss: 0.56042 - acc: 0.7349 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1418  | total loss: \u001b[1m\u001b[32m0.55167\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1418 | loss: 0.55167 - acc: 0.7428 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1419  | total loss: \u001b[1m\u001b[32m0.54363\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1419 | loss: 0.54363 - acc: 0.7496 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1420  | total loss: \u001b[1m\u001b[32m0.57221\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1420 | loss: 0.57221 - acc: 0.7260 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1421  | total loss: \u001b[1m\u001b[32m0.56213\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1421 | loss: 0.56213 - acc: 0.7344 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1422  | total loss: \u001b[1m\u001b[32m0.55315\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1422 | loss: 0.55315 - acc: 0.7422 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1423  | total loss: \u001b[1m\u001b[32m0.54497\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1423 | loss: 0.54497 - acc: 0.7496 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1424  | total loss: \u001b[1m\u001b[32m0.57019\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1424 | loss: 0.57019 - acc: 0.7284 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1425  | total loss: \u001b[1m\u001b[32m0.56037\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1425 | loss: 0.56037 - acc: 0.7368 -- iter: 891/891\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1426  | total loss: \u001b[1m\u001b[32m0.55163\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1426 | loss: 0.55163 - acc: 0.7444 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1427  | total loss: \u001b[1m\u001b[32m0.54368\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1427 | loss: 0.54368 - acc: 0.7515 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1428  | total loss: \u001b[1m\u001b[32m0.57207\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1428 | loss: 0.57207 - acc: 0.7287 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1429  | total loss: \u001b[1m\u001b[32m0.56211\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1429 | loss: 0.56211 - acc: 0.7373 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1430  | total loss: \u001b[1m\u001b[32m0.55324\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1430 | loss: 0.55324 - acc: 0.7448 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1431  | total loss: \u001b[1m\u001b[32m0.54513\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1431 | loss: 0.54513 - acc: 0.7516 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1432  | total loss: \u001b[1m\u001b[32m0.53751\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1432 | loss: 0.53751 - acc: 0.7578 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1433  | total loss: \u001b[1m\u001b[32m0.53022\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1433 | loss: 0.53022 - acc: 0.7636 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1434  | total loss: \u001b[1m\u001b[32m0.56105\u001b[0m\u001b[0m | time: 0.011s\n",
      "| Adam | epoch: 1434 | loss: 0.56105 - acc: 0.7400 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1435  | total loss: \u001b[1m\u001b[32m0.55092\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1435 | loss: 0.55092 - acc: 0.7474 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1436  | total loss: \u001b[1m\u001b[32m0.54167\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1436 | loss: 0.54167 - acc: 0.7537 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1437  | total loss: \u001b[1m\u001b[32m0.53313\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1437 | loss: 0.53313 - acc: 0.7595 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1438  | total loss: \u001b[1m\u001b[32m0.56574\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1438 | loss: 0.56574 - acc: 0.7363 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1439  | total loss: \u001b[1m\u001b[32m0.55471\u001b[0m\u001b[0m | time: 0.011s\n",
      "| Adam | epoch: 1439 | loss: 0.55471 - acc: 0.7437 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1440  | total loss: \u001b[1m\u001b[32m0.58403\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1440 | loss: 0.58403 - acc: 0.7222 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1441  | total loss: \u001b[1m\u001b[32m0.57191\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1441 | loss: 0.57191 - acc: 0.7311 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1442  | total loss: \u001b[1m\u001b[32m0.59680\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1442 | loss: 0.59680 - acc: 0.7098 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1443  | total loss: \u001b[1m\u001b[32m0.58503\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1443 | loss: 0.58503 - acc: 0.7195 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1444  | total loss: \u001b[1m\u001b[32m0.57509\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1444 | loss: 0.57509 - acc: 0.7283 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1445  | total loss: \u001b[1m\u001b[32m0.56622\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1445 | loss: 0.56622 - acc: 0.7360 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1446  | total loss: \u001b[1m\u001b[32m0.58711\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1446 | loss: 0.58711 - acc: 0.7166 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1447  | total loss: \u001b[1m\u001b[32m0.57684\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1447 | loss: 0.57684 - acc: 0.7257 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1448  | total loss: \u001b[1m\u001b[32m0.56740\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1448 | loss: 0.56740 - acc: 0.7343 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1449  | total loss: \u001b[1m\u001b[32m0.55842\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1449 | loss: 0.55842 - acc: 0.7419 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1450  | total loss: \u001b[1m\u001b[32m0.54974\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1450 | loss: 0.54974 - acc: 0.7490 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1451  | total loss: \u001b[1m\u001b[32m0.54141\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1451 | loss: 0.54141 - acc: 0.7558 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1452  | total loss: \u001b[1m\u001b[32m0.57063\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1452 | loss: 0.57063 - acc: 0.7331 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1453  | total loss: \u001b[1m\u001b[32m0.55987\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1453 | loss: 0.55987 - acc: 0.7412 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1454  | total loss: \u001b[1m\u001b[32m0.55015\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1454 | loss: 0.55015 - acc: 0.7483 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1455  | total loss: \u001b[1m\u001b[32m0.54135\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1455 | loss: 0.54135 - acc: 0.7547 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1456  | total loss: \u001b[1m\u001b[32m0.53330\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1456 | loss: 0.53330 - acc: 0.7605 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1457  | total loss: \u001b[1m\u001b[32m0.52585\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1457 | loss: 0.52585 - acc: 0.7657 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1458  | total loss: \u001b[1m\u001b[32m0.56052\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1458 | loss: 0.56052 - acc: 0.7437 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1459  | total loss: \u001b[1m\u001b[32m0.55008\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1459 | loss: 0.55008 - acc: 0.7508 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1460  | total loss: \u001b[1m\u001b[32m0.57786\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1460 | loss: 0.57786 - acc: 0.7295 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1461  | total loss: \u001b[1m\u001b[32m0.56593\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1461 | loss: 0.56593 - acc: 0.7381 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1462  | total loss: \u001b[1m\u001b[32m0.55558\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1462 | loss: 0.55558 - acc: 0.7456 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1463  | total loss: \u001b[1m\u001b[32m0.54657\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1463 | loss: 0.54657 - acc: 0.7523 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1464  | total loss: \u001b[1m\u001b[32m0.53859\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1464 | loss: 0.53859 - acc: 0.7583 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1465  | total loss: \u001b[1m\u001b[32m0.53130\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1465 | loss: 0.53130 - acc: 0.7638 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1466  | total loss: \u001b[1m\u001b[32m0.55819\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1466 | loss: 0.55819 - acc: 0.7435 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1467  | total loss: \u001b[1m\u001b[32m0.54889\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1467 | loss: 0.54889 - acc: 0.7504 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1468  | total loss: \u001b[1m\u001b[32m0.57297\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1468 | loss: 0.57297 - acc: 0.7302 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1469  | total loss: \u001b[1m\u001b[32m0.56283\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1469 | loss: 0.56283 - acc: 0.7385 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1470  | total loss: \u001b[1m\u001b[32m0.58213\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1470 | loss: 0.58213 - acc: 0.7202 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1471  | total loss: \u001b[1m\u001b[32m0.57207\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1471 | loss: 0.57207 - acc: 0.7294 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1472  | total loss: \u001b[1m\u001b[32m0.59142\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1472 | loss: 0.59142 - acc: 0.7096 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1473  | total loss: \u001b[1m\u001b[32m0.58145\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1473 | loss: 0.58145 - acc: 0.7197 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1474  | total loss: \u001b[1m\u001b[32m0.57273\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1474 | loss: 0.57273 - acc: 0.7288 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1475  | total loss: \u001b[1m\u001b[32m0.56472\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1475 | loss: 0.56472 - acc: 0.7367 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1476  | total loss: \u001b[1m\u001b[32m0.55700\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1476 | loss: 0.55700 - acc: 0.7441 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1477  | total loss: \u001b[1m\u001b[32m0.54937\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1477 | loss: 0.54937 - acc: 0.7508 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1478  | total loss: \u001b[1m\u001b[32m0.57077\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1478 | loss: 0.57077 - acc: 0.7328 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1479  | total loss: \u001b[1m\u001b[32m0.56094\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1479 | loss: 0.56094 - acc: 0.7410 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1480  | total loss: \u001b[1m\u001b[32m0.58446\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1480 | loss: 0.58446 - acc: 0.7211 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1481  | total loss: \u001b[1m\u001b[32m0.57327\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1481 | loss: 0.57327 - acc: 0.7303 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1482  | total loss: \u001b[1m\u001b[32m0.56326\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1482 | loss: 0.56326 - acc: 0.7388 -- iter: 891/891\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1483  | total loss: \u001b[1m\u001b[32m0.55415\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1483 | loss: 0.55415 - acc: 0.7465 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1484  | total loss: \u001b[1m\u001b[32m0.54570\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1484 | loss: 0.54570 - acc: 0.7533 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1485  | total loss: \u001b[1m\u001b[32m0.53773\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1485 | loss: 0.53773 - acc: 0.7595 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1486  | total loss: \u001b[1m\u001b[32m0.56775\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1486 | loss: 0.56775 - acc: 0.7361 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1487  | total loss: \u001b[1m\u001b[32m0.55720\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1487 | loss: 0.55720 - acc: 0.7442 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1488  | total loss: \u001b[1m\u001b[32m0.54764\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1488 | loss: 0.54764 - acc: 0.7511 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1489  | total loss: \u001b[1m\u001b[32m0.53888\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1489 | loss: 0.53888 - acc: 0.7576 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1490  | total loss: \u001b[1m\u001b[32m0.56724\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1490 | loss: 0.56724 - acc: 0.7353 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1491  | total loss: \u001b[1m\u001b[32m0.55648\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1491 | loss: 0.55648 - acc: 0.7428 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1492  | total loss: \u001b[1m\u001b[32m0.58218\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1492 | loss: 0.58218 - acc: 0.7211 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1493  | total loss: \u001b[1m\u001b[32m0.57083\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1493 | loss: 0.57083 - acc: 0.7303 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1494  | total loss: \u001b[1m\u001b[32m0.56117\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1494 | loss: 0.56117 - acc: 0.7378 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1495  | total loss: \u001b[1m\u001b[32m0.55266\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1495 | loss: 0.55266 - acc: 0.7450 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1496  | total loss: \u001b[1m\u001b[32m0.57190\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1496 | loss: 0.57190 - acc: 0.7276 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1497  | total loss: \u001b[1m\u001b[32m0.56249\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1497 | loss: 0.56249 - acc: 0.7358 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1498  | total loss: \u001b[1m\u001b[32m0.55396\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1498 | loss: 0.55396 - acc: 0.7432 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1499  | total loss: \u001b[1m\u001b[32m0.54585\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1499 | loss: 0.54585 - acc: 0.7497 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1500  | total loss: \u001b[1m\u001b[32m0.56879\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1500 | loss: 0.56879 - acc: 0.7304 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1501  | total loss: \u001b[1m\u001b[32m0.55860\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1501 | loss: 0.55860 - acc: 0.7384 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1502  | total loss: \u001b[1m\u001b[32m0.54926\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1502 | loss: 0.54926 - acc: 0.7457 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1503  | total loss: \u001b[1m\u001b[32m0.54056\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1503 | loss: 0.54056 - acc: 0.7524 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1504  | total loss: \u001b[1m\u001b[32m0.57054\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1504 | loss: 0.57054 - acc: 0.7301 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1505  | total loss: \u001b[1m\u001b[32m0.55943\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1505 | loss: 0.55943 - acc: 0.7386 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1506  | total loss: \u001b[1m\u001b[32m0.54939\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1506 | loss: 0.54939 - acc: 0.7462 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1507  | total loss: \u001b[1m\u001b[32m0.54022\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1507 | loss: 0.54022 - acc: 0.7531 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1508  | total loss: \u001b[1m\u001b[32m0.53176\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1508 | loss: 0.53176 - acc: 0.7592 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1509  | total loss: \u001b[1m\u001b[32m0.52390\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1509 | loss: 0.52390 - acc: 0.7645 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1510  | total loss: \u001b[1m\u001b[32m0.51661\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1510 | loss: 0.51661 - acc: 0.7691 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1511  | total loss: \u001b[1m\u001b[32m0.50985\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1511 | loss: 0.50985 - acc: 0.7733 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1512  | total loss: \u001b[1m\u001b[32m0.54973\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1512 | loss: 0.54973 - acc: 0.7495 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1513  | total loss: \u001b[1m\u001b[32m0.53945\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1513 | loss: 0.53945 - acc: 0.7552 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1514  | total loss: \u001b[1m\u001b[32m0.53017\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1514 | loss: 0.53017 - acc: 0.7602 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1515  | total loss: \u001b[1m\u001b[32m0.52178\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1515 | loss: 0.52178 - acc: 0.7652 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1516  | total loss: \u001b[1m\u001b[32m0.51421\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1516 | loss: 0.51421 - acc: 0.7695 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1517  | total loss: \u001b[1m\u001b[32m0.50737\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1517 | loss: 0.50737 - acc: 0.7735 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1518  | total loss: \u001b[1m\u001b[32m0.50114\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1518 | loss: 0.50114 - acc: 0.7773 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1519  | total loss: \u001b[1m\u001b[32m0.49542\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1519 | loss: 0.49542 - acc: 0.7800 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1520  | total loss: \u001b[1m\u001b[32m0.53201\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1520 | loss: 0.53201 - acc: 0.7578 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1521  | total loss: \u001b[1m\u001b[32m0.52334\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1521 | loss: 0.52334 - acc: 0.7621 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1522  | total loss: \u001b[1m\u001b[32m0.51567\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1522 | loss: 0.51567 - acc: 0.7661 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1523  | total loss: \u001b[1m\u001b[32m0.50878\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1523 | loss: 0.50878 - acc: 0.7694 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1524  | total loss: \u001b[1m\u001b[32m0.50254\u001b[0m\u001b[0m | time: 0.013s\n",
      "| Adam | epoch: 1524 | loss: 0.50254 - acc: 0.7732 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1525  | total loss: \u001b[1m\u001b[32m0.49681\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1525 | loss: 0.49681 - acc: 0.7769 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1526  | total loss: \u001b[1m\u001b[32m0.53618\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1526 | loss: 0.53618 - acc: 0.7555 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1527  | total loss: \u001b[1m\u001b[32m0.52707\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1527 | loss: 0.52707 - acc: 0.7605 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1528  | total loss: \u001b[1m\u001b[32m0.56379\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1528 | loss: 0.56379 - acc: 0.7374 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1529  | total loss: \u001b[1m\u001b[32m0.55263\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1529 | loss: 0.55263 - acc: 0.7439 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1530  | total loss: \u001b[1m\u001b[32m0.54305\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1530 | loss: 0.54305 - acc: 0.7498 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1531  | total loss: \u001b[1m\u001b[32m0.53466\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1531 | loss: 0.53466 - acc: 0.7557 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1532  | total loss: \u001b[1m\u001b[32m0.52710\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1532 | loss: 0.52710 - acc: 0.7614 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1533  | total loss: \u001b[1m\u001b[32m0.52009\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1533 | loss: 0.52009 - acc: 0.7662 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1534  | total loss: \u001b[1m\u001b[32m0.51350\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1534 | loss: 0.51350 - acc: 0.7706 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1535  | total loss: \u001b[1m\u001b[32m0.50728\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1535 | loss: 0.50728 - acc: 0.7745 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1536  | total loss: \u001b[1m\u001b[32m0.54324\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1536 | loss: 0.54324 - acc: 0.7500 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1537  | total loss: \u001b[1m\u001b[32m0.53390\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1537 | loss: 0.53390 - acc: 0.7559 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1538  | total loss: \u001b[1m\u001b[32m0.56869\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1538 | loss: 0.56869 - acc: 0.7335 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1539  | total loss: \u001b[1m\u001b[32m0.55712\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1539 | loss: 0.55712 - acc: 0.7411 -- iter: 891/891\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1540  | total loss: \u001b[1m\u001b[32m0.54697\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1540 | loss: 0.54697 - acc: 0.7480 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1541  | total loss: \u001b[1m\u001b[32m0.53800\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1541 | loss: 0.53800 - acc: 0.7544 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1542  | total loss: \u001b[1m\u001b[32m0.56733\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1542 | loss: 0.56733 - acc: 0.7310 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1543  | total loss: \u001b[1m\u001b[32m0.55702\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1543 | loss: 0.55702 - acc: 0.7389 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1544  | total loss: \u001b[1m\u001b[32m0.54821\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1544 | loss: 0.54821 - acc: 0.7464 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1545  | total loss: \u001b[1m\u001b[32m0.54041\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1545 | loss: 0.54041 - acc: 0.7530 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1546  | total loss: \u001b[1m\u001b[32m0.56482\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1546 | loss: 0.56482 - acc: 0.7322 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1547  | total loss: \u001b[1m\u001b[32m0.55568\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1547 | loss: 0.55568 - acc: 0.7400 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1548  | total loss: \u001b[1m\u001b[32m0.57678\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1548 | loss: 0.57678 - acc: 0.7198 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1549  | total loss: \u001b[1m\u001b[32m0.56722\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1549 | loss: 0.56722 - acc: 0.7289 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1550  | total loss: \u001b[1m\u001b[32m0.55884\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1550 | loss: 0.55884 - acc: 0.7373 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1551  | total loss: \u001b[1m\u001b[32m0.55115\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1551 | loss: 0.55115 - acc: 0.7448 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1552  | total loss: \u001b[1m\u001b[32m0.54380\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1552 | loss: 0.54380 - acc: 0.7513 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1553  | total loss: \u001b[1m\u001b[32m0.53657\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1553 | loss: 0.53657 - acc: 0.7574 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1554  | total loss: \u001b[1m\u001b[32m0.56675\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1554 | loss: 0.56675 - acc: 0.7336 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1555  | total loss: \u001b[1m\u001b[32m0.55658\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1555 | loss: 0.55658 - acc: 0.7414 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1556  | total loss: \u001b[1m\u001b[32m0.54728\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1556 | loss: 0.54728 - acc: 0.7483 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1557  | total loss: \u001b[1m\u001b[32m0.53869\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1557 | loss: 0.53869 - acc: 0.7545 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1558  | total loss: \u001b[1m\u001b[32m0.53068\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1558 | loss: 0.53068 - acc: 0.7599 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1559  | total loss: \u001b[1m\u001b[32m0.52319\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1559 | loss: 0.52319 - acc: 0.7649 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1560  | total loss: \u001b[1m\u001b[32m0.55615\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1560 | loss: 0.55615 - acc: 0.7424 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1561  | total loss: \u001b[1m\u001b[32m0.54584\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1561 | loss: 0.54584 - acc: 0.7492 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1562  | total loss: \u001b[1m\u001b[32m0.53651\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1562 | loss: 0.53651 - acc: 0.7555 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1563  | total loss: \u001b[1m\u001b[32m0.52803\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1563 | loss: 0.52803 - acc: 0.7611 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1564  | total loss: \u001b[1m\u001b[32m0.52027\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1564 | loss: 0.52027 - acc: 0.7661 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1565  | total loss: \u001b[1m\u001b[32m0.51313\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1565 | loss: 0.51313 - acc: 0.7707 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1566  | total loss: \u001b[1m\u001b[32m0.55145\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1566 | loss: 0.55145 - acc: 0.7477 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1567  | total loss: \u001b[1m\u001b[32m0.54120\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1567 | loss: 0.54120 - acc: 0.7538 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1568  | total loss: \u001b[1m\u001b[32m0.53216\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1568 | loss: 0.53216 - acc: 0.7596 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1569  | total loss: \u001b[1m\u001b[32m0.52411\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1569 | loss: 0.52411 - acc: 0.7646 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1570  | total loss: \u001b[1m\u001b[32m0.51682\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1570 | loss: 0.51682 - acc: 0.7684 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1571  | total loss: \u001b[1m\u001b[32m0.51009\u001b[0m\u001b[0m | time: 0.012s\n",
      "| Adam | epoch: 1571 | loss: 0.51009 - acc: 0.7719 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1572  | total loss: \u001b[1m\u001b[32m0.50377\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1572 | loss: 0.50377 - acc: 0.7751 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1573  | total loss: \u001b[1m\u001b[32m0.49784\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1573 | loss: 0.49784 - acc: 0.7780 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1574  | total loss: \u001b[1m\u001b[32m0.53846\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1574 | loss: 0.53846 - acc: 0.7561 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1575  | total loss: \u001b[1m\u001b[32m0.52894\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1575 | loss: 0.52894 - acc: 0.7614 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1576  | total loss: \u001b[1m\u001b[32m0.52038\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1576 | loss: 0.52038 - acc: 0.7663 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1577  | total loss: \u001b[1m\u001b[32m0.51265\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1577 | loss: 0.51265 - acc: 0.7706 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1578  | total loss: \u001b[1m\u001b[32m0.50568\u001b[0m\u001b[0m | time: 0.011s\n",
      "| Adam | epoch: 1578 | loss: 0.50568 - acc: 0.7739 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1579  | total loss: \u001b[1m\u001b[32m0.49937\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1579 | loss: 0.49937 - acc: 0.7766 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1580  | total loss: \u001b[1m\u001b[32m0.53813\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1580 | loss: 0.53813 - acc: 0.7542 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1581  | total loss: \u001b[1m\u001b[32m0.52864\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1581 | loss: 0.52864 - acc: 0.7594 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1582  | total loss: \u001b[1m\u001b[32m0.56639\u001b[0m\u001b[0m | time: 0.012s\n",
      "| Adam | epoch: 1582 | loss: 0.56639 - acc: 0.7363 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1583  | total loss: \u001b[1m\u001b[32m0.55483\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1583 | loss: 0.55483 - acc: 0.7432 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1584  | total loss: \u001b[1m\u001b[32m0.58415\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1584 | loss: 0.58415 - acc: 0.7223 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1585  | total loss: \u001b[1m\u001b[32m0.57283\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1585 | loss: 0.57283 - acc: 0.7299 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1586  | total loss: \u001b[1m\u001b[32m0.59406\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1586 | loss: 0.59406 - acc: 0.7111 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1587  | total loss: \u001b[1m\u001b[32m0.58423\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1587 | loss: 0.58423 - acc: 0.7197 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1588  | total loss: \u001b[1m\u001b[32m0.57608\u001b[0m\u001b[0m | time: 0.011s\n",
      "| Adam | epoch: 1588 | loss: 0.57608 - acc: 0.7271 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1589  | total loss: \u001b[1m\u001b[32m0.56878\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1589 | loss: 0.56878 - acc: 0.7345 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1590  | total loss: \u001b[1m\u001b[32m0.56177\u001b[0m\u001b[0m | time: 0.011s\n",
      "| Adam | epoch: 1590 | loss: 0.56177 - acc: 0.7411 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1591  | total loss: \u001b[1m\u001b[32m0.55472\u001b[0m\u001b[0m | time: 0.011s\n",
      "| Adam | epoch: 1591 | loss: 0.55472 - acc: 0.7478 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1592  | total loss: \u001b[1m\u001b[32m0.57710\u001b[0m\u001b[0m | time: 0.011s\n",
      "| Adam | epoch: 1592 | loss: 0.57710 - acc: 0.7266 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1593  | total loss: \u001b[1m\u001b[32m0.56759\u001b[0m\u001b[0m | time: 0.011s\n",
      "| Adam | epoch: 1593 | loss: 0.56759 - acc: 0.7351 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1594  | total loss: \u001b[1m\u001b[32m0.58849\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1594 | loss: 0.58849 - acc: 0.7153 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1595  | total loss: \u001b[1m\u001b[32m0.57795\u001b[0m\u001b[0m | time: 0.013s\n",
      "| Adam | epoch: 1595 | loss: 0.57795 - acc: 0.7243 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1596  | total loss: \u001b[1m\u001b[32m0.59703\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1596 | loss: 0.59703 - acc: 0.7056 -- iter: 891/891\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1597  | total loss: \u001b[1m\u001b[32m0.58624\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1597 | loss: 0.58624 - acc: 0.7156 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1598  | total loss: \u001b[1m\u001b[32m0.57679\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1598 | loss: 0.57679 - acc: 0.7244 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1599  | total loss: \u001b[1m\u001b[32m0.56828\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1599 | loss: 0.56828 - acc: 0.7326 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1600  | total loss: \u001b[1m\u001b[32m0.58632\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1600 | loss: 0.58632 - acc: 0.7142 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1601  | total loss: \u001b[1m\u001b[32m0.57686\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1601 | loss: 0.57686 - acc: 0.7232 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1602  | total loss: \u001b[1m\u001b[32m0.59479\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1602 | loss: 0.59479 - acc: 0.7046 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1603  | total loss: \u001b[1m\u001b[32m0.58485\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1603 | loss: 0.58485 - acc: 0.7144 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1604  | total loss: \u001b[1m\u001b[32m0.57606\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1604 | loss: 0.57606 - acc: 0.7237 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1605  | total loss: \u001b[1m\u001b[32m0.56802\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1605 | loss: 0.56802 - acc: 0.7320 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1606  | total loss: \u001b[1m\u001b[32m0.56044\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1606 | loss: 0.56044 - acc: 0.7396 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1607  | total loss: \u001b[1m\u001b[32m0.55307\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1607 | loss: 0.55307 - acc: 0.7464 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1608  | total loss: \u001b[1m\u001b[32m0.57407\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1608 | loss: 0.57407 - acc: 0.7289 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1609  | total loss: \u001b[1m\u001b[32m0.56452\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1609 | loss: 0.56452 - acc: 0.7376 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1610  | total loss: \u001b[1m\u001b[32m0.58390\u001b[0m\u001b[0m | time: 0.011s\n",
      "| Adam | epoch: 1610 | loss: 0.58390 - acc: 0.7193 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1611  | total loss: \u001b[1m\u001b[32m0.57321\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1611 | loss: 0.57321 - acc: 0.7289 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1612  | total loss: \u001b[1m\u001b[32m0.59434\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1612 | loss: 0.59434 - acc: 0.7110 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1613  | total loss: \u001b[1m\u001b[32m0.58307\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1613 | loss: 0.58307 - acc: 0.7214 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1614  | total loss: \u001b[1m\u001b[32m0.57316\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1614 | loss: 0.57316 - acc: 0.7304 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1615  | total loss: \u001b[1m\u001b[32m0.56416\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1615 | loss: 0.56416 - acc: 0.7388 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1616  | total loss: \u001b[1m\u001b[32m0.55569\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1616 | loss: 0.55569 - acc: 0.7464 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1617  | total loss: \u001b[1m\u001b[32m0.54748\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1617 | loss: 0.54748 - acc: 0.7532 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1618  | total loss: \u001b[1m\u001b[32m0.57397\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1618 | loss: 0.57397 - acc: 0.7318 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1619  | total loss: \u001b[1m\u001b[32m0.56319\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1619 | loss: 0.56319 - acc: 0.7402 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1620  | total loss: \u001b[1m\u001b[32m0.58520\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1620 | loss: 0.58520 - acc: 0.7216 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1621  | total loss: \u001b[1m\u001b[32m0.57348\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1621 | loss: 0.57348 - acc: 0.7306 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1622  | total loss: \u001b[1m\u001b[32m0.59516\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1622 | loss: 0.59516 - acc: 0.7130 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1623  | total loss: \u001b[1m\u001b[32m0.58306\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1623 | loss: 0.58306 - acc: 0.7232 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1624  | total loss: \u001b[1m\u001b[32m0.57245\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1624 | loss: 0.57245 - acc: 0.7321 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1625  | total loss: \u001b[1m\u001b[32m0.56292\u001b[0m\u001b[0m | time: 0.011s\n",
      "| Adam | epoch: 1625 | loss: 0.56292 - acc: 0.7402 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1626  | total loss: \u001b[1m\u001b[32m0.58303\u001b[0m\u001b[0m | time: 0.011s\n",
      "| Adam | epoch: 1626 | loss: 0.58303 - acc: 0.7224 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1627  | total loss: \u001b[1m\u001b[32m0.57235\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1627 | loss: 0.57235 - acc: 0.7315 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1628  | total loss: \u001b[1m\u001b[32m0.59153\u001b[0m\u001b[0m | time: 0.012s\n",
      "| Adam | epoch: 1628 | loss: 0.59153 - acc: 0.7128 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1629  | total loss: \u001b[1m\u001b[32m0.58041\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1629 | loss: 0.58041 - acc: 0.7230 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1630  | total loss: \u001b[1m\u001b[32m0.59812\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1630 | loss: 0.59812 - acc: 0.7051 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1631  | total loss: \u001b[1m\u001b[32m0.58714\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1631 | loss: 0.58714 - acc: 0.7159 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1632  | total loss: \u001b[1m\u001b[32m0.57748\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1632 | loss: 0.57748 - acc: 0.7254 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1633  | total loss: \u001b[1m\u001b[32m0.56865\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1633 | loss: 0.56865 - acc: 0.7341 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1634  | total loss: \u001b[1m\u001b[32m0.58969\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1634 | loss: 0.58969 - acc: 0.7122 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1635  | total loss: \u001b[1m\u001b[32m0.57947\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1635 | loss: 0.57947 - acc: 0.7227 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1636  | total loss: \u001b[1m\u001b[32m0.59505\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1636 | loss: 0.59505 - acc: 0.7067 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1637  | total loss: \u001b[1m\u001b[32m0.58452\u001b[0m\u001b[0m | time: 0.011s\n",
      "| Adam | epoch: 1637 | loss: 0.58452 - acc: 0.7175 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1638  | total loss: \u001b[1m\u001b[32m0.60114\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1638 | loss: 0.60114 - acc: 0.7007 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1639  | total loss: \u001b[1m\u001b[32m0.59049\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1639 | loss: 0.59049 - acc: 0.7117 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1640  | total loss: \u001b[1m\u001b[32m0.60577\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1640 | loss: 0.60577 - acc: 0.6946 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1641  | total loss: \u001b[1m\u001b[32m0.59533\u001b[0m\u001b[0m | time: 0.014s\n",
      "| Adam | epoch: 1641 | loss: 0.59533 - acc: 0.7063 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1642  | total loss: \u001b[1m\u001b[32m0.58610\u001b[0m\u001b[0m | time: 0.011s\n",
      "| Adam | epoch: 1642 | loss: 0.58610 - acc: 0.7168 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1643  | total loss: \u001b[1m\u001b[32m0.57760\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1643 | loss: 0.57760 - acc: 0.7266 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1644  | total loss: \u001b[1m\u001b[32m0.56948\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1644 | loss: 0.56948 - acc: 0.7354 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1645  | total loss: \u001b[1m\u001b[32m0.56150\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1645 | loss: 0.56150 - acc: 0.7428 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1646  | total loss: \u001b[1m\u001b[32m0.58019\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1646 | loss: 0.58019 - acc: 0.7247 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1647  | total loss: \u001b[1m\u001b[32m0.57012\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1647 | loss: 0.57012 - acc: 0.7328 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1648  | total loss: \u001b[1m\u001b[32m0.56065\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1648 | loss: 0.56065 - acc: 0.7411 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1649  | total loss: \u001b[1m\u001b[32m0.55167\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1649 | loss: 0.55167 - acc: 0.7487 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1650  | total loss: \u001b[1m\u001b[32m0.54310\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1650 | loss: 0.54310 - acc: 0.7554 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1651  | total loss: \u001b[1m\u001b[32m0.53494\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1651 | loss: 0.53494 - acc: 0.7615 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1652  | total loss: \u001b[1m\u001b[32m0.56642\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1652 | loss: 0.56642 - acc: 0.7394 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1653  | total loss: \u001b[1m\u001b[32m0.55539\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1653 | loss: 0.55539 - acc: 0.7468 -- iter: 891/891\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1654  | total loss: \u001b[1m\u001b[32m0.54528\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1654 | loss: 0.54528 - acc: 0.7532 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1655  | total loss: \u001b[1m\u001b[32m0.53597\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1655 | loss: 0.53597 - acc: 0.7592 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1656  | total loss: \u001b[1m\u001b[32m0.52739\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1656 | loss: 0.52739 - acc: 0.7645 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1657  | total loss: \u001b[1m\u001b[32m0.51947\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1657 | loss: 0.51947 - acc: 0.7692 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1658  | total loss: \u001b[1m\u001b[32m0.51218\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1658 | loss: 0.51218 - acc: 0.7738 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1659  | total loss: \u001b[1m\u001b[32m0.50546\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1659 | loss: 0.50546 - acc: 0.7778 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1660  | total loss: \u001b[1m\u001b[32m0.54662\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1660 | loss: 0.54662 - acc: 0.7542 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1661  | total loss: \u001b[1m\u001b[32m0.53651\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1661 | loss: 0.53651 - acc: 0.7593 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1662  | total loss: \u001b[1m\u001b[32m0.52763\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1662 | loss: 0.52763 - acc: 0.7636 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1663  | total loss: \u001b[1m\u001b[32m0.51979\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1663 | loss: 0.51979 - acc: 0.7678 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1664  | total loss: \u001b[1m\u001b[32m0.55541\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1664 | loss: 0.55541 - acc: 0.7465 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1665  | total loss: \u001b[1m\u001b[32m0.54539\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1665 | loss: 0.54539 - acc: 0.7522 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1666  | total loss: \u001b[1m\u001b[32m0.53664\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1666 | loss: 0.53664 - acc: 0.7573 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1667  | total loss: \u001b[1m\u001b[32m0.52873\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1667 | loss: 0.52873 - acc: 0.7615 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1668  | total loss: \u001b[1m\u001b[32m0.55659\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1668 | loss: 0.55659 - acc: 0.7410 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1669  | total loss: \u001b[1m\u001b[32m0.54682\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1669 | loss: 0.54682 - acc: 0.7471 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1670  | total loss: \u001b[1m\u001b[32m0.53818\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1670 | loss: 0.53818 - acc: 0.7529 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1671  | total loss: \u001b[1m\u001b[32m0.53039\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1671 | loss: 0.53039 - acc: 0.7587 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1672  | total loss: \u001b[1m\u001b[32m0.56137\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1672 | loss: 0.56137 - acc: 0.7370 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1673  | total loss: \u001b[1m\u001b[32m0.55154\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1673 | loss: 0.55154 - acc: 0.7444 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1674  | total loss: \u001b[1m\u001b[32m0.57790\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1674 | loss: 0.57790 - acc: 0.7227 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1675  | total loss: \u001b[1m\u001b[32m0.56741\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1675 | loss: 0.56741 - acc: 0.7315 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1676  | total loss: \u001b[1m\u001b[32m0.55853\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1676 | loss: 0.55853 - acc: 0.7395 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1677  | total loss: \u001b[1m\u001b[32m0.55077\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1677 | loss: 0.55077 - acc: 0.7467 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1678  | total loss: \u001b[1m\u001b[32m0.54372\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1678 | loss: 0.54372 - acc: 0.7531 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1679  | total loss: \u001b[1m\u001b[32m0.53707\u001b[0m\u001b[0m | time: 0.012s\n",
      "| Adam | epoch: 1679 | loss: 0.53707 - acc: 0.7587 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1680  | total loss: \u001b[1m\u001b[32m0.53066\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1680 | loss: 0.53066 - acc: 0.7642 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1681  | total loss: \u001b[1m\u001b[32m0.52440\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1681 | loss: 0.52440 - acc: 0.7691 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1682  | total loss: \u001b[1m\u001b[32m0.51826\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1682 | loss: 0.51826 - acc: 0.7736 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1683  | total loss: \u001b[1m\u001b[32m0.51230\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1683 | loss: 0.51230 - acc: 0.7774 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1684  | total loss: \u001b[1m\u001b[32m0.54626\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1684 | loss: 0.54626 - acc: 0.7548 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1685  | total loss: \u001b[1m\u001b[32m0.53709\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1685 | loss: 0.53709 - acc: 0.7599 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1686  | total loss: \u001b[1m\u001b[32m0.52868\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1686 | loss: 0.52868 - acc: 0.7642 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1687  | total loss: \u001b[1m\u001b[32m0.52090\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1687 | loss: 0.52090 - acc: 0.7683 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1688  | total loss: \u001b[1m\u001b[32m0.51367\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1688 | loss: 0.51367 - acc: 0.7723 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1689  | total loss: \u001b[1m\u001b[32m0.50700\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1689 | loss: 0.50700 - acc: 0.7760 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1690  | total loss: \u001b[1m\u001b[32m0.50083\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1690 | loss: 0.50083 - acc: 0.7795 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1691  | total loss: \u001b[1m\u001b[32m0.49512\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1691 | loss: 0.49512 - acc: 0.7825 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1692  | total loss: \u001b[1m\u001b[32m0.48987\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1692 | loss: 0.48987 - acc: 0.7847 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1693  | total loss: \u001b[1m\u001b[32m0.48511\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1693 | loss: 0.48511 - acc: 0.7866 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1694  | total loss: \u001b[1m\u001b[32m0.53414\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1694 | loss: 0.53414 - acc: 0.7602 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1695  | total loss: \u001b[1m\u001b[32m0.52515\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1695 | loss: 0.52515 - acc: 0.7646 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1696  | total loss: \u001b[1m\u001b[32m0.56792\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1696 | loss: 0.56792 - acc: 0.7403 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1697  | total loss: \u001b[1m\u001b[32m0.55681\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1697 | loss: 0.55681 - acc: 0.7465 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1698  | total loss: \u001b[1m\u001b[32m0.54750\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1698 | loss: 0.54750 - acc: 0.7519 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1699  | total loss: \u001b[1m\u001b[32m0.53922\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1699 | loss: 0.53922 - acc: 0.7568 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1700  | total loss: \u001b[1m\u001b[32m0.57032\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1700 | loss: 0.57032 - acc: 0.7346 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1701  | total loss: \u001b[1m\u001b[32m0.55988\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1701 | loss: 0.55988 - acc: 0.7410 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1702  | total loss: \u001b[1m\u001b[32m0.58369\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1702 | loss: 0.58369 - acc: 0.7220 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1703  | total loss: \u001b[1m\u001b[32m0.57282\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1703 | loss: 0.57282 - acc: 0.7304 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1704  | total loss: \u001b[1m\u001b[32m0.56345\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1704 | loss: 0.56345 - acc: 0.7381 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1705  | total loss: \u001b[1m\u001b[32m0.55509\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1705 | loss: 0.55509 - acc: 0.7452 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1706  | total loss: \u001b[1m\u001b[32m0.57731\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1706 | loss: 0.57731 - acc: 0.7250 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1707  | total loss: \u001b[1m\u001b[32m0.56781\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1707 | loss: 0.56781 - acc: 0.7335 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1708  | total loss: \u001b[1m\u001b[32m0.55941\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1708 | loss: 0.55941 - acc: 0.7414 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1709  | total loss: \u001b[1m\u001b[32m0.55175\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1709 | loss: 0.55175 - acc: 0.7480 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1710  | total loss: \u001b[1m\u001b[32m0.57568\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1710 | loss: 0.57568 - acc: 0.7259 -- iter: 891/891\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1711  | total loss: \u001b[1m\u001b[32m0.56640\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1711 | loss: 0.56640 - acc: 0.7339 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1712  | total loss: \u001b[1m\u001b[32m0.55809\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1712 | loss: 0.55809 - acc: 0.7411 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1713  | total loss: \u001b[1m\u001b[32m0.55045\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1713 | loss: 0.55045 - acc: 0.7476 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1714  | total loss: \u001b[1m\u001b[32m0.57507\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1714 | loss: 0.57507 - acc: 0.7266 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1715  | total loss: \u001b[1m\u001b[32m0.56561\u001b[0m\u001b[0m | time: 0.011s\n",
      "| Adam | epoch: 1715 | loss: 0.56561 - acc: 0.7345 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1716  | total loss: \u001b[1m\u001b[32m0.55710\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1716 | loss: 0.55710 - acc: 0.7421 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1717  | total loss: \u001b[1m\u001b[32m0.54925\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1717 | loss: 0.54925 - acc: 0.7491 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1718  | total loss: \u001b[1m\u001b[32m0.54185\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1718 | loss: 0.54185 - acc: 0.7556 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1719  | total loss: \u001b[1m\u001b[32m0.53474\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1719 | loss: 0.53474 - acc: 0.7615 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1720  | total loss: \u001b[1m\u001b[32m0.52783\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1720 | loss: 0.52783 - acc: 0.7670 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1721  | total loss: \u001b[1m\u001b[32m0.52108\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1721 | loss: 0.52108 - acc: 0.7716 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1722  | total loss: \u001b[1m\u001b[32m0.51453\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1722 | loss: 0.51453 - acc: 0.7760 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1723  | total loss: \u001b[1m\u001b[32m0.50824\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1723 | loss: 0.50824 - acc: 0.7796 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1724  | total loss: \u001b[1m\u001b[32m0.54979\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1724 | loss: 0.54979 - acc: 0.7550 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1725  | total loss: \u001b[1m\u001b[32m0.53957\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1725 | loss: 0.53957 - acc: 0.7605 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1726  | total loss: \u001b[1m\u001b[32m0.57708\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1726 | loss: 0.57708 - acc: 0.7378 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1727  | total loss: \u001b[1m\u001b[32m0.56406\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1727 | loss: 0.56406 - acc: 0.7451 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1728  | total loss: \u001b[1m\u001b[32m0.55260\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1728 | loss: 0.55260 - acc: 0.7520 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1729  | total loss: \u001b[1m\u001b[32m0.54255\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1729 | loss: 0.54255 - acc: 0.7583 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1730  | total loss: \u001b[1m\u001b[32m0.57705\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1730 | loss: 0.57705 - acc: 0.7342 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1731  | total loss: \u001b[1m\u001b[32m0.56540\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1731 | loss: 0.56540 - acc: 0.7412 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1732  | total loss: \u001b[1m\u001b[32m0.59047\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1732 | loss: 0.59047 - acc: 0.7218 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1733  | total loss: \u001b[1m\u001b[32m0.57908\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1733 | loss: 0.57908 - acc: 0.7295 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1734  | total loss: \u001b[1m\u001b[32m0.59833\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1734 | loss: 0.59833 - acc: 0.7109 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1735  | total loss: \u001b[1m\u001b[32m0.58771\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1735 | loss: 0.58771 - acc: 0.7196 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1736  | total loss: \u001b[1m\u001b[32m0.60548\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1736 | loss: 0.60548 - acc: 0.7005 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1737  | total loss: \u001b[1m\u001b[32m0.59536\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1737 | loss: 0.59536 - acc: 0.7108 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1738  | total loss: \u001b[1m\u001b[32m0.58648\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1738 | loss: 0.58648 - acc: 0.7201 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1739  | total loss: \u001b[1m\u001b[32m0.57822\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1739 | loss: 0.57822 - acc: 0.7289 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1740  | total loss: \u001b[1m\u001b[32m0.57020\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1740 | loss: 0.57020 - acc: 0.7368 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1741  | total loss: \u001b[1m\u001b[32m0.56223\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1741 | loss: 0.56223 - acc: 0.7437 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1742  | total loss: \u001b[1m\u001b[32m0.55426\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1742 | loss: 0.55426 - acc: 0.7497 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1743  | total loss: \u001b[1m\u001b[32m0.54637\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1743 | loss: 0.54637 - acc: 0.7554 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1744  | total loss: \u001b[1m\u001b[32m0.57204\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1744 | loss: 0.57204 - acc: 0.7337 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1745  | total loss: \u001b[1m\u001b[32m0.56183\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1745 | loss: 0.56183 - acc: 0.7407 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1746  | total loss: \u001b[1m\u001b[32m0.59151\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1746 | loss: 0.59151 - acc: 0.7193 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1747  | total loss: \u001b[1m\u001b[32m0.57953\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1747 | loss: 0.57953 - acc: 0.7282 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1748  | total loss: \u001b[1m\u001b[32m0.60851\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1748 | loss: 0.60851 - acc: 0.7046 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1749  | total loss: \u001b[1m\u001b[32m0.59518\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1749 | loss: 0.59518 - acc: 0.7153 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1750  | total loss: \u001b[1m\u001b[32m0.58340\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1750 | loss: 0.58340 - acc: 0.7248 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1751  | total loss: \u001b[1m\u001b[32m0.57294\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1751 | loss: 0.57294 - acc: 0.7328 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1752  | total loss: \u001b[1m\u001b[32m0.56360\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1752 | loss: 0.56360 - acc: 0.7403 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1753  | total loss: \u001b[1m\u001b[32m0.55509\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1753 | loss: 0.55509 - acc: 0.7468 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1754  | total loss: \u001b[1m\u001b[32m0.57816\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1754 | loss: 0.57816 - acc: 0.7275 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1755  | total loss: \u001b[1m\u001b[32m0.56798\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1755 | loss: 0.56798 - acc: 0.7360 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1756  | total loss: \u001b[1m\u001b[32m0.58771\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1756 | loss: 0.58771 - acc: 0.7165 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1757  | total loss: \u001b[1m\u001b[32m0.57744\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1757 | loss: 0.57744 - acc: 0.7262 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1758  | total loss: \u001b[1m\u001b[32m0.59649\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1758 | loss: 0.59649 - acc: 0.7061 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1759  | total loss: \u001b[1m\u001b[32m0.58664\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1759 | loss: 0.58664 - acc: 0.7168 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1760  | total loss: \u001b[1m\u001b[32m0.57821\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1760 | loss: 0.57821 - acc: 0.7257 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1761  | total loss: \u001b[1m\u001b[32m0.57049\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1761 | loss: 0.57049 - acc: 0.7342 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1762  | total loss: \u001b[1m\u001b[32m0.56290\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1762 | loss: 0.56290 - acc: 0.7417 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1763  | total loss: \u001b[1m\u001b[32m0.55506\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1763 | loss: 0.55506 - acc: 0.7486 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1764  | total loss: \u001b[1m\u001b[32m0.58027\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1764 | loss: 0.58027 - acc: 0.7257 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1765  | total loss: \u001b[1m\u001b[32m0.56940\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1765 | loss: 0.56940 - acc: 0.7344 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1766  | total loss: \u001b[1m\u001b[32m0.55932\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1766 | loss: 0.55932 - acc: 0.7424 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1767  | total loss: \u001b[1m\u001b[32m0.54986\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1767 | loss: 0.54986 - acc: 0.7491 -- iter: 891/891\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1768  | total loss: \u001b[1m\u001b[32m0.54094\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1768 | loss: 0.54094 - acc: 0.7556 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1769  | total loss: \u001b[1m\u001b[32m0.53258\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1769 | loss: 0.53258 - acc: 0.7615 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1770  | total loss: \u001b[1m\u001b[32m0.52484\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1770 | loss: 0.52484 - acc: 0.7665 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1771  | total loss: \u001b[1m\u001b[32m0.51773\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1771 | loss: 0.51773 - acc: 0.7711 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1772  | total loss: \u001b[1m\u001b[32m0.51119\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1772 | loss: 0.51119 - acc: 0.7754 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1773  | total loss: \u001b[1m\u001b[32m0.50521\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1773 | loss: 0.50521 - acc: 0.7788 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1774  | total loss: \u001b[1m\u001b[32m0.49975\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1774 | loss: 0.49975 - acc: 0.7811 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1775  | total loss: \u001b[1m\u001b[32m0.49469\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1775 | loss: 0.49469 - acc: 0.7832 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1776  | total loss: \u001b[1m\u001b[32m0.54356\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1776 | loss: 0.54356 - acc: 0.7611 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1777  | total loss: \u001b[1m\u001b[32m0.53359\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1777 | loss: 0.53359 - acc: 0.7657 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1778  | total loss: \u001b[1m\u001b[32m0.52456\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1778 | loss: 0.52456 - acc: 0.7701 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1779  | total loss: \u001b[1m\u001b[32m0.51664\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1779 | loss: 0.51664 - acc: 0.7744 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1780  | total loss: \u001b[1m\u001b[32m0.55514\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1780 | loss: 0.55514 - acc: 0.7525 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1781  | total loss: \u001b[1m\u001b[32m0.54542\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1781 | loss: 0.54542 - acc: 0.7578 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1782  | total loss: \u001b[1m\u001b[32m0.53740\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1782 | loss: 0.53740 - acc: 0.7622 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1783  | total loss: \u001b[1m\u001b[32m0.53023\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1783 | loss: 0.53023 - acc: 0.7662 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1784  | total loss: \u001b[1m\u001b[32m0.56059\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1784 | loss: 0.56059 - acc: 0.7433 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1785  | total loss: \u001b[1m\u001b[32m0.55095\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1785 | loss: 0.55095 - acc: 0.7497 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1786  | total loss: \u001b[1m\u001b[32m0.57709\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1786 | loss: 0.57709 - acc: 0.7282 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1787  | total loss: \u001b[1m\u001b[32m0.56633\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1787 | loss: 0.56633 - acc: 0.7363 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1788  | total loss: \u001b[1m\u001b[32m0.55694\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1788 | loss: 0.55694 - acc: 0.7439 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1789  | total loss: \u001b[1m\u001b[32m0.54851\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1789 | loss: 0.54851 - acc: 0.7505 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1790  | total loss: \u001b[1m\u001b[32m0.56930\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1790 | loss: 0.56930 - acc: 0.7308 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1791  | total loss: \u001b[1m\u001b[32m0.55983\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1791 | loss: 0.55983 - acc: 0.7389 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1792  | total loss: \u001b[1m\u001b[32m0.55142\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1792 | loss: 0.55142 - acc: 0.7464 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1793  | total loss: \u001b[1m\u001b[32m0.54380\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1793 | loss: 0.54380 - acc: 0.7526 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1794  | total loss: \u001b[1m\u001b[32m0.53676\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1794 | loss: 0.53676 - acc: 0.7580 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1795  | total loss: \u001b[1m\u001b[32m0.53016\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1795 | loss: 0.53016 - acc: 0.7629 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1796  | total loss: \u001b[1m\u001b[32m0.52391\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1796 | loss: 0.52391 - acc: 0.7669 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1797  | total loss: \u001b[1m\u001b[32m0.51796\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1797 | loss: 0.51796 - acc: 0.7708 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1798  | total loss: \u001b[1m\u001b[32m0.51232\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1798 | loss: 0.51232 - acc: 0.7747 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1799  | total loss: \u001b[1m\u001b[32m0.50702\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1799 | loss: 0.50702 - acc: 0.7785 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1800  | total loss: \u001b[1m\u001b[32m0.50204\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1800 | loss: 0.50204 - acc: 0.7812 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1801  | total loss: \u001b[1m\u001b[32m0.49732\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1801 | loss: 0.49732 - acc: 0.7840 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1802  | total loss: \u001b[1m\u001b[32m0.49285\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1802 | loss: 0.49285 - acc: 0.7869 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1803  | total loss: \u001b[1m\u001b[32m0.48858\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1803 | loss: 0.48858 - acc: 0.7896 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1804  | total loss: \u001b[1m\u001b[32m0.48446\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1804 | loss: 0.48446 - acc: 0.7919 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1805  | total loss: \u001b[1m\u001b[32m0.48047\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1805 | loss: 0.48047 - acc: 0.7937 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1806  | total loss: \u001b[1m\u001b[32m0.47669\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1806 | loss: 0.47669 - acc: 0.7948 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1807  | total loss: \u001b[1m\u001b[32m0.47325\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1807 | loss: 0.47325 - acc: 0.7958 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1808  | total loss: \u001b[1m\u001b[32m0.47034\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1808 | loss: 0.47034 - acc: 0.7965 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1809  | total loss: \u001b[1m\u001b[32m0.46795\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1809 | loss: 0.46795 - acc: 0.7975 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1810  | total loss: \u001b[1m\u001b[32m0.51827\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1810 | loss: 0.51827 - acc: 0.7714 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1811  | total loss: \u001b[1m\u001b[32m0.51185\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1811 | loss: 0.51185 - acc: 0.7745 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1812  | total loss: \u001b[1m\u001b[32m0.50622\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1812 | loss: 0.50622 - acc: 0.7772 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1813  | total loss: \u001b[1m\u001b[32m0.50087\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1813 | loss: 0.50087 - acc: 0.7796 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1814  | total loss: \u001b[1m\u001b[32m0.49579\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1814 | loss: 0.49579 - acc: 0.7828 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1815  | total loss: \u001b[1m\u001b[32m0.49094\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1815 | loss: 0.49094 - acc: 0.7858 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1816  | total loss: \u001b[1m\u001b[32m0.53156\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1816 | loss: 0.53156 - acc: 0.7618 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1817  | total loss: \u001b[1m\u001b[32m0.52333\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1817 | loss: 0.52333 - acc: 0.7666 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1818  | total loss: \u001b[1m\u001b[32m0.51618\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1818 | loss: 0.51618 - acc: 0.7705 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1819  | total loss: \u001b[1m\u001b[32m0.50986\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1819 | loss: 0.50986 - acc: 0.7741 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1820  | total loss: \u001b[1m\u001b[32m0.54681\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1820 | loss: 0.54681 - acc: 0.7495 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1821  | total loss: \u001b[1m\u001b[32m0.53791\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1821 | loss: 0.53791 - acc: 0.7555 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1822  | total loss: \u001b[1m\u001b[32m0.53024\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1822 | loss: 0.53024 - acc: 0.7608 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1823  | total loss: \u001b[1m\u001b[32m0.52354\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1823 | loss: 0.52354 - acc: 0.7660 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1824  | total loss: \u001b[1m\u001b[32m0.54989\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1824 | loss: 0.54989 - acc: 0.7450 -- iter: 891/891\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1825  | total loss: \u001b[1m\u001b[32m0.54182\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1825 | loss: 0.54182 - acc: 0.7516 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1826  | total loss: \u001b[1m\u001b[32m0.56661\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1826 | loss: 0.56661 - acc: 0.7283 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1827  | total loss: \u001b[1m\u001b[32m0.55832\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1827 | loss: 0.55832 - acc: 0.7366 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1828  | total loss: \u001b[1m\u001b[32m0.57757\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1828 | loss: 0.57757 - acc: 0.7183 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1829  | total loss: \u001b[1m\u001b[32m0.57005\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1829 | loss: 0.57005 - acc: 0.7275 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1830  | total loss: \u001b[1m\u001b[32m0.56386\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1830 | loss: 0.56386 - acc: 0.7353 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1831  | total loss: \u001b[1m\u001b[32m0.55822\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1831 | loss: 0.55822 - acc: 0.7419 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1832  | total loss: \u001b[1m\u001b[32m0.57658\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1832 | loss: 0.57658 - acc: 0.7217 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1833  | total loss: \u001b[1m\u001b[32m0.56910\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1833 | loss: 0.56910 - acc: 0.7306 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1834  | total loss: \u001b[1m\u001b[32m0.58608\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1834 | loss: 0.58608 - acc: 0.7113 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1835  | total loss: \u001b[1m\u001b[32m0.57735\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1835 | loss: 0.57735 - acc: 0.7213 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1836  | total loss: \u001b[1m\u001b[32m0.56922\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1836 | loss: 0.56922 - acc: 0.7306 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1837  | total loss: \u001b[1m\u001b[32m0.56122\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1837 | loss: 0.56122 - acc: 0.7387 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1838  | total loss: \u001b[1m\u001b[32m0.58025\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1838 | loss: 0.58025 - acc: 0.7197 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1839  | total loss: \u001b[1m\u001b[32m0.57009\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1839 | loss: 0.57009 - acc: 0.7289 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1840  | total loss: \u001b[1m\u001b[32m0.56059\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1840 | loss: 0.56059 - acc: 0.7374 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1841  | total loss: \u001b[1m\u001b[32m0.55156\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1841 | loss: 0.55156 - acc: 0.7451 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1842  | total loss: \u001b[1m\u001b[32m0.54295\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1842 | loss: 0.54295 - acc: 0.7516 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1843  | total loss: \u001b[1m\u001b[32m0.53474\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1843 | loss: 0.53474 - acc: 0.7577 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1844  | total loss: \u001b[1m\u001b[32m0.56683\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1844 | loss: 0.56683 - acc: 0.7363 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1845  | total loss: \u001b[1m\u001b[32m0.55579\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1845 | loss: 0.55579 - acc: 0.7441 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1846  | total loss: \u001b[1m\u001b[32m0.54572\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1846 | loss: 0.54572 - acc: 0.7508 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1847  | total loss: \u001b[1m\u001b[32m0.53647\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1847 | loss: 0.53647 - acc: 0.7568 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1848  | total loss: \u001b[1m\u001b[32m0.57097\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1848 | loss: 0.57097 - acc: 0.7347 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1849  | total loss: \u001b[1m\u001b[32m0.55904\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1849 | loss: 0.55904 - acc: 0.7427 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1850  | total loss: \u001b[1m\u001b[32m0.58746\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1850 | loss: 0.58746 - acc: 0.7226 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1851  | total loss: \u001b[1m\u001b[32m0.57467\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1851 | loss: 0.57467 - acc: 0.7319 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1852  | total loss: \u001b[1m\u001b[32m0.56383\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1852 | loss: 0.56383 - acc: 0.7399 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1853  | total loss: \u001b[1m\u001b[32m0.55449\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1853 | loss: 0.55449 - acc: 0.7475 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1854  | total loss: \u001b[1m\u001b[32m0.54613\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1854 | loss: 0.54613 - acc: 0.7540 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1855  | total loss: \u001b[1m\u001b[32m0.53827\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1855 | loss: 0.53827 - acc: 0.7593 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1856  | total loss: \u001b[1m\u001b[32m0.53060\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1856 | loss: 0.53060 - acc: 0.7649 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1857  | total loss: \u001b[1m\u001b[32m0.52304\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1857 | loss: 0.52304 - acc: 0.7698 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1858  | total loss: \u001b[1m\u001b[32m0.55895\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1858 | loss: 0.55895 - acc: 0.7461 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1859  | total loss: \u001b[1m\u001b[32m0.54801\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1859 | loss: 0.54801 - acc: 0.7527 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1860  | total loss: \u001b[1m\u001b[32m0.53810\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1860 | loss: 0.53810 - acc: 0.7587 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1861  | total loss: \u001b[1m\u001b[32m0.52911\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1861 | loss: 0.52911 - acc: 0.7639 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1862  | total loss: \u001b[1m\u001b[32m0.56331\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1862 | loss: 0.56331 - acc: 0.7428 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1863  | total loss: \u001b[1m\u001b[32m0.55187\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1863 | loss: 0.55187 - acc: 0.7492 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1864  | total loss: \u001b[1m\u001b[32m0.58413\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1864 | loss: 0.58413 - acc: 0.7276 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1865  | total loss: \u001b[1m\u001b[32m0.57126\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1865 | loss: 0.57126 - acc: 0.7360 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1866  | total loss: \u001b[1m\u001b[32m0.59173\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1866 | loss: 0.59173 - acc: 0.7196 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1867  | total loss: \u001b[1m\u001b[32m0.57970\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1867 | loss: 0.57970 - acc: 0.7287 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1868  | total loss: \u001b[1m\u001b[32m0.60063\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1868 | loss: 0.60063 - acc: 0.7080 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1869  | total loss: \u001b[1m\u001b[32m0.58982\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1869 | loss: 0.58982 - acc: 0.7175 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1870  | total loss: \u001b[1m\u001b[32m0.60743\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1870 | loss: 0.60743 - acc: 0.6980 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1871  | total loss: \u001b[1m\u001b[32m0.59791\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1871 | loss: 0.59791 - acc: 0.7082 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1872  | total loss: \u001b[1m\u001b[32m0.61191\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1872 | loss: 0.61191 - acc: 0.6914 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1873  | total loss: \u001b[1m\u001b[32m0.60328\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1873 | loss: 0.60328 - acc: 0.7022 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1874  | total loss: \u001b[1m\u001b[32m0.61396\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1874 | loss: 0.61396 - acc: 0.6881 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1875  | total loss: \u001b[1m\u001b[32m0.60569\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1875 | loss: 0.60569 - acc: 0.6993 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1876  | total loss: \u001b[1m\u001b[32m0.61541\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1876 | loss: 0.61541 - acc: 0.6829 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1877  | total loss: \u001b[1m\u001b[32m0.60708\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1877 | loss: 0.60708 - acc: 0.6945 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1878  | total loss: \u001b[1m\u001b[32m0.61786\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1878 | loss: 0.61786 - acc: 0.6807 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1879  | total loss: \u001b[1m\u001b[32m0.60915\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1879 | loss: 0.60915 - acc: 0.6921 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1880  | total loss: \u001b[1m\u001b[32m0.61860\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1880 | loss: 0.61860 - acc: 0.6786 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1881  | total loss: \u001b[1m\u001b[32m0.60956\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1881 | loss: 0.60956 - acc: 0.6902 -- iter: 891/891\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1882  | total loss: \u001b[1m\u001b[32m0.60115\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1882 | loss: 0.60115 - acc: 0.6997 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1883  | total loss: \u001b[1m\u001b[32m0.59304\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1883 | loss: 0.59304 - acc: 0.7075 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1884  | total loss: \u001b[1m\u001b[32m0.60725\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1884 | loss: 0.60725 - acc: 0.6918 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1885  | total loss: \u001b[1m\u001b[32m0.59758\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1885 | loss: 0.59758 - acc: 0.7005 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1886  | total loss: \u001b[1m\u001b[32m0.58845\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1886 | loss: 0.58845 - acc: 0.7081 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1887  | total loss: \u001b[1m\u001b[32m0.57966\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1887 | loss: 0.57966 - acc: 0.7148 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1888  | total loss: \u001b[1m\u001b[32m0.59490\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1888 | loss: 0.59490 - acc: 0.7022 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1889  | total loss: \u001b[1m\u001b[32m0.58458\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1889 | loss: 0.58458 - acc: 0.7103 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1890  | total loss: \u001b[1m\u001b[32m0.57492\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1890 | loss: 0.57492 - acc: 0.7178 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1891  | total loss: \u001b[1m\u001b[32m0.56578\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1891 | loss: 0.56578 - acc: 0.7248 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1892  | total loss: \u001b[1m\u001b[32m0.55705\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1892 | loss: 0.55705 - acc: 0.7317 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1893  | total loss: \u001b[1m\u001b[32m0.54868\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1893 | loss: 0.54868 - acc: 0.7379 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1894  | total loss: \u001b[1m\u001b[32m0.54065\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1894 | loss: 0.54065 - acc: 0.7447 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1895  | total loss: \u001b[1m\u001b[32m0.53296\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1895 | loss: 0.53296 - acc: 0.7515 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1896  | total loss: \u001b[1m\u001b[32m0.52562\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1896 | loss: 0.52562 - acc: 0.7571 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1897  | total loss: \u001b[1m\u001b[32m0.51865\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1897 | loss: 0.51865 - acc: 0.7627 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1898  | total loss: \u001b[1m\u001b[32m0.56365\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1898 | loss: 0.56365 - acc: 0.7375 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1899  | total loss: \u001b[1m\u001b[32m0.55229\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1899 | loss: 0.55229 - acc: 0.7443 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1900  | total loss: \u001b[1m\u001b[32m0.58905\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1900 | loss: 0.58905 - acc: 0.7220 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1901  | total loss: \u001b[1m\u001b[32m0.57536\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1901 | loss: 0.57536 - acc: 0.7309 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1902  | total loss: \u001b[1m\u001b[32m0.56390\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1902 | loss: 0.56390 - acc: 0.7393 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1903  | total loss: \u001b[1m\u001b[32m0.55413\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1903 | loss: 0.55413 - acc: 0.7462 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1904  | total loss: \u001b[1m\u001b[32m0.58021\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1904 | loss: 0.58021 - acc: 0.7272 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1905  | total loss: \u001b[1m\u001b[32m0.56921\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1905 | loss: 0.56921 - acc: 0.7353 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1906  | total loss: \u001b[1m\u001b[32m0.59326\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1906 | loss: 0.59326 - acc: 0.7153 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1907  | total loss: \u001b[1m\u001b[32m0.58135\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1907 | loss: 0.58135 - acc: 0.7245 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1908  | total loss: \u001b[1m\u001b[32m0.57070\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1908 | loss: 0.57070 - acc: 0.7329 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1909  | total loss: \u001b[1m\u001b[32m0.56091\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1909 | loss: 0.56091 - acc: 0.7407 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1910  | total loss: \u001b[1m\u001b[32m0.58211\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1910 | loss: 0.58211 - acc: 0.7213 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1911  | total loss: \u001b[1m\u001b[32m0.57122\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1911 | loss: 0.57122 - acc: 0.7304 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1912  | total loss: \u001b[1m\u001b[32m0.56156\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1912 | loss: 0.56156 - acc: 0.7387 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1913  | total loss: \u001b[1m\u001b[32m0.55282\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1913 | loss: 0.55282 - acc: 0.7460 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1914  | total loss: \u001b[1m\u001b[32m0.54475\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1914 | loss: 0.54475 - acc: 0.7523 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1915  | total loss: \u001b[1m\u001b[32m0.53722\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1915 | loss: 0.53722 - acc: 0.7575 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1916  | total loss: \u001b[1m\u001b[32m0.53015\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1916 | loss: 0.53015 - acc: 0.7614 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1917  | total loss: \u001b[1m\u001b[32m0.52348\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1917 | loss: 0.52348 - acc: 0.7651 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1918  | total loss: \u001b[1m\u001b[32m0.51719\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1918 | loss: 0.51719 - acc: 0.7689 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1919  | total loss: \u001b[1m\u001b[32m0.51126\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1919 | loss: 0.51126 - acc: 0.7730 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1920  | total loss: \u001b[1m\u001b[32m0.55103\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1920 | loss: 0.55103 - acc: 0.7504 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1921  | total loss: \u001b[1m\u001b[32m0.54121\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1921 | loss: 0.54121 - acc: 0.7555 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1922  | total loss: \u001b[1m\u001b[32m0.53210\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1922 | loss: 0.53210 - acc: 0.7603 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1923  | total loss: \u001b[1m\u001b[32m0.52368\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1923 | loss: 0.52368 - acc: 0.7651 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1924  | total loss: \u001b[1m\u001b[32m0.51599\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1924 | loss: 0.51599 - acc: 0.7698 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1925  | total loss: \u001b[1m\u001b[32m0.50902\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1925 | loss: 0.50902 - acc: 0.7735 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1926  | total loss: \u001b[1m\u001b[32m0.54669\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1926 | loss: 0.54669 - acc: 0.7502 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1927  | total loss: \u001b[1m\u001b[32m0.53709\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1927 | loss: 0.53709 - acc: 0.7554 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1928  | total loss: \u001b[1m\u001b[32m0.56845\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1928 | loss: 0.56845 - acc: 0.7346 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1929  | total loss: \u001b[1m\u001b[32m0.55844\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1929 | loss: 0.55844 - acc: 0.7411 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1930  | total loss: \u001b[1m\u001b[32m0.58327\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1930 | loss: 0.58327 - acc: 0.7226 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1931  | total loss: \u001b[1m\u001b[32m0.57355\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1931 | loss: 0.57355 - acc: 0.7301 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1932  | total loss: \u001b[1m\u001b[32m0.56501\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1932 | loss: 0.56501 - acc: 0.7370 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1933  | total loss: \u001b[1m\u001b[32m0.55690\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1933 | loss: 0.55690 - acc: 0.7431 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1934  | total loss: \u001b[1m\u001b[32m0.54886\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1934 | loss: 0.54886 - acc: 0.7495 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1935  | total loss: \u001b[1m\u001b[32m0.54090\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1935 | loss: 0.54090 - acc: 0.7557 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1936  | total loss: \u001b[1m\u001b[32m0.53321\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1936 | loss: 0.53321 - acc: 0.7612 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1937  | total loss: \u001b[1m\u001b[32m0.52598\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1937 | loss: 0.52598 - acc: 0.7666 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1938  | total loss: \u001b[1m\u001b[32m0.51936\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1938 | loss: 0.51936 - acc: 0.7707 -- iter: 891/891\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1939  | total loss: \u001b[1m\u001b[32m0.51336\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1939 | loss: 0.51336 - acc: 0.7745 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1940  | total loss: \u001b[1m\u001b[32m0.55442\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1940 | loss: 0.55442 - acc: 0.7496 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1941  | total loss: \u001b[1m\u001b[32m0.54506\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1941 | loss: 0.54506 - acc: 0.7547 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1942  | total loss: \u001b[1m\u001b[32m0.53667\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1942 | loss: 0.53667 - acc: 0.7584 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1943  | total loss: \u001b[1m\u001b[32m0.52885\u001b[0m\u001b[0m | time: 0.012s\n",
      "| Adam | epoch: 1943 | loss: 0.52885 - acc: 0.7626 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1944  | total loss: \u001b[1m\u001b[32m0.52151\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1944 | loss: 0.52151 - acc: 0.7676 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1945  | total loss: \u001b[1m\u001b[32m0.51480\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1945 | loss: 0.51480 - acc: 0.7721 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1946  | total loss: \u001b[1m\u001b[32m0.54980\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1946 | loss: 0.54980 - acc: 0.7500 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1947  | total loss: \u001b[1m\u001b[32m0.54025\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1947 | loss: 0.54025 - acc: 0.7563 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1948  | total loss: \u001b[1m\u001b[32m0.53200\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1948 | loss: 0.53200 - acc: 0.7617 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1949  | total loss: \u001b[1m\u001b[32m0.52507\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1949 | loss: 0.52507 - acc: 0.7659 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1950  | total loss: \u001b[1m\u001b[32m0.51889\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1950 | loss: 0.51889 - acc: 0.7698 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1951  | total loss: \u001b[1m\u001b[32m0.51280\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1951 | loss: 0.51280 - acc: 0.7732 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1952  | total loss: \u001b[1m\u001b[32m0.50671\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1952 | loss: 0.50671 - acc: 0.7771 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1953  | total loss: \u001b[1m\u001b[32m0.50080\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1953 | loss: 0.50080 - acc: 0.7806 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1954  | total loss: \u001b[1m\u001b[32m0.49524\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1954 | loss: 0.49524 - acc: 0.7841 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1955  | total loss: \u001b[1m\u001b[32m0.49020\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 1955 | loss: 0.49020 - acc: 0.7865 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1956  | total loss: \u001b[1m\u001b[32m0.48581\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1956 | loss: 0.48581 - acc: 0.7881 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1957  | total loss: \u001b[1m\u001b[32m0.48200\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1957 | loss: 0.48200 - acc: 0.7892 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1958  | total loss: \u001b[1m\u001b[32m0.47860\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1958 | loss: 0.47860 - acc: 0.7906 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1959  | total loss: \u001b[1m\u001b[32m0.47548\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1959 | loss: 0.47548 - acc: 0.7924 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1960  | total loss: \u001b[1m\u001b[32m0.47255\u001b[0m\u001b[0m | time: 0.012s\n",
      "| Adam | epoch: 1960 | loss: 0.47255 - acc: 0.7941 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1961  | total loss: \u001b[1m\u001b[32m0.46969\u001b[0m\u001b[0m | time: 0.012s\n",
      "| Adam | epoch: 1961 | loss: 0.46969 - acc: 0.7953 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1962  | total loss: \u001b[1m\u001b[32m0.46695\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1962 | loss: 0.46695 - acc: 0.7959 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1963  | total loss: \u001b[1m\u001b[32m0.46445\u001b[0m\u001b[0m | time: 0.013s\n",
      "| Adam | epoch: 1963 | loss: 0.46445 - acc: 0.7966 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1964  | total loss: \u001b[1m\u001b[32m0.46227\u001b[0m\u001b[0m | time: 0.011s\n",
      "| Adam | epoch: 1964 | loss: 0.46227 - acc: 0.7972 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1965  | total loss: \u001b[1m\u001b[32m0.46043\u001b[0m\u001b[0m | time: 0.011s\n",
      "| Adam | epoch: 1965 | loss: 0.46043 - acc: 0.7975 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1966  | total loss: \u001b[1m\u001b[32m0.51429\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1966 | loss: 0.51429 - acc: 0.7691 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1967  | total loss: \u001b[1m\u001b[32m0.50785\u001b[0m\u001b[0m | time: 0.011s\n",
      "| Adam | epoch: 1967 | loss: 0.50785 - acc: 0.7724 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1968  | total loss: \u001b[1m\u001b[32m0.55227\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1968 | loss: 0.55227 - acc: 0.7452 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1969  | total loss: \u001b[1m\u001b[32m0.54365\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1969 | loss: 0.54365 - acc: 0.7507 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1970  | total loss: \u001b[1m\u001b[32m0.57212\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1970 | loss: 0.57212 - acc: 0.7289 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1971  | total loss: \u001b[1m\u001b[32m0.56339\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1971 | loss: 0.56339 - acc: 0.7357 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1972  | total loss: \u001b[1m\u001b[32m0.55599\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1972 | loss: 0.55599 - acc: 0.7422 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1973  | total loss: \u001b[1m\u001b[32m0.54924\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1973 | loss: 0.54924 - acc: 0.7484 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1974  | total loss: \u001b[1m\u001b[32m0.54276\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1974 | loss: 0.54276 - acc: 0.7545 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1975  | total loss: \u001b[1m\u001b[32m0.53643\u001b[0m\u001b[0m | time: 0.011s\n",
      "| Adam | epoch: 1975 | loss: 0.53643 - acc: 0.7603 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1976  | total loss: \u001b[1m\u001b[32m0.53026\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1976 | loss: 0.53026 - acc: 0.7658 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1977  | total loss: \u001b[1m\u001b[32m0.52434\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1977 | loss: 0.52434 - acc: 0.7700 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1978  | total loss: \u001b[1m\u001b[32m0.55353\u001b[0m\u001b[0m | time: 0.011s\n",
      "| Adam | epoch: 1978 | loss: 0.55353 - acc: 0.7468 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1979  | total loss: \u001b[1m\u001b[32m0.54519\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1979 | loss: 0.54519 - acc: 0.7524 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1980  | total loss: \u001b[1m\u001b[32m0.53776\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1980 | loss: 0.53776 - acc: 0.7580 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1981  | total loss: \u001b[1m\u001b[32m0.53102\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1981 | loss: 0.53102 - acc: 0.7628 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1982  | total loss: \u001b[1m\u001b[32m0.56213\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1982 | loss: 0.56213 - acc: 0.7413 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1983  | total loss: \u001b[1m\u001b[32m0.55295\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1983 | loss: 0.55295 - acc: 0.7478 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1984  | total loss: \u001b[1m\u001b[32m0.54475\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1984 | loss: 0.54475 - acc: 0.7541 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1985  | total loss: \u001b[1m\u001b[32m0.53732\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1985 | loss: 0.53732 - acc: 0.7596 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1986  | total loss: \u001b[1m\u001b[32m0.56292\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1986 | loss: 0.56292 - acc: 0.7388 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1987  | total loss: \u001b[1m\u001b[32m0.55388\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1987 | loss: 0.55388 - acc: 0.7466 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1988  | total loss: \u001b[1m\u001b[32m0.54603\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1988 | loss: 0.54603 - acc: 0.7534 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1989  | total loss: \u001b[1m\u001b[32m0.53906\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1989 | loss: 0.53906 - acc: 0.7595 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1990  | total loss: \u001b[1m\u001b[32m0.56574\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1990 | loss: 0.56574 - acc: 0.7356 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1991  | total loss: \u001b[1m\u001b[32m0.55732\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1991 | loss: 0.55732 - acc: 0.7435 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1992  | total loss: \u001b[1m\u001b[32m0.55007\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1992 | loss: 0.55007 - acc: 0.7503 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1993  | total loss: \u001b[1m\u001b[32m0.54343\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1993 | loss: 0.54343 - acc: 0.7564 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1994  | total loss: \u001b[1m\u001b[32m0.56769\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1994 | loss: 0.56769 - acc: 0.7338 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1995  | total loss: \u001b[1m\u001b[32m0.55896\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1995 | loss: 0.55896 - acc: 0.7416 -- iter: 891/891\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1996  | total loss: \u001b[1m\u001b[32m0.58000\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1996 | loss: 0.58000 - acc: 0.7219 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1997  | total loss: \u001b[1m\u001b[32m0.57024\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1997 | loss: 0.57024 - acc: 0.7309 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1998  | total loss: \u001b[1m\u001b[32m0.58668\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1998 | loss: 0.58668 - acc: 0.7144 -- iter: 891/891\n",
      "--\n",
      "Training Step: 1999  | total loss: \u001b[1m\u001b[32m0.57662\u001b[0m\u001b[0m | time: 0.013s\n",
      "| Adam | epoch: 1999 | loss: 0.57662 - acc: 0.7241 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2000  | total loss: \u001b[1m\u001b[32m0.59708\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2000 | loss: 0.59708 - acc: 0.7031 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2001  | total loss: \u001b[1m\u001b[32m0.58648\u001b[0m\u001b[0m | time: 0.012s\n",
      "| Adam | epoch: 2001 | loss: 0.58648 - acc: 0.7138 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2002  | total loss: \u001b[1m\u001b[32m0.57700\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 2002 | loss: 0.57700 - acc: 0.7238 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2003  | total loss: \u001b[1m\u001b[32m0.56820\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2003 | loss: 0.56820 - acc: 0.7327 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2004  | total loss: \u001b[1m\u001b[32m0.58550\u001b[0m\u001b[0m | time: 0.011s\n",
      "| Adam | epoch: 2004 | loss: 0.58550 - acc: 0.7157 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2005  | total loss: \u001b[1m\u001b[32m0.57533\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2005 | loss: 0.57533 - acc: 0.7252 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2006  | total loss: \u001b[1m\u001b[32m0.59451\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 2006 | loss: 0.59451 - acc: 0.7084 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2007  | total loss: \u001b[1m\u001b[32m0.58355\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2007 | loss: 0.58355 - acc: 0.7190 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2008  | total loss: \u001b[1m\u001b[32m0.57373\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2008 | loss: 0.57373 - acc: 0.7285 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2009  | total loss: \u001b[1m\u001b[32m0.56468\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2009 | loss: 0.56468 - acc: 0.7373 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2010  | total loss: \u001b[1m\u001b[32m0.55614\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2010 | loss: 0.55614 - acc: 0.7453 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2011  | total loss: \u001b[1m\u001b[32m0.54799\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 2011 | loss: 0.54799 - acc: 0.7520 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2012  | total loss: \u001b[1m\u001b[32m0.57163\u001b[0m\u001b[0m | time: 0.012s\n",
      "| Adam | epoch: 2012 | loss: 0.57163 - acc: 0.7324 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2013  | total loss: \u001b[1m\u001b[32m0.56135\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2013 | loss: 0.56135 - acc: 0.7406 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2014  | total loss: \u001b[1m\u001b[32m0.58409\u001b[0m\u001b[0m | time: 0.011s\n",
      "| Adam | epoch: 2014 | loss: 0.58409 - acc: 0.7227 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2015  | total loss: \u001b[1m\u001b[32m0.57272\u001b[0m\u001b[0m | time: 0.012s\n",
      "| Adam | epoch: 2015 | loss: 0.57272 - acc: 0.7316 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2016  | total loss: \u001b[1m\u001b[32m0.59076\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2016 | loss: 0.59076 - acc: 0.7151 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2017  | total loss: \u001b[1m\u001b[32m0.57948\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 2017 | loss: 0.57948 - acc: 0.7247 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2018  | total loss: \u001b[1m\u001b[32m0.56965\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2018 | loss: 0.56965 - acc: 0.7335 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2019  | total loss: \u001b[1m\u001b[32m0.56091\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2019 | loss: 0.56091 - acc: 0.7412 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2020  | total loss: \u001b[1m\u001b[32m0.58138\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2020 | loss: 0.58138 - acc: 0.7209 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2021  | total loss: \u001b[1m\u001b[32m0.57174\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2021 | loss: 0.57174 - acc: 0.7302 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2022  | total loss: \u001b[1m\u001b[32m0.58834\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2022 | loss: 0.58834 - acc: 0.7131 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2023  | total loss: \u001b[1m\u001b[32m0.57848\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2023 | loss: 0.57848 - acc: 0.7229 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2024  | total loss: \u001b[1m\u001b[32m0.59738\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2024 | loss: 0.59738 - acc: 0.7044 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2025  | total loss: \u001b[1m\u001b[32m0.58731\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2025 | loss: 0.58731 - acc: 0.7149 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2026  | total loss: \u001b[1m\u001b[32m0.57829\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2026 | loss: 0.57829 - acc: 0.7248 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2027  | total loss: \u001b[1m\u001b[32m0.56978\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2027 | loss: 0.56978 - acc: 0.7334 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2028  | total loss: \u001b[1m\u001b[32m0.58586\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2028 | loss: 0.58586 - acc: 0.7154 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2029  | total loss: \u001b[1m\u001b[32m0.57580\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2029 | loss: 0.57580 - acc: 0.7251 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2030  | total loss: \u001b[1m\u001b[32m0.59538\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2030 | loss: 0.59538 - acc: 0.7075 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2031  | total loss: \u001b[1m\u001b[32m0.58412\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 2031 | loss: 0.58412 - acc: 0.7180 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2032  | total loss: \u001b[1m\u001b[32m0.60167\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2032 | loss: 0.60167 - acc: 0.7002 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2033  | total loss: \u001b[1m\u001b[32m0.59004\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 2033 | loss: 0.59004 - acc: 0.7113 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2034  | total loss: \u001b[1m\u001b[32m0.57966\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2034 | loss: 0.57966 - acc: 0.7214 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2035  | total loss: \u001b[1m\u001b[32m0.57012\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2035 | loss: 0.57012 - acc: 0.7308 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2036  | total loss: \u001b[1m\u001b[32m0.58774\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2036 | loss: 0.58774 - acc: 0.7130 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2037  | total loss: \u001b[1m\u001b[32m0.57710\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2037 | loss: 0.57710 - acc: 0.7231 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2038  | total loss: \u001b[1m\u001b[32m0.56740\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2038 | loss: 0.56740 - acc: 0.7317 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2039  | total loss: \u001b[1m\u001b[32m0.55839\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2039 | loss: 0.55839 - acc: 0.7397 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2040  | total loss: \u001b[1m\u001b[32m0.57863\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 2040 | loss: 0.57863 - acc: 0.7224 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2041  | total loss: \u001b[1m\u001b[32m0.56809\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2041 | loss: 0.56809 - acc: 0.7319 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2042  | total loss: \u001b[1m\u001b[32m0.59267\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2042 | loss: 0.59267 - acc: 0.7110 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2043  | total loss: \u001b[1m\u001b[32m0.58101\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2043 | loss: 0.58101 - acc: 0.7211 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2044  | total loss: \u001b[1m\u001b[32m0.59683\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2044 | loss: 0.59683 - acc: 0.7058 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2045  | total loss: \u001b[1m\u001b[32m0.58548\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2045 | loss: 0.58548 - acc: 0.7166 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2046  | total loss: \u001b[1m\u001b[32m0.60268\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2046 | loss: 0.60268 - acc: 0.6990 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2047  | total loss: \u001b[1m\u001b[32m0.59170\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2047 | loss: 0.59170 - acc: 0.7104 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2048  | total loss: \u001b[1m\u001b[32m0.58217\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2048 | loss: 0.58217 - acc: 0.7206 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2049  | total loss: \u001b[1m\u001b[32m0.57358\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2049 | loss: 0.57358 - acc: 0.7298 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2050  | total loss: \u001b[1m\u001b[32m0.56553\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 2050 | loss: 0.56553 - acc: 0.7381 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2051  | total loss: \u001b[1m\u001b[32m0.55771\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2051 | loss: 0.55771 - acc: 0.7458 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2052  | total loss: \u001b[1m\u001b[32m0.54995\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2052 | loss: 0.54995 - acc: 0.7525 -- iter: 891/891\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 2053  | total loss: \u001b[1m\u001b[32m0.54221\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2053 | loss: 0.54221 - acc: 0.7587 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2054  | total loss: \u001b[1m\u001b[32m0.56752\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2054 | loss: 0.56752 - acc: 0.7373 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2055  | total loss: \u001b[1m\u001b[32m0.55720\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2055 | loss: 0.55720 - acc: 0.7450 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2056  | total loss: \u001b[1m\u001b[32m0.54770\u001b[0m\u001b[0m | time: 0.011s\n",
      "| Adam | epoch: 2056 | loss: 0.54770 - acc: 0.7520 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2057  | total loss: \u001b[1m\u001b[32m0.53890\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2057 | loss: 0.53890 - acc: 0.7584 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2058  | total loss: \u001b[1m\u001b[32m0.57097\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2058 | loss: 0.57097 - acc: 0.7359 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2059  | total loss: \u001b[1m\u001b[32m0.55962\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2059 | loss: 0.55962 - acc: 0.7435 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2060  | total loss: \u001b[1m\u001b[32m0.58995\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 2060 | loss: 0.58995 - acc: 0.7239 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2061  | total loss: \u001b[1m\u001b[32m0.57701\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2061 | loss: 0.57701 - acc: 0.7330 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2062  | total loss: \u001b[1m\u001b[32m0.59992\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 2062 | loss: 0.59992 - acc: 0.7138 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2063  | total loss: \u001b[1m\u001b[32m0.58717\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 2063 | loss: 0.58717 - acc: 0.7237 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2064  | total loss: \u001b[1m\u001b[32m0.60390\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2064 | loss: 0.60390 - acc: 0.7060 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2065  | total loss: \u001b[1m\u001b[32m0.59277\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2065 | loss: 0.59277 - acc: 0.7167 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2066  | total loss: \u001b[1m\u001b[32m0.60724\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2066 | loss: 0.60724 - acc: 0.7001 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2067  | total loss: \u001b[1m\u001b[32m0.59770\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 2067 | loss: 0.59770 - acc: 0.7112 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2068  | total loss: \u001b[1m\u001b[32m0.61067\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 2068 | loss: 0.61067 - acc: 0.6954 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2069  | total loss: \u001b[1m\u001b[32m0.60209\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2069 | loss: 0.60209 - acc: 0.7060 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2070  | total loss: \u001b[1m\u001b[32m0.61419\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 2070 | loss: 0.61419 - acc: 0.6897 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2071  | total loss: \u001b[1m\u001b[32m0.60587\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2071 | loss: 0.60587 - acc: 0.7007 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2072  | total loss: \u001b[1m\u001b[32m0.59835\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 2072 | loss: 0.59835 - acc: 0.7108 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2073  | total loss: \u001b[1m\u001b[32m0.59118\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2073 | loss: 0.59118 - acc: 0.7201 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2074  | total loss: \u001b[1m\u001b[32m0.60483\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2074 | loss: 0.60483 - acc: 0.7021 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2075  | total loss: \u001b[1m\u001b[32m0.59606\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 2075 | loss: 0.59606 - acc: 0.7128 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2076  | total loss: \u001b[1m\u001b[32m0.60759\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2076 | loss: 0.60759 - acc: 0.6980 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2077  | total loss: \u001b[1m\u001b[32m0.59784\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 2077 | loss: 0.59784 - acc: 0.7089 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2078  | total loss: \u001b[1m\u001b[32m0.61130\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2078 | loss: 0.61130 - acc: 0.6925 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2079  | total loss: \u001b[1m\u001b[32m0.60080\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2079 | loss: 0.60080 - acc: 0.7036 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2080  | total loss: \u001b[1m\u001b[32m0.61266\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 2080 | loss: 0.61266 - acc: 0.6898 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2081  | total loss: \u001b[1m\u001b[32m0.60188\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2081 | loss: 0.60188 - acc: 0.7013 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2082  | total loss: \u001b[1m\u001b[32m0.59202\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2082 | loss: 0.59202 - acc: 0.7118 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2083  | total loss: \u001b[1m\u001b[32m0.58280\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2083 | loss: 0.58280 - acc: 0.7212 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2084  | total loss: \u001b[1m\u001b[32m0.57401\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 2084 | loss: 0.57401 - acc: 0.7296 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2085  | total loss: \u001b[1m\u001b[32m0.56554\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2085 | loss: 0.56554 - acc: 0.7360 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2086  | total loss: \u001b[1m\u001b[32m0.55733\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 2086 | loss: 0.55733 - acc: 0.7413 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2087  | total loss: \u001b[1m\u001b[32m0.54937\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2087 | loss: 0.54937 - acc: 0.7461 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2088  | total loss: \u001b[1m\u001b[32m0.54168\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2088 | loss: 0.54168 - acc: 0.7505 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2089  | total loss: \u001b[1m\u001b[32m0.53430\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2089 | loss: 0.53430 - acc: 0.7563 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2090  | total loss: \u001b[1m\u001b[32m0.52732\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2090 | loss: 0.52732 - acc: 0.7610 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2091  | total loss: \u001b[1m\u001b[32m0.52079\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2091 | loss: 0.52079 - acc: 0.7656 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2092  | total loss: \u001b[1m\u001b[32m0.51473\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2092 | loss: 0.51473 - acc: 0.7702 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2093  | total loss: \u001b[1m\u001b[32m0.50909\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2093 | loss: 0.50909 - acc: 0.7743 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2094  | total loss: \u001b[1m\u001b[32m0.55004\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2094 | loss: 0.55004 - acc: 0.7532 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2095  | total loss: \u001b[1m\u001b[32m0.54025\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2095 | loss: 0.54025 - acc: 0.7588 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2096  | total loss: \u001b[1m\u001b[32m0.57871\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2096 | loss: 0.57871 - acc: 0.7393 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2097  | total loss: \u001b[1m\u001b[32m0.56548\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 2097 | loss: 0.56548 - acc: 0.7463 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2098  | total loss: \u001b[1m\u001b[32m0.55379\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2098 | loss: 0.55379 - acc: 0.7529 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2099  | total loss: \u001b[1m\u001b[32m0.54377\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2099 | loss: 0.54377 - acc: 0.7586 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2100  | total loss: \u001b[1m\u001b[32m0.57346\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2100 | loss: 0.57346 - acc: 0.7386 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2101  | total loss: \u001b[1m\u001b[32m0.56310\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2101 | loss: 0.56310 - acc: 0.7452 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2102  | total loss: \u001b[1m\u001b[32m0.55449\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 2102 | loss: 0.55449 - acc: 0.7509 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2103  | total loss: \u001b[1m\u001b[32m0.54680\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2103 | loss: 0.54680 - acc: 0.7561 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2104  | total loss: \u001b[1m\u001b[32m0.53940\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 2104 | loss: 0.53940 - acc: 0.7610 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2105  | total loss: \u001b[1m\u001b[32m0.53197\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2105 | loss: 0.53197 - acc: 0.7657 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2106  | total loss: \u001b[1m\u001b[32m0.56152\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2106 | loss: 0.56152 - acc: 0.7415 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2107  | total loss: \u001b[1m\u001b[32m0.55112\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2107 | loss: 0.55112 - acc: 0.7485 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2108  | total loss: \u001b[1m\u001b[32m0.57598\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2108 | loss: 0.57598 - acc: 0.7297 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2109  | total loss: \u001b[1m\u001b[32m0.56453\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 2109 | loss: 0.56453 - acc: 0.7381 -- iter: 891/891\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 2110  | total loss: \u001b[1m\u001b[32m0.55453\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2110 | loss: 0.55453 - acc: 0.7457 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2111  | total loss: \u001b[1m\u001b[32m0.54565\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2111 | loss: 0.54565 - acc: 0.7524 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2112  | total loss: \u001b[1m\u001b[32m0.53763\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2112 | loss: 0.53763 - acc: 0.7584 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2113  | total loss: \u001b[1m\u001b[32m0.53030\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2113 | loss: 0.53030 - acc: 0.7645 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2114  | total loss: \u001b[1m\u001b[32m0.55465\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2114 | loss: 0.55465 - acc: 0.7463 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2115  | total loss: \u001b[1m\u001b[32m0.54557\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 2115 | loss: 0.54557 - acc: 0.7534 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2116  | total loss: \u001b[1m\u001b[32m0.53744\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2116 | loss: 0.53744 - acc: 0.7588 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2117  | total loss: \u001b[1m\u001b[32m0.53006\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2117 | loss: 0.53006 - acc: 0.7639 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2118  | total loss: \u001b[1m\u001b[32m0.52326\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 2118 | loss: 0.52326 - acc: 0.7683 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2119  | total loss: \u001b[1m\u001b[32m0.51689\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2119 | loss: 0.51689 - acc: 0.7722 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2120  | total loss: \u001b[1m\u001b[32m0.54956\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 2120 | loss: 0.54956 - acc: 0.7494 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2121  | total loss: \u001b[1m\u001b[32m0.54037\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2121 | loss: 0.54037 - acc: 0.7554 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2122  | total loss: \u001b[1m\u001b[32m0.57143\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 2122 | loss: 0.57143 - acc: 0.7315 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2123  | total loss: \u001b[1m\u001b[32m0.56055\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2123 | loss: 0.56055 - acc: 0.7393 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2124  | total loss: \u001b[1m\u001b[32m0.55117\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2124 | loss: 0.55117 - acc: 0.7464 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2125  | total loss: \u001b[1m\u001b[32m0.54294\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2125 | loss: 0.54294 - acc: 0.7525 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2126  | total loss: \u001b[1m\u001b[32m0.53553\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2126 | loss: 0.53553 - acc: 0.7580 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2127  | total loss: \u001b[1m\u001b[32m0.52861\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2127 | loss: 0.52861 - acc: 0.7631 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2128  | total loss: \u001b[1m\u001b[32m0.52195\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2128 | loss: 0.52195 - acc: 0.7678 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2129  | total loss: \u001b[1m\u001b[32m0.51545\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2129 | loss: 0.51545 - acc: 0.7720 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2130  | total loss: \u001b[1m\u001b[32m0.50912\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2130 | loss: 0.50912 - acc: 0.7761 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2131  | total loss: \u001b[1m\u001b[32m0.50306\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2131 | loss: 0.50306 - acc: 0.7796 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2132  | total loss: \u001b[1m\u001b[32m0.49739\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2132 | loss: 0.49739 - acc: 0.7827 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2133  | total loss: \u001b[1m\u001b[32m0.49223\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 2133 | loss: 0.49223 - acc: 0.7855 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2134  | total loss: \u001b[1m\u001b[32m0.53750\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2134 | loss: 0.53750 - acc: 0.7631 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2135  | total loss: \u001b[1m\u001b[32m0.52826\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2135 | loss: 0.52826 - acc: 0.7677 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2136  | total loss: \u001b[1m\u001b[32m0.51984\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2136 | loss: 0.51984 - acc: 0.7719 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2137  | total loss: \u001b[1m\u001b[32m0.51216\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2137 | loss: 0.51216 - acc: 0.7756 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2138  | total loss: \u001b[1m\u001b[32m0.55586\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 2138 | loss: 0.55586 - acc: 0.7525 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2139  | total loss: \u001b[1m\u001b[32m0.54460\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2139 | loss: 0.54460 - acc: 0.7578 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2140  | total loss: \u001b[1m\u001b[32m0.57883\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 2140 | loss: 0.57883 - acc: 0.7366 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2141  | total loss: \u001b[1m\u001b[32m0.56669\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2141 | loss: 0.56669 - acc: 0.7427 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2142  | total loss: \u001b[1m\u001b[32m0.59477\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2142 | loss: 0.59477 - acc: 0.7214 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2143  | total loss: \u001b[1m\u001b[32m0.58391\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2143 | loss: 0.58391 - acc: 0.7292 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2144  | total loss: \u001b[1m\u001b[32m0.60407\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2144 | loss: 0.60407 - acc: 0.7088 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2145  | total loss: \u001b[1m\u001b[32m0.59489\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2145 | loss: 0.59489 - acc: 0.7173 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2146  | total loss: \u001b[1m\u001b[32m0.61072\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2146 | loss: 0.61072 - acc: 0.6974 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2147  | total loss: \u001b[1m\u001b[32m0.60259\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2147 | loss: 0.60259 - acc: 0.7067 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2148  | total loss: \u001b[1m\u001b[32m0.59566\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2148 | loss: 0.59566 - acc: 0.7152 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2149  | total loss: \u001b[1m\u001b[32m0.58933\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2149 | loss: 0.58933 - acc: 0.7233 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2150  | total loss: \u001b[1m\u001b[32m0.58318\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2150 | loss: 0.58318 - acc: 0.7308 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2151  | total loss: \u001b[1m\u001b[32m0.57694\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2151 | loss: 0.57694 - acc: 0.7384 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2152  | total loss: \u001b[1m\u001b[32m0.57044\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 2152 | loss: 0.57044 - acc: 0.7457 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2153  | total loss: \u001b[1m\u001b[32m0.56366\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2153 | loss: 0.56366 - acc: 0.7511 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2154  | total loss: \u001b[1m\u001b[32m0.55666\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2154 | loss: 0.55666 - acc: 0.7560 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2155  | total loss: \u001b[1m\u001b[32m0.54957\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2155 | loss: 0.54957 - acc: 0.7587 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2156  | total loss: \u001b[1m\u001b[32m0.57560\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2156 | loss: 0.57560 - acc: 0.7387 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2157  | total loss: \u001b[1m\u001b[32m0.56591\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2157 | loss: 0.56591 - acc: 0.7433 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2158  | total loss: \u001b[1m\u001b[32m0.55705\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 2158 | loss: 0.55705 - acc: 0.7477 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2159  | total loss: \u001b[1m\u001b[32m0.54892\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2159 | loss: 0.54892 - acc: 0.7526 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2160  | total loss: \u001b[1m\u001b[32m0.58243\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 2160 | loss: 0.58243 - acc: 0.7311 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2161  | total loss: \u001b[1m\u001b[32m0.57154\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2161 | loss: 0.57154 - acc: 0.7385 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2162  | total loss: \u001b[1m\u001b[32m0.56158\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2162 | loss: 0.56158 - acc: 0.7449 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2163  | total loss: \u001b[1m\u001b[32m0.55240\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2163 | loss: 0.55240 - acc: 0.7512 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2164  | total loss: \u001b[1m\u001b[32m0.54391\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2164 | loss: 0.54391 - acc: 0.7566 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2165  | total loss: \u001b[1m\u001b[32m0.53600\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2165 | loss: 0.53600 - acc: 0.7615 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2166  | total loss: \u001b[1m\u001b[32m0.56599\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 2166 | loss: 0.56599 - acc: 0.7401 -- iter: 891/891\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 2167  | total loss: \u001b[1m\u001b[32m0.55563\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2167 | loss: 0.55563 - acc: 0.7479 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2168  | total loss: \u001b[1m\u001b[32m0.58091\u001b[0m\u001b[0m | time: 0.011s\n",
      "| Adam | epoch: 2168 | loss: 0.58091 - acc: 0.7272 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2169  | total loss: \u001b[1m\u001b[32m0.57037\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2169 | loss: 0.57037 - acc: 0.7356 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2170  | total loss: \u001b[1m\u001b[32m0.59098\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2170 | loss: 0.59098 - acc: 0.7155 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2171  | total loss: \u001b[1m\u001b[32m0.58168\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2171 | loss: 0.58168 - acc: 0.7250 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2172  | total loss: \u001b[1m\u001b[32m0.57438\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2172 | loss: 0.57438 - acc: 0.7326 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2173  | total loss: \u001b[1m\u001b[32m0.56816\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2173 | loss: 0.56816 - acc: 0.7396 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2174  | total loss: \u001b[1m\u001b[32m0.58581\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2174 | loss: 0.58581 - acc: 0.7192 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2175  | total loss: \u001b[1m\u001b[32m0.57787\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2175 | loss: 0.57787 - acc: 0.7281 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2176  | total loss: \u001b[1m\u001b[32m0.57022\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2176 | loss: 0.57022 - acc: 0.7365 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2177  | total loss: \u001b[1m\u001b[32m0.56263\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2177 | loss: 0.56263 - acc: 0.7441 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2178  | total loss: \u001b[1m\u001b[32m0.55496\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2178 | loss: 0.55496 - acc: 0.7512 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2179  | total loss: \u001b[1m\u001b[32m0.54724\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2179 | loss: 0.54724 - acc: 0.7578 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2180  | total loss: \u001b[1m\u001b[32m0.57171\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2180 | loss: 0.57171 - acc: 0.7373 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2181  | total loss: \u001b[1m\u001b[32m0.56157\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2181 | loss: 0.56157 - acc: 0.7441 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2182  | total loss: \u001b[1m\u001b[32m0.58835\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2182 | loss: 0.58835 - acc: 0.7235 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2183  | total loss: \u001b[1m\u001b[32m0.57652\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2183 | loss: 0.57652 - acc: 0.7326 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2184  | total loss: \u001b[1m\u001b[32m0.59851\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2184 | loss: 0.59851 - acc: 0.7125 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2185  | total loss: \u001b[1m\u001b[32m0.58628\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2185 | loss: 0.58628 - acc: 0.7226 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2186  | total loss: \u001b[1m\u001b[32m0.57561\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2186 | loss: 0.57561 - acc: 0.7317 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2187  | total loss: \u001b[1m\u001b[32m0.56608\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2187 | loss: 0.56608 - acc: 0.7401 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2188  | total loss: \u001b[1m\u001b[32m0.55745\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2188 | loss: 0.55745 - acc: 0.7475 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2189  | total loss: \u001b[1m\u001b[32m0.54952\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2189 | loss: 0.54952 - acc: 0.7537 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2190  | total loss: \u001b[1m\u001b[32m0.57221\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2190 | loss: 0.57221 - acc: 0.7332 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2191  | total loss: \u001b[1m\u001b[32m0.56252\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2191 | loss: 0.56252 - acc: 0.7410 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2192  | total loss: \u001b[1m\u001b[32m0.58329\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2192 | loss: 0.58329 - acc: 0.7229 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2193  | total loss: \u001b[1m\u001b[32m0.57281\u001b[0m\u001b[0m | time: 0.011s\n",
      "| Adam | epoch: 2193 | loss: 0.57281 - acc: 0.7319 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2194  | total loss: \u001b[1m\u001b[32m0.59234\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2194 | loss: 0.59234 - acc: 0.7122 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2195  | total loss: \u001b[1m\u001b[32m0.58192\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2195 | loss: 0.58192 - acc: 0.7223 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2196  | total loss: \u001b[1m\u001b[32m0.57284\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2196 | loss: 0.57284 - acc: 0.7313 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2197  | total loss: \u001b[1m\u001b[32m0.56457\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2197 | loss: 0.56457 - acc: 0.7393 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2198  | total loss: \u001b[1m\u001b[32m0.58158\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2198 | loss: 0.58158 - acc: 0.7213 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2199  | total loss: \u001b[1m\u001b[32m0.57206\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2199 | loss: 0.57206 - acc: 0.7307 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2200  | total loss: \u001b[1m\u001b[32m0.58972\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2200 | loss: 0.58972 - acc: 0.7129 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2201  | total loss: \u001b[1m\u001b[32m0.57916\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2201 | loss: 0.57916 - acc: 0.7232 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2202  | total loss: \u001b[1m\u001b[32m0.56948\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2202 | loss: 0.56948 - acc: 0.7320 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2203  | total loss: \u001b[1m\u001b[32m0.56038\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2203 | loss: 0.56038 - acc: 0.7400 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2204  | total loss: \u001b[1m\u001b[32m0.58072\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2204 | loss: 0.58072 - acc: 0.7222 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2205  | total loss: \u001b[1m\u001b[32m0.56990\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2205 | loss: 0.56990 - acc: 0.7316 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2206  | total loss: \u001b[1m\u001b[32m0.59176\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2206 | loss: 0.59176 - acc: 0.7131 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2207  | total loss: \u001b[1m\u001b[32m0.57990\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2207 | loss: 0.57990 - acc: 0.7235 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2208  | total loss: \u001b[1m\u001b[32m0.60001\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2208 | loss: 0.60001 - acc: 0.7062 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2209  | total loss: \u001b[1m\u001b[32m0.58800\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2209 | loss: 0.58800 - acc: 0.7170 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2210  | total loss: \u001b[1m\u001b[32m0.57755\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2210 | loss: 0.57755 - acc: 0.7266 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2211  | total loss: \u001b[1m\u001b[32m0.56823\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2211 | loss: 0.56823 - acc: 0.7351 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2212  | total loss: \u001b[1m\u001b[32m0.55969\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2212 | loss: 0.55969 - acc: 0.7426 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2213  | total loss: \u001b[1m\u001b[32m0.55164\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2213 | loss: 0.55164 - acc: 0.7496 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2214  | total loss: \u001b[1m\u001b[32m0.54389\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2214 | loss: 0.54389 - acc: 0.7560 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2215  | total loss: \u001b[1m\u001b[32m0.53632\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2215 | loss: 0.53632 - acc: 0.7618 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2216  | total loss: \u001b[1m\u001b[32m0.56468\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2216 | loss: 0.56468 - acc: 0.7396 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2217  | total loss: \u001b[1m\u001b[32m0.55439\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2217 | loss: 0.55439 - acc: 0.7475 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2218  | total loss: \u001b[1m\u001b[32m0.58242\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2218 | loss: 0.58242 - acc: 0.7260 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2219  | total loss: \u001b[1m\u001b[32m0.57044\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2219 | loss: 0.57044 - acc: 0.7349 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2220  | total loss: \u001b[1m\u001b[32m0.55985\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2220 | loss: 0.55985 - acc: 0.7429 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2221  | total loss: \u001b[1m\u001b[32m0.55035\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2221 | loss: 0.55035 - acc: 0.7499 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2222  | total loss: \u001b[1m\u001b[32m0.54168\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2222 | loss: 0.54168 - acc: 0.7560 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2223  | total loss: \u001b[1m\u001b[32m0.53363\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2223 | loss: 0.53363 - acc: 0.7619 -- iter: 891/891\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 2224  | total loss: \u001b[1m\u001b[32m0.52607\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2224 | loss: 0.52607 - acc: 0.7670 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2225  | total loss: \u001b[1m\u001b[32m0.51889\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2225 | loss: 0.51889 - acc: 0.7716 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2226  | total loss: \u001b[1m\u001b[32m0.51209\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2226 | loss: 0.51209 - acc: 0.7760 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2227  | total loss: \u001b[1m\u001b[32m0.50568\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2227 | loss: 0.50568 - acc: 0.7793 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2228  | total loss: \u001b[1m\u001b[32m0.49975\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2228 | loss: 0.49975 - acc: 0.7822 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2229  | total loss: \u001b[1m\u001b[32m0.49432\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2229 | loss: 0.49432 - acc: 0.7848 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2230  | total loss: \u001b[1m\u001b[32m0.53932\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2230 | loss: 0.53932 - acc: 0.7603 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2231  | total loss: \u001b[1m\u001b[32m0.52980\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2231 | loss: 0.52980 - acc: 0.7648 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2232  | total loss: \u001b[1m\u001b[32m0.52115\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2232 | loss: 0.52115 - acc: 0.7690 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2233  | total loss: \u001b[1m\u001b[32m0.51332\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2233 | loss: 0.51332 - acc: 0.7733 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2234  | total loss: \u001b[1m\u001b[32m0.50628\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2234 | loss: 0.50628 - acc: 0.7766 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2235  | total loss: \u001b[1m\u001b[32m0.49996\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2235 | loss: 0.49996 - acc: 0.7792 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2236  | total loss: \u001b[1m\u001b[32m0.54059\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2236 | loss: 0.54059 - acc: 0.7563 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2237  | total loss: \u001b[1m\u001b[32m0.53118\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2237 | loss: 0.53118 - acc: 0.7608 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2238  | total loss: \u001b[1m\u001b[32m0.52303\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2238 | loss: 0.52303 - acc: 0.7649 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2239  | total loss: \u001b[1m\u001b[32m0.51583\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2239 | loss: 0.51583 - acc: 0.7684 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2240  | total loss: \u001b[1m\u001b[32m0.55107\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2240 | loss: 0.55107 - acc: 0.7468 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2241  | total loss: \u001b[1m\u001b[32m0.54152\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 2241 | loss: 0.54152 - acc: 0.7522 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2242  | total loss: \u001b[1m\u001b[32m0.53321\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2242 | loss: 0.53321 - acc: 0.7571 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2243  | total loss: \u001b[1m\u001b[32m0.52576\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2243 | loss: 0.52576 - acc: 0.7618 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2244  | total loss: \u001b[1m\u001b[32m0.51889\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2244 | loss: 0.51889 - acc: 0.7662 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2245  | total loss: \u001b[1m\u001b[32m0.51248\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2245 | loss: 0.51248 - acc: 0.7705 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2246  | total loss: \u001b[1m\u001b[32m0.54499\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2246 | loss: 0.54499 - acc: 0.7491 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2247  | total loss: \u001b[1m\u001b[32m0.53599\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2247 | loss: 0.53599 - acc: 0.7553 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2248  | total loss: \u001b[1m\u001b[32m0.52802\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2248 | loss: 0.52802 - acc: 0.7607 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2249  | total loss: \u001b[1m\u001b[32m0.52090\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2249 | loss: 0.52090 - acc: 0.7656 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2250  | total loss: \u001b[1m\u001b[32m0.54960\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2250 | loss: 0.54960 - acc: 0.7447 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2251  | total loss: \u001b[1m\u001b[32m0.54061\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2251 | loss: 0.54061 - acc: 0.7511 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2252  | total loss: \u001b[1m\u001b[32m0.53273\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2252 | loss: 0.53273 - acc: 0.7569 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2253  | total loss: \u001b[1m\u001b[32m0.52575\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2253 | loss: 0.52575 - acc: 0.7623 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2254  | total loss: \u001b[1m\u001b[32m0.55391\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2254 | loss: 0.55391 - acc: 0.7394 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2255  | total loss: \u001b[1m\u001b[32m0.54520\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2255 | loss: 0.54520 - acc: 0.7464 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2256  | total loss: \u001b[1m\u001b[32m0.53762\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2256 | loss: 0.53762 - acc: 0.7526 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2257  | total loss: \u001b[1m\u001b[32m0.53087\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2257 | loss: 0.53087 - acc: 0.7583 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2258  | total loss: \u001b[1m\u001b[32m0.55471\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2258 | loss: 0.55471 - acc: 0.7385 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2259  | total loss: \u001b[1m\u001b[32m0.54647\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2259 | loss: 0.54647 - acc: 0.7458 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2260  | total loss: \u001b[1m\u001b[32m0.56702\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2260 | loss: 0.56702 - acc: 0.7276 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2261  | total loss: \u001b[1m\u001b[32m0.55823\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2261 | loss: 0.55823 - acc: 0.7360 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2262  | total loss: \u001b[1m\u001b[32m0.55055\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2262 | loss: 0.55055 - acc: 0.7437 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2263  | total loss: \u001b[1m\u001b[32m0.54354\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2263 | loss: 0.54354 - acc: 0.7503 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2264  | total loss: \u001b[1m\u001b[32m0.53685\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 2264 | loss: 0.53685 - acc: 0.7562 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2265  | total loss: \u001b[1m\u001b[32m0.53029\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2265 | loss: 0.53029 - acc: 0.7613 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2266  | total loss: \u001b[1m\u001b[32m0.52376\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2266 | loss: 0.52376 - acc: 0.7662 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2267  | total loss: \u001b[1m\u001b[32m0.51729\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2267 | loss: 0.51729 - acc: 0.7708 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2268  | total loss: \u001b[1m\u001b[32m0.51100\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2268 | loss: 0.51100 - acc: 0.7748 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2269  | total loss: \u001b[1m\u001b[32m0.50503\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2269 | loss: 0.50503 - acc: 0.7783 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2270  | total loss: \u001b[1m\u001b[32m0.54363\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2270 | loss: 0.54363 - acc: 0.7548 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2271  | total loss: \u001b[1m\u001b[32m0.53421\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2271 | loss: 0.53421 - acc: 0.7603 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2272  | total loss: \u001b[1m\u001b[32m0.57522\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2272 | loss: 0.57522 - acc: 0.7371 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2273  | total loss: \u001b[1m\u001b[32m0.56247\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2273 | loss: 0.56247 - acc: 0.7443 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2274  | total loss: \u001b[1m\u001b[32m0.55096\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2274 | loss: 0.55096 - acc: 0.7508 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2275  | total loss: \u001b[1m\u001b[32m0.54062\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2275 | loss: 0.54062 - acc: 0.7571 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2276  | total loss: \u001b[1m\u001b[32m0.53135\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2276 | loss: 0.53135 - acc: 0.7628 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2277  | total loss: \u001b[1m\u001b[32m0.52302\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2277 | loss: 0.52302 - acc: 0.7676 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2278  | total loss: \u001b[1m\u001b[32m0.51549\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2278 | loss: 0.51549 - acc: 0.7721 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2279  | total loss: \u001b[1m\u001b[32m0.50865\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2279 | loss: 0.50865 - acc: 0.7756 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2280  | total loss: \u001b[1m\u001b[32m0.50238\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2280 | loss: 0.50238 - acc: 0.7787 -- iter: 891/891\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 2281  | total loss: \u001b[1m\u001b[32m0.49659\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2281 | loss: 0.49659 - acc: 0.7816 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2282  | total loss: \u001b[1m\u001b[32m0.49124\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2282 | loss: 0.49124 - acc: 0.7842 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2283  | total loss: \u001b[1m\u001b[32m0.48635\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2283 | loss: 0.48635 - acc: 0.7868 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2284  | total loss: \u001b[1m\u001b[32m0.53773\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2284 | loss: 0.53773 - acc: 0.7609 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2285  | total loss: \u001b[1m\u001b[32m0.52814\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2285 | loss: 0.52814 - acc: 0.7653 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2286  | total loss: \u001b[1m\u001b[32m0.51955\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2286 | loss: 0.51955 - acc: 0.7690 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2287  | total loss: \u001b[1m\u001b[32m0.51185\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2287 | loss: 0.51185 - acc: 0.7725 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2288  | total loss: \u001b[1m\u001b[32m0.55303\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 2288 | loss: 0.55303 - acc: 0.7494 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2289  | total loss: \u001b[1m\u001b[32m0.54230\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2289 | loss: 0.54230 - acc: 0.7551 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2290  | total loss: \u001b[1m\u001b[32m0.53292\u001b[0m\u001b[0m | time: 0.011s\n",
      "| Adam | epoch: 2290 | loss: 0.53292 - acc: 0.7602 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2291  | total loss: \u001b[1m\u001b[32m0.52460\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2291 | loss: 0.52460 - acc: 0.7645 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2292  | total loss: \u001b[1m\u001b[32m0.51711\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 2292 | loss: 0.51711 - acc: 0.7680 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2293  | total loss: \u001b[1m\u001b[32m0.51025\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2293 | loss: 0.51025 - acc: 0.7715 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2294  | total loss: \u001b[1m\u001b[32m0.50389\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 2294 | loss: 0.50389 - acc: 0.7744 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2295  | total loss: \u001b[1m\u001b[32m0.49801\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2295 | loss: 0.49801 - acc: 0.7777 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2296  | total loss: \u001b[1m\u001b[32m0.49266\u001b[0m\u001b[0m | time: 0.011s\n",
      "| Adam | epoch: 2296 | loss: 0.49266 - acc: 0.7809 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2297  | total loss: \u001b[1m\u001b[32m0.48782\u001b[0m\u001b[0m | time: 0.012s\n",
      "| Adam | epoch: 2297 | loss: 0.48782 - acc: 0.7838 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2298  | total loss: \u001b[1m\u001b[32m0.53388\u001b[0m\u001b[0m | time: 0.012s\n",
      "| Adam | epoch: 2298 | loss: 0.53388 - acc: 0.7595 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2299  | total loss: \u001b[1m\u001b[32m0.52498\u001b[0m\u001b[0m | time: 0.013s\n",
      "| Adam | epoch: 2299 | loss: 0.52498 - acc: 0.7635 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2300  | total loss: \u001b[1m\u001b[32m0.56300\u001b[0m\u001b[0m | time: 0.012s\n",
      "| Adam | epoch: 2300 | loss: 0.56300 - acc: 0.7402 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2301  | total loss: \u001b[1m\u001b[32m0.55148\u001b[0m\u001b[0m | time: 0.012s\n",
      "| Adam | epoch: 2301 | loss: 0.55148 - acc: 0.7460 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2302  | total loss: \u001b[1m\u001b[32m0.54136\u001b[0m\u001b[0m | time: 0.012s\n",
      "| Adam | epoch: 2302 | loss: 0.54136 - acc: 0.7519 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2303  | total loss: \u001b[1m\u001b[32m0.53255\u001b[0m\u001b[0m | time: 0.014s\n",
      "| Adam | epoch: 2303 | loss: 0.53255 - acc: 0.7577 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2304  | total loss: \u001b[1m\u001b[32m0.56237\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 2304 | loss: 0.56237 - acc: 0.7363 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2305  | total loss: \u001b[1m\u001b[32m0.55237\u001b[0m\u001b[0m | time: 0.012s\n",
      "| Adam | epoch: 2305 | loss: 0.55237 - acc: 0.7435 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2306  | total loss: \u001b[1m\u001b[32m0.57581\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2306 | loss: 0.57581 - acc: 0.7252 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2307  | total loss: \u001b[1m\u001b[32m0.56618\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2307 | loss: 0.56618 - acc: 0.7329 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2308  | total loss: \u001b[1m\u001b[32m0.55821\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2308 | loss: 0.55821 - acc: 0.7397 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2309  | total loss: \u001b[1m\u001b[32m0.55121\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2309 | loss: 0.55121 - acc: 0.7457 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2310  | total loss: \u001b[1m\u001b[32m0.57066\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2310 | loss: 0.57066 - acc: 0.7262 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2311  | total loss: \u001b[1m\u001b[32m0.56242\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2311 | loss: 0.56242 - acc: 0.7338 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2312  | total loss: \u001b[1m\u001b[32m0.55482\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2312 | loss: 0.55482 - acc: 0.7411 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2313  | total loss: \u001b[1m\u001b[32m0.54749\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2313 | loss: 0.54749 - acc: 0.7479 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2314  | total loss: \u001b[1m\u001b[32m0.56893\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2314 | loss: 0.56893 - acc: 0.7283 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2315  | total loss: \u001b[1m\u001b[32m0.55954\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 2315 | loss: 0.55954 - acc: 0.7369 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2316  | total loss: \u001b[1m\u001b[32m0.55093\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 2316 | loss: 0.55093 - acc: 0.7440 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2317  | total loss: \u001b[1m\u001b[32m0.54283\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2317 | loss: 0.54283 - acc: 0.7504 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2318  | total loss: \u001b[1m\u001b[32m0.53517\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 2318 | loss: 0.53517 - acc: 0.7570 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2319  | total loss: \u001b[1m\u001b[32m0.52797\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2319 | loss: 0.52797 - acc: 0.7624 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2320  | total loss: \u001b[1m\u001b[32m0.55747\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2320 | loss: 0.55747 - acc: 0.7426 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2321  | total loss: \u001b[1m\u001b[32m0.54776\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2321 | loss: 0.54776 - acc: 0.7496 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2322  | total loss: \u001b[1m\u001b[32m0.53896\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2322 | loss: 0.53896 - acc: 0.7560 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2323  | total loss: \u001b[1m\u001b[32m0.53097\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2323 | loss: 0.53097 - acc: 0.7613 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2324  | total loss: \u001b[1m\u001b[32m0.56274\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 2324 | loss: 0.56274 - acc: 0.7403 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2325  | total loss: \u001b[1m\u001b[32m0.55235\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 2325 | loss: 0.55235 - acc: 0.7472 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2326  | total loss: \u001b[1m\u001b[32m0.57882\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2326 | loss: 0.57882 - acc: 0.7271 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2327  | total loss: \u001b[1m\u001b[32m0.56730\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2327 | loss: 0.56730 - acc: 0.7358 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2328  | total loss: \u001b[1m\u001b[32m0.55740\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2328 | loss: 0.55740 - acc: 0.7436 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2329  | total loss: \u001b[1m\u001b[32m0.54882\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2329 | loss: 0.54882 - acc: 0.7506 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2330  | total loss: \u001b[1m\u001b[32m0.57103\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2330 | loss: 0.57103 - acc: 0.7312 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2331  | total loss: \u001b[1m\u001b[32m0.56170\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2331 | loss: 0.56170 - acc: 0.7392 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2332  | total loss: \u001b[1m\u001b[32m0.58268\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2332 | loss: 0.58268 - acc: 0.7197 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2333  | total loss: \u001b[1m\u001b[32m0.57326\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2333 | loss: 0.57326 - acc: 0.7280 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2334  | total loss: \u001b[1m\u001b[32m0.59130\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2334 | loss: 0.59130 - acc: 0.7090 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2335  | total loss: \u001b[1m\u001b[32m0.58196\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2335 | loss: 0.58196 - acc: 0.7192 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2336  | total loss: \u001b[1m\u001b[32m0.59909\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2336 | loss: 0.59909 - acc: 0.7009 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2337  | total loss: \u001b[1m\u001b[32m0.58979\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2337 | loss: 0.58979 - acc: 0.7115 -- iter: 891/891\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 2338  | total loss: \u001b[1m\u001b[32m0.58154\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2338 | loss: 0.58154 - acc: 0.7213 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2339  | total loss: \u001b[1m\u001b[32m0.57382\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2339 | loss: 0.57382 - acc: 0.7301 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2340  | total loss: \u001b[1m\u001b[32m0.56624\u001b[0m\u001b[0m | time: 0.011s\n",
      "| Adam | epoch: 2340 | loss: 0.56624 - acc: 0.7385 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2341  | total loss: \u001b[1m\u001b[32m0.55864\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2341 | loss: 0.55864 - acc: 0.7460 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2342  | total loss: \u001b[1m\u001b[32m0.57726\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2342 | loss: 0.57726 - acc: 0.7274 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2343  | total loss: \u001b[1m\u001b[32m0.56742\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2343 | loss: 0.56742 - acc: 0.7361 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2344  | total loss: \u001b[1m\u001b[32m0.58843\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2344 | loss: 0.58843 - acc: 0.7167 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2345  | total loss: \u001b[1m\u001b[32m0.57713\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2345 | loss: 0.57713 - acc: 0.7269 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2346  | total loss: \u001b[1m\u001b[32m0.56688\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2346 | loss: 0.56688 - acc: 0.7358 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2347  | total loss: \u001b[1m\u001b[32m0.55747\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 2347 | loss: 0.55747 - acc: 0.7432 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2348  | total loss: \u001b[1m\u001b[32m0.54876\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 2348 | loss: 0.54876 - acc: 0.7496 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2349  | total loss: \u001b[1m\u001b[32m0.54065\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2349 | loss: 0.54065 - acc: 0.7553 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2350  | total loss: \u001b[1m\u001b[32m0.53306\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 2350 | loss: 0.53306 - acc: 0.7606 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2351  | total loss: \u001b[1m\u001b[32m0.52593\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2351 | loss: 0.52593 - acc: 0.7657 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2352  | total loss: \u001b[1m\u001b[32m0.51927\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2352 | loss: 0.51927 - acc: 0.7704 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2353  | total loss: \u001b[1m\u001b[32m0.51305\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2353 | loss: 0.51305 - acc: 0.7745 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2354  | total loss: \u001b[1m\u001b[32m0.50725\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2354 | loss: 0.50725 - acc: 0.7780 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2355  | total loss: \u001b[1m\u001b[32m0.50184\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 2355 | loss: 0.50184 - acc: 0.7811 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2356  | total loss: \u001b[1m\u001b[32m0.54263\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2356 | loss: 0.54263 - acc: 0.7569 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2357  | total loss: \u001b[1m\u001b[32m0.53323\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2357 | loss: 0.53323 - acc: 0.7622 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2358  | total loss: \u001b[1m\u001b[32m0.52458\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2358 | loss: 0.52458 - acc: 0.7670 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2359  | total loss: \u001b[1m\u001b[32m0.51669\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2359 | loss: 0.51669 - acc: 0.7713 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2360  | total loss: \u001b[1m\u001b[32m0.55486\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2360 | loss: 0.55486 - acc: 0.7482 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2361  | total loss: \u001b[1m\u001b[32m0.54442\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2361 | loss: 0.54442 - acc: 0.7549 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2362  | total loss: \u001b[1m\u001b[32m0.53562\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 2362 | loss: 0.53562 - acc: 0.7601 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2363  | total loss: \u001b[1m\u001b[32m0.52812\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2363 | loss: 0.52812 - acc: 0.7645 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2364  | total loss: \u001b[1m\u001b[32m0.52146\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2364 | loss: 0.52146 - acc: 0.7684 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2365  | total loss: \u001b[1m\u001b[32m0.51522\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2365 | loss: 0.51522 - acc: 0.7720 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2366  | total loss: \u001b[1m\u001b[32m0.50912\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 2366 | loss: 0.50912 - acc: 0.7751 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2367  | total loss: \u001b[1m\u001b[32m0.50313\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2367 | loss: 0.50313 - acc: 0.7782 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2368  | total loss: \u001b[1m\u001b[32m0.49739\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2368 | loss: 0.49739 - acc: 0.7812 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2369  | total loss: \u001b[1m\u001b[32m0.49208\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2369 | loss: 0.49208 - acc: 0.7843 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2370  | total loss: \u001b[1m\u001b[32m0.48731\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2370 | loss: 0.48731 - acc: 0.7865 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2371  | total loss: \u001b[1m\u001b[32m0.48314\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 2371 | loss: 0.48314 - acc: 0.7882 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2372  | total loss: \u001b[1m\u001b[32m0.54014\u001b[0m\u001b[0m | time: 0.011s\n",
      "| Adam | epoch: 2372 | loss: 0.54014 - acc: 0.7601 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2373  | total loss: \u001b[1m\u001b[32m0.53083\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2373 | loss: 0.53083 - acc: 0.7639 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2374  | total loss: \u001b[1m\u001b[32m0.52234\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 2374 | loss: 0.52234 - acc: 0.7673 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2375  | total loss: \u001b[1m\u001b[32m0.51454\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2375 | loss: 0.51454 - acc: 0.7707 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2376  | total loss: \u001b[1m\u001b[32m0.55376\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2376 | loss: 0.55376 - acc: 0.7482 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2377  | total loss: \u001b[1m\u001b[32m0.54295\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2377 | loss: 0.54295 - acc: 0.7545 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2378  | total loss: \u001b[1m\u001b[32m0.53351\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2378 | loss: 0.53351 - acc: 0.7604 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2379  | total loss: \u001b[1m\u001b[32m0.52529\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2379 | loss: 0.52529 - acc: 0.7651 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2380  | total loss: \u001b[1m\u001b[32m0.51817\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2380 | loss: 0.51817 - acc: 0.7689 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2381  | total loss: \u001b[1m\u001b[32m0.51186\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2381 | loss: 0.51186 - acc: 0.7719 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2382  | total loss: \u001b[1m\u001b[32m0.54772\u001b[0m\u001b[0m | time: 0.011s\n",
      "| Adam | epoch: 2382 | loss: 0.54772 - acc: 0.7473 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2383  | total loss: \u001b[1m\u001b[32m0.53866\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2383 | loss: 0.53866 - acc: 0.7530 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2384  | total loss: \u001b[1m\u001b[32m0.53072\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 2384 | loss: 0.53072 - acc: 0.7590 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2385  | total loss: \u001b[1m\u001b[32m0.52353\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 2385 | loss: 0.52353 - acc: 0.7644 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2386  | total loss: \u001b[1m\u001b[32m0.51681\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2386 | loss: 0.51681 - acc: 0.7694 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2387  | total loss: \u001b[1m\u001b[32m0.51047\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 2387 | loss: 0.51047 - acc: 0.7736 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2388  | total loss: \u001b[1m\u001b[32m0.54194\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2388 | loss: 0.54194 - acc: 0.7516 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2389  | total loss: \u001b[1m\u001b[32m0.53300\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2389 | loss: 0.53300 - acc: 0.7571 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2390  | total loss: \u001b[1m\u001b[32m0.56366\u001b[0m\u001b[0m | time: 0.012s\n",
      "| Adam | epoch: 2390 | loss: 0.56366 - acc: 0.7344 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2391  | total loss: \u001b[1m\u001b[32m0.55300\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2391 | loss: 0.55300 - acc: 0.7418 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2392  | total loss: \u001b[1m\u001b[32m0.54371\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2392 | loss: 0.54371 - acc: 0.7489 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2393  | total loss: \u001b[1m\u001b[32m0.53551\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2393 | loss: 0.53551 - acc: 0.7553 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2394  | total loss: \u001b[1m\u001b[32m0.56497\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2394 | loss: 0.56497 - acc: 0.7330 -- iter: 891/891\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 2395  | total loss: \u001b[1m\u001b[32m0.55520\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2395 | loss: 0.55520 - acc: 0.7408 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2396  | total loss: \u001b[1m\u001b[32m0.54674\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2396 | loss: 0.54674 - acc: 0.7480 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2397  | total loss: \u001b[1m\u001b[32m0.53923\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2397 | loss: 0.53923 - acc: 0.7544 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2398  | total loss: \u001b[1m\u001b[32m0.53236\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2398 | loss: 0.53236 - acc: 0.7602 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2399  | total loss: \u001b[1m\u001b[32m0.52587\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2399 | loss: 0.52587 - acc: 0.7655 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2400  | total loss: \u001b[1m\u001b[32m0.55505\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2400 | loss: 0.55505 - acc: 0.7413 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2401  | total loss: \u001b[1m\u001b[32m0.54598\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2401 | loss: 0.54598 - acc: 0.7486 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2402  | total loss: \u001b[1m\u001b[32m0.53777\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2402 | loss: 0.53777 - acc: 0.7551 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2403  | total loss: \u001b[1m\u001b[32m0.53018\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2403 | loss: 0.53018 - acc: 0.7608 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2404  | total loss: \u001b[1m\u001b[32m0.52308\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2404 | loss: 0.52308 - acc: 0.7665 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2405  | total loss: \u001b[1m\u001b[32m0.51640\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2405 | loss: 0.51640 - acc: 0.7714 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2406  | total loss: \u001b[1m\u001b[32m0.55280\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2406 | loss: 0.55280 - acc: 0.7463 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2407  | total loss: \u001b[1m\u001b[32m0.54286\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2407 | loss: 0.54286 - acc: 0.7532 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2408  | total loss: \u001b[1m\u001b[32m0.57467\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2408 | loss: 0.57467 - acc: 0.7310 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2409  | total loss: \u001b[1m\u001b[32m0.56283\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2409 | loss: 0.56283 - acc: 0.7392 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2410  | total loss: \u001b[1m\u001b[32m0.55244\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2410 | loss: 0.55244 - acc: 0.7462 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2411  | total loss: \u001b[1m\u001b[32m0.54320\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2411 | loss: 0.54320 - acc: 0.7529 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2412  | total loss: \u001b[1m\u001b[32m0.57160\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2412 | loss: 0.57160 - acc: 0.7302 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2413  | total loss: \u001b[1m\u001b[32m0.56101\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2413 | loss: 0.56101 - acc: 0.7381 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2414  | total loss: \u001b[1m\u001b[32m0.58406\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2414 | loss: 0.58406 - acc: 0.7186 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2415  | total loss: \u001b[1m\u001b[32m0.57336\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2415 | loss: 0.57336 - acc: 0.7280 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2416  | total loss: \u001b[1m\u001b[32m0.56418\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2416 | loss: 0.56418 - acc: 0.7360 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2417  | total loss: \u001b[1m\u001b[32m0.55597\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2417 | loss: 0.55597 - acc: 0.7427 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2418  | total loss: \u001b[1m\u001b[32m0.54827\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 2418 | loss: 0.54827 - acc: 0.7492 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2419  | total loss: \u001b[1m\u001b[32m0.54079\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2419 | loss: 0.54079 - acc: 0.7553 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2420  | total loss: \u001b[1m\u001b[32m0.56755\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2420 | loss: 0.56755 - acc: 0.7329 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2421  | total loss: \u001b[1m\u001b[32m0.55746\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2421 | loss: 0.55746 - acc: 0.7408 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2422  | total loss: \u001b[1m\u001b[32m0.54821\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 2422 | loss: 0.54821 - acc: 0.7481 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2423  | total loss: \u001b[1m\u001b[32m0.53965\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2423 | loss: 0.53965 - acc: 0.7547 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2424  | total loss: \u001b[1m\u001b[32m0.56736\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2424 | loss: 0.56736 - acc: 0.7335 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2425  | total loss: \u001b[1m\u001b[32m0.55677\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2425 | loss: 0.55677 - acc: 0.7410 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2426  | total loss: \u001b[1m\u001b[32m0.54730\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 2426 | loss: 0.54730 - acc: 0.7476 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2427  | total loss: \u001b[1m\u001b[32m0.53866\u001b[0m\u001b[0m | time: 0.018s\n",
      "| Adam | epoch: 2427 | loss: 0.53866 - acc: 0.7539 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2428  | total loss: \u001b[1m\u001b[32m0.53070\u001b[0m\u001b[0m | time: 0.013s\n",
      "| Adam | epoch: 2428 | loss: 0.53070 - acc: 0.7597 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2429  | total loss: \u001b[1m\u001b[32m0.52337\u001b[0m\u001b[0m | time: 0.011s\n",
      "| Adam | epoch: 2429 | loss: 0.52337 - acc: 0.7655 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2430  | total loss: \u001b[1m\u001b[32m0.55561\u001b[0m\u001b[0m | time: 0.011s\n",
      "| Adam | epoch: 2430 | loss: 0.55561 - acc: 0.7429 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2431  | total loss: \u001b[1m\u001b[32m0.54564\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2431 | loss: 0.54564 - acc: 0.7504 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2432  | total loss: \u001b[1m\u001b[32m0.53665\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2432 | loss: 0.53665 - acc: 0.7568 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2433  | total loss: \u001b[1m\u001b[32m0.52853\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2433 | loss: 0.52853 - acc: 0.7622 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2434  | total loss: \u001b[1m\u001b[32m0.55940\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2434 | loss: 0.55940 - acc: 0.7402 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2435  | total loss: \u001b[1m\u001b[32m0.54924\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2435 | loss: 0.54924 - acc: 0.7471 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2436  | total loss: \u001b[1m\u001b[32m0.54027\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2436 | loss: 0.54027 - acc: 0.7533 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2437  | total loss: \u001b[1m\u001b[32m0.53225\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2437 | loss: 0.53225 - acc: 0.7592 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2438  | total loss: \u001b[1m\u001b[32m0.56441\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2438 | loss: 0.56441 - acc: 0.7338 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2439  | total loss: \u001b[1m\u001b[32m0.55434\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2439 | loss: 0.55434 - acc: 0.7418 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2440  | total loss: \u001b[1m\u001b[32m0.54548\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2440 | loss: 0.54548 - acc: 0.7489 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2441  | total loss: \u001b[1m\u001b[32m0.53747\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2441 | loss: 0.53747 - acc: 0.7548 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2442  | total loss: \u001b[1m\u001b[32m0.56347\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2442 | loss: 0.56347 - acc: 0.7336 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2443  | total loss: \u001b[1m\u001b[32m0.55360\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2443 | loss: 0.55360 - acc: 0.7417 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2444  | total loss: \u001b[1m\u001b[32m0.57708\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2444 | loss: 0.57708 - acc: 0.7217 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2445  | total loss: \u001b[1m\u001b[32m0.56639\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2445 | loss: 0.56639 - acc: 0.7306 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2446  | total loss: \u001b[1m\u001b[32m0.58905\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2446 | loss: 0.58905 - acc: 0.7113 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2447  | total loss: \u001b[1m\u001b[32m0.57817\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2447 | loss: 0.57817 - acc: 0.7212 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2448  | total loss: \u001b[1m\u001b[32m0.56880\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2448 | loss: 0.56880 - acc: 0.7300 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2449  | total loss: \u001b[1m\u001b[32m0.56048\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2449 | loss: 0.56048 - acc: 0.7381 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2450  | total loss: \u001b[1m\u001b[32m0.55281\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2450 | loss: 0.55281 - acc: 0.7453 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2451  | total loss: \u001b[1m\u001b[32m0.54552\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2451 | loss: 0.54552 - acc: 0.7521 -- iter: 891/891\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 2452  | total loss: \u001b[1m\u001b[32m0.53844\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2452 | loss: 0.53844 - acc: 0.7582 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2453  | total loss: \u001b[1m\u001b[32m0.53153\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2453 | loss: 0.53153 - acc: 0.7642 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2454  | total loss: \u001b[1m\u001b[32m0.52483\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2454 | loss: 0.52483 - acc: 0.7694 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2455  | total loss: \u001b[1m\u001b[32m0.51842\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2455 | loss: 0.51842 - acc: 0.7736 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2456  | total loss: \u001b[1m\u001b[32m0.51236\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2456 | loss: 0.51236 - acc: 0.7774 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2457  | total loss: \u001b[1m\u001b[32m0.50673\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2457 | loss: 0.50673 - acc: 0.7808 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2458  | total loss: \u001b[1m\u001b[32m0.50158\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2458 | loss: 0.50158 - acc: 0.7839 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2459  | total loss: \u001b[1m\u001b[32m0.49692\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2459 | loss: 0.49692 - acc: 0.7861 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2460  | total loss: \u001b[1m\u001b[32m0.49269\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2460 | loss: 0.49269 - acc: 0.7878 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2461  | total loss: \u001b[1m\u001b[32m0.48878\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2461 | loss: 0.48878 - acc: 0.7890 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2462  | total loss: \u001b[1m\u001b[32m0.54141\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2462 | loss: 0.54141 - acc: 0.7645 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2463  | total loss: \u001b[1m\u001b[32m0.53206\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2463 | loss: 0.53206 - acc: 0.7691 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2464  | total loss: \u001b[1m\u001b[32m0.57297\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2464 | loss: 0.57297 - acc: 0.7456 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2465  | total loss: \u001b[1m\u001b[32m0.56003\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2465 | loss: 0.56003 - acc: 0.7520 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2466  | total loss: \u001b[1m\u001b[32m0.58866\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2466 | loss: 0.58866 - acc: 0.7331 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2467  | total loss: \u001b[1m\u001b[32m0.57571\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2467 | loss: 0.57571 - acc: 0.7401 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2468  | total loss: \u001b[1m\u001b[32m0.59795\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2468 | loss: 0.59795 - acc: 0.7199 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2469  | total loss: \u001b[1m\u001b[32m0.58733\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2469 | loss: 0.58733 - acc: 0.7275 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2470  | total loss: \u001b[1m\u001b[32m0.57892\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2470 | loss: 0.57892 - acc: 0.7342 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2471  | total loss: \u001b[1m\u001b[32m0.57171\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2471 | loss: 0.57171 - acc: 0.7403 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2472  | total loss: \u001b[1m\u001b[32m0.56495\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2472 | loss: 0.56495 - acc: 0.7462 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2473  | total loss: \u001b[1m\u001b[32m0.55815\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2473 | loss: 0.55815 - acc: 0.7521 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2474  | total loss: \u001b[1m\u001b[32m0.55107\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2474 | loss: 0.55107 - acc: 0.7579 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2475  | total loss: \u001b[1m\u001b[32m0.54371\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2475 | loss: 0.54371 - acc: 0.7634 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2476  | total loss: \u001b[1m\u001b[32m0.56490\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2476 | loss: 0.56490 - acc: 0.7434 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2477  | total loss: \u001b[1m\u001b[32m0.55523\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2477 | loss: 0.55523 - acc: 0.7508 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2478  | total loss: \u001b[1m\u001b[32m0.54638\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2478 | loss: 0.54638 - acc: 0.7569 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2479  | total loss: \u001b[1m\u001b[32m0.53831\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2479 | loss: 0.53831 - acc: 0.7619 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2480  | total loss: \u001b[1m\u001b[32m0.53095\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2480 | loss: 0.53095 - acc: 0.7662 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2481  | total loss: \u001b[1m\u001b[32m0.52427\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2481 | loss: 0.52427 - acc: 0.7703 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2482  | total loss: \u001b[1m\u001b[32m0.56670\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2482 | loss: 0.56670 - acc: 0.7440 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2483  | total loss: \u001b[1m\u001b[32m0.55638\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2483 | loss: 0.55638 - acc: 0.7503 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2484  | total loss: \u001b[1m\u001b[32m0.54705\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2484 | loss: 0.54705 - acc: 0.7559 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2485  | total loss: \u001b[1m\u001b[32m0.53856\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2485 | loss: 0.53856 - acc: 0.7610 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2486  | total loss: \u001b[1m\u001b[32m0.53075\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2486 | loss: 0.53075 - acc: 0.7660 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2487  | total loss: \u001b[1m\u001b[32m0.52353\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2487 | loss: 0.52353 - acc: 0.7709 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2488  | total loss: \u001b[1m\u001b[32m0.51683\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2488 | loss: 0.51683 - acc: 0.7754 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2489  | total loss: \u001b[1m\u001b[32m0.51059\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2489 | loss: 0.51059 - acc: 0.7798 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2490  | total loss: \u001b[1m\u001b[32m0.54731\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2490 | loss: 0.54731 - acc: 0.7555 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2491  | total loss: \u001b[1m\u001b[32m0.53776\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2491 | loss: 0.53776 - acc: 0.7611 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2492  | total loss: \u001b[1m\u001b[32m0.56677\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2492 | loss: 0.56677 - acc: 0.7401 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2493  | total loss: \u001b[1m\u001b[32m0.55600\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2493 | loss: 0.55600 - acc: 0.7470 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2494  | total loss: \u001b[1m\u001b[32m0.54696\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2494 | loss: 0.54696 - acc: 0.7535 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2495  | total loss: \u001b[1m\u001b[32m0.53929\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2495 | loss: 0.53929 - acc: 0.7596 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2496  | total loss: \u001b[1m\u001b[32m0.56450\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2496 | loss: 0.56450 - acc: 0.7394 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2497  | total loss: \u001b[1m\u001b[32m0.55581\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2497 | loss: 0.55581 - acc: 0.7466 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2498  | total loss: \u001b[1m\u001b[32m0.57864\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2498 | loss: 0.57864 - acc: 0.7271 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2499  | total loss: \u001b[1m\u001b[32m0.56918\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2499 | loss: 0.56918 - acc: 0.7346 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2500  | total loss: \u001b[1m\u001b[32m0.58933\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2500 | loss: 0.58933 - acc: 0.7150 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2501  | total loss: \u001b[1m\u001b[32m0.57944\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2501 | loss: 0.57944 - acc: 0.7239 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2502  | total loss: \u001b[1m\u001b[32m0.57060\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2502 | loss: 0.57060 - acc: 0.7320 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2503  | total loss: \u001b[1m\u001b[32m0.56233\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2503 | loss: 0.56233 - acc: 0.7400 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2504  | total loss: \u001b[1m\u001b[32m0.58052\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2504 | loss: 0.58052 - acc: 0.7226 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2505  | total loss: \u001b[1m\u001b[32m0.57082\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2505 | loss: 0.57082 - acc: 0.7316 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2506  | total loss: \u001b[1m\u001b[32m0.58911\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2506 | loss: 0.58911 - acc: 0.7127 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2507  | total loss: \u001b[1m\u001b[32m0.57856\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2507 | loss: 0.57856 - acc: 0.7228 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2508  | total loss: \u001b[1m\u001b[32m0.56906\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2508 | loss: 0.56906 - acc: 0.7320 -- iter: 891/891\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 2509  | total loss: \u001b[1m\u001b[32m0.56033\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2509 | loss: 0.56033 - acc: 0.7408 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2510  | total loss: \u001b[1m\u001b[32m0.55216\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2510 | loss: 0.55216 - acc: 0.7485 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2511  | total loss: \u001b[1m\u001b[32m0.54439\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2511 | loss: 0.54439 - acc: 0.7555 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2512  | total loss: \u001b[1m\u001b[32m0.56666\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2512 | loss: 0.56666 - acc: 0.7384 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2513  | total loss: \u001b[1m\u001b[32m0.55695\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2513 | loss: 0.55695 - acc: 0.7457 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2514  | total loss: \u001b[1m\u001b[32m0.58265\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2514 | loss: 0.58265 - acc: 0.7262 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2515  | total loss: \u001b[1m\u001b[32m0.57141\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2515 | loss: 0.57141 - acc: 0.7351 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2516  | total loss: \u001b[1m\u001b[32m0.56140\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2516 | loss: 0.56140 - acc: 0.7432 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2517  | total loss: \u001b[1m\u001b[32m0.55236\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2517 | loss: 0.55236 - acc: 0.7506 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2518  | total loss: \u001b[1m\u001b[32m0.54408\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2518 | loss: 0.54408 - acc: 0.7574 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2519  | total loss: \u001b[1m\u001b[32m0.53640\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2519 | loss: 0.53640 - acc: 0.7635 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2520  | total loss: \u001b[1m\u001b[32m0.56094\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2520 | loss: 0.56094 - acc: 0.7434 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2521  | total loss: \u001b[1m\u001b[32m0.55133\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2521 | loss: 0.55133 - acc: 0.7506 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2522  | total loss: \u001b[1m\u001b[32m0.57493\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2522 | loss: 0.57493 - acc: 0.7310 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2523  | total loss: \u001b[1m\u001b[32m0.56431\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2523 | loss: 0.56431 - acc: 0.7394 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2524  | total loss: \u001b[1m\u001b[32m0.58541\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2524 | loss: 0.58541 - acc: 0.7201 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2525  | total loss: \u001b[1m\u001b[32m0.57478\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2525 | loss: 0.57478 - acc: 0.7290 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2526  | total loss: \u001b[1m\u001b[32m0.59349\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2526 | loss: 0.59349 - acc: 0.7102 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2527  | total loss: \u001b[1m\u001b[32m0.58354\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2527 | loss: 0.58354 - acc: 0.7200 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2528  | total loss: \u001b[1m\u001b[32m0.57510\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2528 | loss: 0.57510 - acc: 0.7288 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2529  | total loss: \u001b[1m\u001b[32m0.56756\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2529 | loss: 0.56756 - acc: 0.7367 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2530  | total loss: \u001b[1m\u001b[32m0.56044\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2530 | loss: 0.56044 - acc: 0.7441 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2531  | total loss: \u001b[1m\u001b[32m0.55342\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2531 | loss: 0.55342 - acc: 0.7506 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2532  | total loss: \u001b[1m\u001b[32m0.57240\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2532 | loss: 0.57240 - acc: 0.7327 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2533  | total loss: \u001b[1m\u001b[32m0.56308\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2533 | loss: 0.56308 - acc: 0.7407 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2534  | total loss: \u001b[1m\u001b[32m0.58312\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2534 | loss: 0.58312 - acc: 0.7207 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2535  | total loss: \u001b[1m\u001b[32m0.57233\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2535 | loss: 0.57233 - acc: 0.7298 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2536  | total loss: \u001b[1m\u001b[32m0.59196\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2536 | loss: 0.59196 - acc: 0.7116 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2537  | total loss: \u001b[1m\u001b[32m0.58041\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2537 | loss: 0.58041 - acc: 0.7219 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2538  | total loss: \u001b[1m\u001b[32m0.59886\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2538 | loss: 0.59886 - acc: 0.7040 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2539  | total loss: \u001b[1m\u001b[32m0.58713\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2539 | loss: 0.58713 - acc: 0.7145 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2540  | total loss: \u001b[1m\u001b[32m0.60626\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2540 | loss: 0.60626 - acc: 0.6961 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2541  | total loss: \u001b[1m\u001b[32m0.59465\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2541 | loss: 0.59465 - acc: 0.7076 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2542  | total loss: \u001b[1m\u001b[32m0.61059\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2542 | loss: 0.61059 - acc: 0.6906 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2543  | total loss: \u001b[1m\u001b[32m0.59963\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2543 | loss: 0.59963 - acc: 0.7027 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2544  | total loss: \u001b[1m\u001b[32m0.59017\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2544 | loss: 0.59017 - acc: 0.7133 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2545  | total loss: \u001b[1m\u001b[32m0.58172\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2545 | loss: 0.58172 - acc: 0.7234 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2546  | total loss: \u001b[1m\u001b[32m0.59707\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2546 | loss: 0.59707 - acc: 0.7043 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2547  | total loss: \u001b[1m\u001b[32m0.58781\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2547 | loss: 0.58781 - acc: 0.7152 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2548  | total loss: \u001b[1m\u001b[32m0.57929\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2548 | loss: 0.57929 - acc: 0.7250 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2549  | total loss: \u001b[1m\u001b[32m0.57120\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2549 | loss: 0.57120 - acc: 0.7341 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2550  | total loss: \u001b[1m\u001b[32m0.56329\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2550 | loss: 0.56329 - acc: 0.7421 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2551  | total loss: \u001b[1m\u001b[32m0.55543\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2551 | loss: 0.55543 - acc: 0.7493 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2552  | total loss: \u001b[1m\u001b[32m0.57750\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2552 | loss: 0.57750 - acc: 0.7285 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2553  | total loss: \u001b[1m\u001b[32m0.56719\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2553 | loss: 0.56719 - acc: 0.7375 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2554  | total loss: \u001b[1m\u001b[32m0.55757\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2554 | loss: 0.55757 - acc: 0.7457 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2555  | total loss: \u001b[1m\u001b[32m0.54853\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2555 | loss: 0.54853 - acc: 0.7526 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2556  | total loss: \u001b[1m\u001b[32m0.54002\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2556 | loss: 0.54002 - acc: 0.7586 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2557  | total loss: \u001b[1m\u001b[32m0.53201\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2557 | loss: 0.53201 - acc: 0.7639 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2558  | total loss: \u001b[1m\u001b[32m0.56333\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2558 | loss: 0.56333 - acc: 0.7440 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2559  | total loss: \u001b[1m\u001b[32m0.55257\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2559 | loss: 0.55257 - acc: 0.7510 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2560  | total loss: \u001b[1m\u001b[32m0.58269\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2560 | loss: 0.58269 - acc: 0.7319 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2561  | total loss: \u001b[1m\u001b[32m0.56991\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2561 | loss: 0.56991 - acc: 0.7401 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2562  | total loss: \u001b[1m\u001b[32m0.59820\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2562 | loss: 0.59820 - acc: 0.7188 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2563  | total loss: \u001b[1m\u001b[32m0.58451\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2563 | loss: 0.58451 - acc: 0.7281 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2564  | total loss: \u001b[1m\u001b[32m0.60678\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2564 | loss: 0.60678 - acc: 0.7095 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2565  | total loss: \u001b[1m\u001b[32m0.59402\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2565 | loss: 0.59402 - acc: 0.7198 -- iter: 891/891\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 2566  | total loss: \u001b[1m\u001b[32m0.58347\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2566 | loss: 0.58347 - acc: 0.7289 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2567  | total loss: \u001b[1m\u001b[32m0.57443\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2567 | loss: 0.57443 - acc: 0.7370 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2568  | total loss: \u001b[1m\u001b[32m0.56627\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2568 | loss: 0.56627 - acc: 0.7443 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2569  | total loss: \u001b[1m\u001b[32m0.55852\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2569 | loss: 0.55852 - acc: 0.7506 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2570  | total loss: \u001b[1m\u001b[32m0.57924\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2570 | loss: 0.57924 - acc: 0.7304 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2571  | total loss: \u001b[1m\u001b[32m0.56937\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2571 | loss: 0.56937 - acc: 0.7383 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2572  | total loss: \u001b[1m\u001b[32m0.59072\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2572 | loss: 0.59072 - acc: 0.7188 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2573  | total loss: \u001b[1m\u001b[32m0.57945\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2573 | loss: 0.57945 - acc: 0.7283 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2574  | total loss: \u001b[1m\u001b[32m0.59921\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2574 | loss: 0.59921 - acc: 0.7091 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2575  | total loss: \u001b[1m\u001b[32m0.58743\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2575 | loss: 0.58743 - acc: 0.7193 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2576  | total loss: \u001b[1m\u001b[32m0.60415\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2576 | loss: 0.60415 - acc: 0.7018 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2577  | total loss: \u001b[1m\u001b[32m0.59256\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2577 | loss: 0.59256 - acc: 0.7128 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2578  | total loss: \u001b[1m\u001b[32m0.58240\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2578 | loss: 0.58240 - acc: 0.7228 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2579  | total loss: \u001b[1m\u001b[32m0.57326\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2579 | loss: 0.57326 - acc: 0.7318 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2580  | total loss: \u001b[1m\u001b[32m0.56479\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2580 | loss: 0.56479 - acc: 0.7397 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2581  | total loss: \u001b[1m\u001b[32m0.55675\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2581 | loss: 0.55675 - acc: 0.7472 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2582  | total loss: \u001b[1m\u001b[32m0.54898\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2582 | loss: 0.54898 - acc: 0.7542 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2583  | total loss: \u001b[1m\u001b[32m0.54141\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2583 | loss: 0.54141 - acc: 0.7604 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2584  | total loss: \u001b[1m\u001b[32m0.53406\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2584 | loss: 0.53406 - acc: 0.7655 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2585  | total loss: \u001b[1m\u001b[32m0.52696\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2585 | loss: 0.52696 - acc: 0.7700 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2586  | total loss: \u001b[1m\u001b[32m0.52021\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2586 | loss: 0.52021 - acc: 0.7739 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2587  | total loss: \u001b[1m\u001b[32m0.51386\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2587 | loss: 0.51386 - acc: 0.7777 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2588  | total loss: \u001b[1m\u001b[32m0.50798\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2588 | loss: 0.50798 - acc: 0.7810 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2589  | total loss: \u001b[1m\u001b[32m0.50260\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2589 | loss: 0.50260 - acc: 0.7838 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2590  | total loss: \u001b[1m\u001b[32m0.54911\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2590 | loss: 0.54911 - acc: 0.7598 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2591  | total loss: \u001b[1m\u001b[32m0.53934\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2591 | loss: 0.53934 - acc: 0.7637 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2592  | total loss: \u001b[1m\u001b[32m0.53029\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2592 | loss: 0.53029 - acc: 0.7676 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2593  | total loss: \u001b[1m\u001b[32m0.52189\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2593 | loss: 0.52189 - acc: 0.7718 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2594  | total loss: \u001b[1m\u001b[32m0.51417\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2594 | loss: 0.51417 - acc: 0.7756 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2595  | total loss: \u001b[1m\u001b[32m0.50716\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2595 | loss: 0.50716 - acc: 0.7793 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2596  | total loss: \u001b[1m\u001b[32m0.50084\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2596 | loss: 0.50084 - acc: 0.7823 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2597  | total loss: \u001b[1m\u001b[32m0.49519\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2597 | loss: 0.49519 - acc: 0.7843 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2598  | total loss: \u001b[1m\u001b[32m0.53471\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2598 | loss: 0.53471 - acc: 0.7608 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2599  | total loss: \u001b[1m\u001b[32m0.52619\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2599 | loss: 0.52619 - acc: 0.7650 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2600  | total loss: \u001b[1m\u001b[32m0.56183\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2600 | loss: 0.56183 - acc: 0.7436 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2601  | total loss: \u001b[1m\u001b[32m0.55194\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2601 | loss: 0.55194 - acc: 0.7492 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2602  | total loss: \u001b[1m\u001b[32m0.54362\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2602 | loss: 0.54362 - acc: 0.7542 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2603  | total loss: \u001b[1m\u001b[32m0.53618\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2603 | loss: 0.53618 - acc: 0.7585 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2604  | total loss: \u001b[1m\u001b[32m0.56546\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2604 | loss: 0.56546 - acc: 0.7368 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2605  | total loss: \u001b[1m\u001b[32m0.55584\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2605 | loss: 0.55584 - acc: 0.7432 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2606  | total loss: \u001b[1m\u001b[32m0.54716\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2606 | loss: 0.54716 - acc: 0.7491 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2607  | total loss: \u001b[1m\u001b[32m0.53912\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2607 | loss: 0.53912 - acc: 0.7548 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2608  | total loss: \u001b[1m\u001b[32m0.56621\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2608 | loss: 0.56621 - acc: 0.7334 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2609  | total loss: \u001b[1m\u001b[32m0.55629\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2609 | loss: 0.55629 - acc: 0.7410 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2610  | total loss: \u001b[1m\u001b[32m0.57851\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2610 | loss: 0.57851 - acc: 0.7196 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2611  | total loss: \u001b[1m\u001b[32m0.56809\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2611 | loss: 0.56809 - acc: 0.7287 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2612  | total loss: \u001b[1m\u001b[32m0.55910\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2612 | loss: 0.55910 - acc: 0.7370 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2613  | total loss: \u001b[1m\u001b[32m0.55114\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2613 | loss: 0.55114 - acc: 0.7443 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2614  | total loss: \u001b[1m\u001b[32m0.57326\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2614 | loss: 0.57326 - acc: 0.7246 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2615  | total loss: \u001b[1m\u001b[32m0.56413\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2615 | loss: 0.56413 - acc: 0.7334 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2616  | total loss: \u001b[1m\u001b[32m0.58587\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2616 | loss: 0.58587 - acc: 0.7130 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2617  | total loss: \u001b[1m\u001b[32m0.57611\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2617 | loss: 0.57611 - acc: 0.7228 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2618  | total loss: \u001b[1m\u001b[32m0.56760\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2618 | loss: 0.56760 - acc: 0.7316 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2619  | total loss: \u001b[1m\u001b[32m0.55995\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2619 | loss: 0.55995 - acc: 0.7395 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2620  | total loss: \u001b[1m\u001b[32m0.55287\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2620 | loss: 0.55287 - acc: 0.7466 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2621  | total loss: \u001b[1m\u001b[32m0.54611\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2621 | loss: 0.54611 - acc: 0.7532 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2622  | total loss: \u001b[1m\u001b[32m0.53952\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2622 | loss: 0.53952 - acc: 0.7593 -- iter: 891/891\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 2623  | total loss: \u001b[1m\u001b[32m0.53302\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2623 | loss: 0.53302 - acc: 0.7648 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2624  | total loss: \u001b[1m\u001b[32m0.55954\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2624 | loss: 0.55954 - acc: 0.7409 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2625  | total loss: \u001b[1m\u001b[32m0.55033\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2625 | loss: 0.55033 - acc: 0.7480 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2626  | total loss: \u001b[1m\u001b[32m0.54181\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2626 | loss: 0.54181 - acc: 0.7545 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2627  | total loss: \u001b[1m\u001b[32m0.53385\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2627 | loss: 0.53385 - acc: 0.7603 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2628  | total loss: \u001b[1m\u001b[32m0.56581\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2628 | loss: 0.56581 - acc: 0.7361 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2629  | total loss: \u001b[1m\u001b[32m0.55521\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2629 | loss: 0.55521 - acc: 0.7439 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2630  | total loss: \u001b[1m\u001b[32m0.58630\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2630 | loss: 0.58630 - acc: 0.7207 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2631  | total loss: \u001b[1m\u001b[32m0.57406\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2631 | loss: 0.57406 - acc: 0.7301 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2632  | total loss: \u001b[1m\u001b[32m0.56335\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2632 | loss: 0.56335 - acc: 0.7384 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2633  | total loss: \u001b[1m\u001b[32m0.55388\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2633 | loss: 0.55388 - acc: 0.7461 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2634  | total loss: \u001b[1m\u001b[32m0.57702\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2634 | loss: 0.57702 - acc: 0.7268 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2635  | total loss: \u001b[1m\u001b[32m0.56669\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2635 | loss: 0.56669 - acc: 0.7357 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2636  | total loss: \u001b[1m\u001b[32m0.58603\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2636 | loss: 0.58603 - acc: 0.7183 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2637  | total loss: \u001b[1m\u001b[32m0.57563\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2637 | loss: 0.57563 - acc: 0.7278 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2638  | total loss: \u001b[1m\u001b[32m0.56654\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2638 | loss: 0.56654 - acc: 0.7363 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2639  | total loss: \u001b[1m\u001b[32m0.55822\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2639 | loss: 0.55822 - acc: 0.7442 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2640  | total loss: \u001b[1m\u001b[32m0.57862\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2640 | loss: 0.57862 - acc: 0.7262 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2641  | total loss: \u001b[1m\u001b[32m0.56866\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2641 | loss: 0.56866 - acc: 0.7349 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2642  | total loss: \u001b[1m\u001b[32m0.55947\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 2642 | loss: 0.55947 - acc: 0.7429 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2643  | total loss: \u001b[1m\u001b[32m0.55079\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 2643 | loss: 0.55079 - acc: 0.7500 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2644  | total loss: \u001b[1m\u001b[32m0.57347\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2644 | loss: 0.57347 - acc: 0.7316 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2645  | total loss: \u001b[1m\u001b[32m0.56292\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 2645 | loss: 0.56292 - acc: 0.7397 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2646  | total loss: \u001b[1m\u001b[32m0.55331\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2646 | loss: 0.55331 - acc: 0.7469 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2647  | total loss: \u001b[1m\u001b[32m0.54445\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 2647 | loss: 0.54445 - acc: 0.7535 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2648  | total loss: \u001b[1m\u001b[32m0.57068\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2648 | loss: 0.57068 - acc: 0.7329 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2649  | total loss: \u001b[1m\u001b[32m0.55996\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 2649 | loss: 0.55996 - acc: 0.7410 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2650  | total loss: \u001b[1m\u001b[32m0.55033\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2650 | loss: 0.55033 - acc: 0.7483 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2651  | total loss: \u001b[1m\u001b[32m0.54157\u001b[0m\u001b[0m | time: 0.011s\n",
      "| Adam | epoch: 2651 | loss: 0.54157 - acc: 0.7548 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2652  | total loss: \u001b[1m\u001b[32m0.53352\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2652 | loss: 0.53352 - acc: 0.7606 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2653  | total loss: \u001b[1m\u001b[32m0.52605\u001b[0m\u001b[0m | time: 0.012s\n",
      "| Adam | epoch: 2653 | loss: 0.52605 - acc: 0.7663 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2654  | total loss: \u001b[1m\u001b[32m0.51908\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2654 | loss: 0.51908 - acc: 0.7715 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2655  | total loss: \u001b[1m\u001b[32m0.51254\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2655 | loss: 0.51254 - acc: 0.7754 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2656  | total loss: \u001b[1m\u001b[32m0.55253\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2656 | loss: 0.55253 - acc: 0.7496 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2657  | total loss: \u001b[1m\u001b[32m0.54237\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2657 | loss: 0.54237 - acc: 0.7555 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2658  | total loss: \u001b[1m\u001b[32m0.57607\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2658 | loss: 0.57607 - acc: 0.7338 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2659  | total loss: \u001b[1m\u001b[32m0.56371\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 2659 | loss: 0.56371 - acc: 0.7414 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2660  | total loss: \u001b[1m\u001b[32m0.59221\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2660 | loss: 0.59221 - acc: 0.7207 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2661  | total loss: \u001b[1m\u001b[32m0.57925\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2661 | loss: 0.57925 - acc: 0.7295 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2662  | total loss: \u001b[1m\u001b[32m0.56825\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2662 | loss: 0.56825 - acc: 0.7370 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2663  | total loss: \u001b[1m\u001b[32m0.55870\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2663 | loss: 0.55870 - acc: 0.7438 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2664  | total loss: \u001b[1m\u001b[32m0.55010\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2664 | loss: 0.55010 - acc: 0.7500 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2665  | total loss: \u001b[1m\u001b[32m0.54206\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2665 | loss: 0.54206 - acc: 0.7555 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2666  | total loss: \u001b[1m\u001b[32m0.53433\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2666 | loss: 0.53433 - acc: 0.7613 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2667  | total loss: \u001b[1m\u001b[32m0.52681\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2667 | loss: 0.52681 - acc: 0.7663 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2668  | total loss: \u001b[1m\u001b[32m0.51957\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2668 | loss: 0.51957 - acc: 0.7710 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2669  | total loss: \u001b[1m\u001b[32m0.51272\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2669 | loss: 0.51272 - acc: 0.7747 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2670  | total loss: \u001b[1m\u001b[32m0.50637\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2670 | loss: 0.50637 - acc: 0.7782 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2671  | total loss: \u001b[1m\u001b[32m0.50063\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2671 | loss: 0.50063 - acc: 0.7813 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2672  | total loss: \u001b[1m\u001b[32m0.54679\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2672 | loss: 0.54679 - acc: 0.7567 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2673  | total loss: \u001b[1m\u001b[32m0.53701\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 2673 | loss: 0.53701 - acc: 0.7618 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2674  | total loss: \u001b[1m\u001b[32m0.52812\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2674 | loss: 0.52812 - acc: 0.7661 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2675  | total loss: \u001b[1m\u001b[32m0.52000\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2675 | loss: 0.52000 - acc: 0.7699 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2676  | total loss: \u001b[1m\u001b[32m0.51256\u001b[0m\u001b[0m | time: 0.011s\n",
      "| Adam | epoch: 2676 | loss: 0.51256 - acc: 0.7736 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2677  | total loss: \u001b[1m\u001b[32m0.50576\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2677 | loss: 0.50576 - acc: 0.7770 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2678  | total loss: \u001b[1m\u001b[32m0.49954\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2678 | loss: 0.49954 - acc: 0.7801 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2679  | total loss: \u001b[1m\u001b[32m0.49388\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2679 | loss: 0.49388 - acc: 0.7830 -- iter: 891/891\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 2680  | total loss: \u001b[1m\u001b[32m0.48875\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2680 | loss: 0.48875 - acc: 0.7852 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2681  | total loss: \u001b[1m\u001b[32m0.48415\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2681 | loss: 0.48415 - acc: 0.7868 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2682  | total loss: \u001b[1m\u001b[32m0.48002\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2682 | loss: 0.48002 - acc: 0.7883 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2683  | total loss: \u001b[1m\u001b[32m0.47629\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2683 | loss: 0.47629 - acc: 0.7896 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2684  | total loss: \u001b[1m\u001b[32m0.47291\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2684 | loss: 0.47291 - acc: 0.7908 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2685  | total loss: \u001b[1m\u001b[32m0.46983\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2685 | loss: 0.46983 - acc: 0.7919 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2686  | total loss: \u001b[1m\u001b[32m0.51946\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 2686 | loss: 0.51946 - acc: 0.7671 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2687  | total loss: \u001b[1m\u001b[32m0.51178\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2687 | loss: 0.51178 - acc: 0.7706 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2688  | total loss: \u001b[1m\u001b[32m0.55702\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 2688 | loss: 0.55702 - acc: 0.7458 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2689  | total loss: \u001b[1m\u001b[32m0.54618\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2689 | loss: 0.54618 - acc: 0.7516 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2690  | total loss: \u001b[1m\u001b[32m0.58059\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2690 | loss: 0.58059 - acc: 0.7290 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2691  | total loss: \u001b[1m\u001b[32m0.56899\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2691 | loss: 0.56899 - acc: 0.7362 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2692  | total loss: \u001b[1m\u001b[32m0.55939\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2692 | loss: 0.55939 - acc: 0.7425 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2693  | total loss: \u001b[1m\u001b[32m0.55109\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2693 | loss: 0.55109 - acc: 0.7480 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2694  | total loss: \u001b[1m\u001b[32m0.57618\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2694 | loss: 0.57618 - acc: 0.7267 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2695  | total loss: \u001b[1m\u001b[32m0.56684\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2695 | loss: 0.56684 - acc: 0.7344 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2696  | total loss: \u001b[1m\u001b[32m0.55869\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2696 | loss: 0.55869 - acc: 0.7416 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2697  | total loss: \u001b[1m\u001b[32m0.55128\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2697 | loss: 0.55128 - acc: 0.7485 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2698  | total loss: \u001b[1m\u001b[32m0.54433\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2698 | loss: 0.54433 - acc: 0.7544 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2699  | total loss: \u001b[1m\u001b[32m0.53770\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2699 | loss: 0.53770 - acc: 0.7598 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2700  | total loss: \u001b[1m\u001b[32m0.56020\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2700 | loss: 0.56020 - acc: 0.7399 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2701  | total loss: \u001b[1m\u001b[32m0.55158\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2701 | loss: 0.55158 - acc: 0.7468 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2702  | total loss: \u001b[1m\u001b[32m0.54375\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2702 | loss: 0.54375 - acc: 0.7529 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2703  | total loss: \u001b[1m\u001b[32m0.53655\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2703 | loss: 0.53655 - acc: 0.7586 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2704  | total loss: \u001b[1m\u001b[32m0.56506\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2704 | loss: 0.56506 - acc: 0.7360 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2705  | total loss: \u001b[1m\u001b[32m0.55560\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2705 | loss: 0.55560 - acc: 0.7432 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2706  | total loss: \u001b[1m\u001b[32m0.54706\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2706 | loss: 0.54706 - acc: 0.7493 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2707  | total loss: \u001b[1m\u001b[32m0.53926\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2707 | loss: 0.53926 - acc: 0.7551 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2708  | total loss: \u001b[1m\u001b[32m0.56795\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2708 | loss: 0.56795 - acc: 0.7332 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2709  | total loss: \u001b[1m\u001b[32m0.55800\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2709 | loss: 0.55800 - acc: 0.7407 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2710  | total loss: \u001b[1m\u001b[32m0.54910\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2710 | loss: 0.54910 - acc: 0.7478 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2711  | total loss: \u001b[1m\u001b[32m0.54105\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2711 | loss: 0.54105 - acc: 0.7546 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2712  | total loss: \u001b[1m\u001b[32m0.53365\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2712 | loss: 0.53365 - acc: 0.7605 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2713  | total loss: \u001b[1m\u001b[32m0.52677\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2713 | loss: 0.52677 - acc: 0.7659 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2714  | total loss: \u001b[1m\u001b[32m0.52027\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2714 | loss: 0.52027 - acc: 0.7708 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2715  | total loss: \u001b[1m\u001b[32m0.51407\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2715 | loss: 0.51407 - acc: 0.7752 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2716  | total loss: \u001b[1m\u001b[32m0.50811\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2716 | loss: 0.50811 - acc: 0.7794 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2717  | total loss: \u001b[1m\u001b[32m0.50240\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2717 | loss: 0.50240 - acc: 0.7827 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2718  | total loss: \u001b[1m\u001b[32m0.53650\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2718 | loss: 0.53650 - acc: 0.7615 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2719  | total loss: \u001b[1m\u001b[32m0.52763\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2719 | loss: 0.52763 - acc: 0.7659 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2720  | total loss: \u001b[1m\u001b[32m0.51961\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2720 | loss: 0.51961 - acc: 0.7701 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2721  | total loss: \u001b[1m\u001b[32m0.51231\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2721 | loss: 0.51231 - acc: 0.7738 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2722  | total loss: \u001b[1m\u001b[32m0.55110\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2722 | loss: 0.55110 - acc: 0.7501 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2723  | total loss: \u001b[1m\u001b[32m0.54089\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2723 | loss: 0.54089 - acc: 0.7563 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2724  | total loss: \u001b[1m\u001b[32m0.57674\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2724 | loss: 0.57674 - acc: 0.7320 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2725  | total loss: \u001b[1m\u001b[32m0.56517\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2725 | loss: 0.56517 - acc: 0.7391 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2726  | total loss: \u001b[1m\u001b[32m0.55547\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2726 | loss: 0.55547 - acc: 0.7456 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2727  | total loss: \u001b[1m\u001b[32m0.54700\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2727 | loss: 0.54700 - acc: 0.7512 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2728  | total loss: \u001b[1m\u001b[32m0.57276\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2728 | loss: 0.57276 - acc: 0.7308 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2729  | total loss: \u001b[1m\u001b[32m0.56291\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2729 | loss: 0.56291 - acc: 0.7384 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2730  | total loss: \u001b[1m\u001b[32m0.55414\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2730 | loss: 0.55414 - acc: 0.7456 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2731  | total loss: \u001b[1m\u001b[32m0.54598\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 2731 | loss: 0.54598 - acc: 0.7522 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2732  | total loss: \u001b[1m\u001b[32m0.53819\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2732 | loss: 0.53819 - acc: 0.7582 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2733  | total loss: \u001b[1m\u001b[32m0.53073\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2733 | loss: 0.53073 - acc: 0.7634 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2734  | total loss: \u001b[1m\u001b[32m0.56002\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2734 | loss: 0.56002 - acc: 0.7427 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2735  | total loss: \u001b[1m\u001b[32m0.55006\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2735 | loss: 0.55006 - acc: 0.7493 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2736  | total loss: \u001b[1m\u001b[32m0.54108\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2736 | loss: 0.54108 - acc: 0.7553 -- iter: 891/891\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 2737  | total loss: \u001b[1m\u001b[32m0.53295\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2737 | loss: 0.53295 - acc: 0.7610 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2738  | total loss: \u001b[1m\u001b[32m0.52554\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2738 | loss: 0.52554 - acc: 0.7661 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2739  | total loss: \u001b[1m\u001b[32m0.51875\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2739 | loss: 0.51875 - acc: 0.7706 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2740  | total loss: \u001b[1m\u001b[32m0.55557\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2740 | loss: 0.55557 - acc: 0.7470 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2741  | total loss: \u001b[1m\u001b[32m0.54563\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2741 | loss: 0.54563 - acc: 0.7539 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2742  | total loss: \u001b[1m\u001b[32m0.57529\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2742 | loss: 0.57529 - acc: 0.7327 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2743  | total loss: \u001b[1m\u001b[32m0.56368\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2743 | loss: 0.56368 - acc: 0.7401 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2744  | total loss: \u001b[1m\u001b[32m0.55343\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2744 | loss: 0.55343 - acc: 0.7468 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2745  | total loss: \u001b[1m\u001b[32m0.54435\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2745 | loss: 0.54435 - acc: 0.7534 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2746  | total loss: \u001b[1m\u001b[32m0.53626\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2746 | loss: 0.53626 - acc: 0.7593 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2747  | total loss: \u001b[1m\u001b[32m0.52893\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2747 | loss: 0.52893 - acc: 0.7648 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2748  | total loss: \u001b[1m\u001b[32m0.55546\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2748 | loss: 0.55546 - acc: 0.7448 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2749  | total loss: \u001b[1m\u001b[32m0.54623\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2749 | loss: 0.54623 - acc: 0.7515 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2750  | total loss: \u001b[1m\u001b[32m0.53807\u001b[0m\u001b[0m | time: 0.015s\n",
      "| Adam | epoch: 2750 | loss: 0.53807 - acc: 0.7568 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2751  | total loss: \u001b[1m\u001b[32m0.53065\u001b[0m\u001b[0m | time: 0.024s\n",
      "| Adam | epoch: 2751 | loss: 0.53065 - acc: 0.7614 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2752  | total loss: \u001b[1m\u001b[32m0.55785\u001b[0m\u001b[0m | time: 0.011s\n",
      "| Adam | epoch: 2752 | loss: 0.55785 - acc: 0.7407 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2753  | total loss: \u001b[1m\u001b[32m0.54838\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2753 | loss: 0.54838 - acc: 0.7478 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2754  | total loss: \u001b[1m\u001b[32m0.57950\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 2754 | loss: 0.57950 - acc: 0.7246 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2755  | total loss: \u001b[1m\u001b[32m0.56865\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2755 | loss: 0.56865 - acc: 0.7331 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2756  | total loss: \u001b[1m\u001b[32m0.55926\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2756 | loss: 0.55926 - acc: 0.7408 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2757  | total loss: \u001b[1m\u001b[32m0.55090\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2757 | loss: 0.55090 - acc: 0.7472 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2758  | total loss: \u001b[1m\u001b[32m0.54315\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2758 | loss: 0.54315 - acc: 0.7533 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2759  | total loss: \u001b[1m\u001b[32m0.53572\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2759 | loss: 0.53572 - acc: 0.7590 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2760  | total loss: \u001b[1m\u001b[32m0.56117\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2760 | loss: 0.56117 - acc: 0.7379 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2761  | total loss: \u001b[1m\u001b[32m0.55135\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2761 | loss: 0.55135 - acc: 0.7453 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2762  | total loss: \u001b[1m\u001b[32m0.54238\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2762 | loss: 0.54238 - acc: 0.7522 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2763  | total loss: \u001b[1m\u001b[32m0.53412\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2763 | loss: 0.53412 - acc: 0.7581 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2764  | total loss: \u001b[1m\u001b[32m0.52648\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2764 | loss: 0.52648 - acc: 0.7633 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2765  | total loss: \u001b[1m\u001b[32m0.51942\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2765 | loss: 0.51942 - acc: 0.7685 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2766  | total loss: \u001b[1m\u001b[32m0.55553\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2766 | loss: 0.55553 - acc: 0.7458 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2767  | total loss: \u001b[1m\u001b[32m0.54538\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2767 | loss: 0.54538 - acc: 0.7527 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2768  | total loss: \u001b[1m\u001b[32m0.57797\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2768 | loss: 0.57797 - acc: 0.7311 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2769  | total loss: \u001b[1m\u001b[32m0.56564\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2769 | loss: 0.56564 - acc: 0.7390 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2770  | total loss: \u001b[1m\u001b[32m0.55470\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2770 | loss: 0.55470 - acc: 0.7459 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2771  | total loss: \u001b[1m\u001b[32m0.54494\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2771 | loss: 0.54494 - acc: 0.7525 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2772  | total loss: \u001b[1m\u001b[32m0.53619\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2772 | loss: 0.53619 - acc: 0.7582 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2773  | total loss: \u001b[1m\u001b[32m0.52827\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2773 | loss: 0.52827 - acc: 0.7634 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2774  | total loss: \u001b[1m\u001b[32m0.55556\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2774 | loss: 0.55556 - acc: 0.7421 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2775  | total loss: \u001b[1m\u001b[32m0.54580\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2775 | loss: 0.54580 - acc: 0.7488 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2776  | total loss: \u001b[1m\u001b[32m0.53715\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2776 | loss: 0.53715 - acc: 0.7547 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2777  | total loss: \u001b[1m\u001b[32m0.52933\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2777 | loss: 0.52933 - acc: 0.7597 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2778  | total loss: \u001b[1m\u001b[32m0.52210\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2778 | loss: 0.52210 - acc: 0.7645 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2779  | total loss: \u001b[1m\u001b[32m0.51531\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2779 | loss: 0.51531 - acc: 0.7690 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2780  | total loss: \u001b[1m\u001b[32m0.50891\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2780 | loss: 0.50891 - acc: 0.7732 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2781  | total loss: \u001b[1m\u001b[32m0.50287\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2781 | loss: 0.50287 - acc: 0.7772 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2782  | total loss: \u001b[1m\u001b[32m0.49722\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2782 | loss: 0.49722 - acc: 0.7802 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2783  | total loss: \u001b[1m\u001b[32m0.49201\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2783 | loss: 0.49201 - acc: 0.7827 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2784  | total loss: \u001b[1m\u001b[32m0.53147\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2784 | loss: 0.53147 - acc: 0.7603 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2785  | total loss: \u001b[1m\u001b[32m0.52277\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2785 | loss: 0.52277 - acc: 0.7644 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2786  | total loss: \u001b[1m\u001b[32m0.56214\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2786 | loss: 0.56214 - acc: 0.7413 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2787  | total loss: \u001b[1m\u001b[32m0.55038\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2787 | loss: 0.55038 - acc: 0.7477 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2788  | total loss: \u001b[1m\u001b[32m0.58153\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2788 | loss: 0.58153 - acc: 0.7276 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2789  | total loss: \u001b[1m\u001b[32m0.56872\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2789 | loss: 0.56872 - acc: 0.7357 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2790  | total loss: \u001b[1m\u001b[32m0.59314\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2790 | loss: 0.59314 - acc: 0.7165 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2791  | total loss: \u001b[1m\u001b[32m0.58137\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2791 | loss: 0.58137 - acc: 0.7248 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2792  | total loss: \u001b[1m\u001b[32m0.57182\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2792 | loss: 0.57182 - acc: 0.7320 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2793  | total loss: \u001b[1m\u001b[32m0.56370\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2793 | loss: 0.56370 - acc: 0.7385 -- iter: 891/891\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 2794  | total loss: \u001b[1m\u001b[32m0.55634\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2794 | loss: 0.55634 - acc: 0.7445 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2795  | total loss: \u001b[1m\u001b[32m0.54924\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2795 | loss: 0.54924 - acc: 0.7501 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2796  | total loss: \u001b[1m\u001b[32m0.56943\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2796 | loss: 0.56943 - acc: 0.7314 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2797  | total loss: \u001b[1m\u001b[32m0.56018\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2797 | loss: 0.56018 - acc: 0.7392 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2798  | total loss: \u001b[1m\u001b[32m0.57829\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2798 | loss: 0.57829 - acc: 0.7216 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2799  | total loss: \u001b[1m\u001b[32m0.56799\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2799 | loss: 0.56799 - acc: 0.7305 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2800  | total loss: \u001b[1m\u001b[32m0.55866\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2800 | loss: 0.55866 - acc: 0.7386 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2801  | total loss: \u001b[1m\u001b[32m0.55007\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2801 | loss: 0.55007 - acc: 0.7460 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2802  | total loss: \u001b[1m\u001b[32m0.57279\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2802 | loss: 0.57279 - acc: 0.7285 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2803  | total loss: \u001b[1m\u001b[32m0.56265\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2803 | loss: 0.56265 - acc: 0.7369 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2804  | total loss: \u001b[1m\u001b[32m0.55351\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2804 | loss: 0.55351 - acc: 0.7447 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2805  | total loss: \u001b[1m\u001b[32m0.54515\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2805 | loss: 0.54515 - acc: 0.7516 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2806  | total loss: \u001b[1m\u001b[32m0.57289\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2806 | loss: 0.57289 - acc: 0.7280 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2807  | total loss: \u001b[1m\u001b[32m0.56255\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2807 | loss: 0.56255 - acc: 0.7368 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2808  | total loss: \u001b[1m\u001b[32m0.58370\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2808 | loss: 0.58370 - acc: 0.7186 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2809  | total loss: \u001b[1m\u001b[32m0.57273\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2809 | loss: 0.57273 - acc: 0.7282 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2810  | total loss: \u001b[1m\u001b[32m0.56313\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2810 | loss: 0.56313 - acc: 0.7369 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2811  | total loss: \u001b[1m\u001b[32m0.55456\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2811 | loss: 0.55456 - acc: 0.7449 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2812  | total loss: \u001b[1m\u001b[32m0.57412\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2812 | loss: 0.57412 - acc: 0.7268 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2813  | total loss: \u001b[1m\u001b[32m0.56459\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2813 | loss: 0.56459 - acc: 0.7351 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2814  | total loss: \u001b[1m\u001b[32m0.58371\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2814 | loss: 0.58371 - acc: 0.7164 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2815  | total loss: \u001b[1m\u001b[32m0.57379\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2815 | loss: 0.57379 - acc: 0.7258 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2816  | total loss: \u001b[1m\u001b[32m0.56508\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2816 | loss: 0.56508 - acc: 0.7344 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2817  | total loss: \u001b[1m\u001b[32m0.55716\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2817 | loss: 0.55716 - acc: 0.7423 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2818  | total loss: \u001b[1m\u001b[32m0.54973\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2818 | loss: 0.54973 - acc: 0.7490 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2819  | total loss: \u001b[1m\u001b[32m0.54255\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2819 | loss: 0.54255 - acc: 0.7553 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2820  | total loss: \u001b[1m\u001b[32m0.53545\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2820 | loss: 0.53545 - acc: 0.7611 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2821  | total loss: \u001b[1m\u001b[32m0.52841\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2821 | loss: 0.52841 - acc: 0.7664 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2822  | total loss: \u001b[1m\u001b[32m0.52149\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2822 | loss: 0.52149 - acc: 0.7713 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2823  | total loss: \u001b[1m\u001b[32m0.51483\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2823 | loss: 0.51483 - acc: 0.7756 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2824  | total loss: \u001b[1m\u001b[32m0.50854\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2824 | loss: 0.50854 - acc: 0.7797 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2825  | total loss: \u001b[1m\u001b[32m0.50272\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2825 | loss: 0.50272 - acc: 0.7831 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2826  | total loss: \u001b[1m\u001b[32m0.49744\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2826 | loss: 0.49744 - acc: 0.7853 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2827  | total loss: \u001b[1m\u001b[32m0.49272\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2827 | loss: 0.49272 - acc: 0.7867 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2828  | total loss: \u001b[1m\u001b[32m0.54112\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2828 | loss: 0.54112 - acc: 0.7623 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2829  | total loss: \u001b[1m\u001b[32m0.53184\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2829 | loss: 0.53184 - acc: 0.7657 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2830  | total loss: \u001b[1m\u001b[32m0.52324\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2830 | loss: 0.52324 - acc: 0.7694 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2831  | total loss: \u001b[1m\u001b[32m0.51529\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2831 | loss: 0.51529 - acc: 0.7733 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2832  | total loss: \u001b[1m\u001b[32m0.56484\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2832 | loss: 0.56484 - acc: 0.7482 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2833  | total loss: \u001b[1m\u001b[32m0.55283\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2833 | loss: 0.55283 - acc: 0.7547 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2834  | total loss: \u001b[1m\u001b[32m0.54241\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2834 | loss: 0.54241 - acc: 0.7596 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2835  | total loss: \u001b[1m\u001b[32m0.53347\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2835 | loss: 0.53347 - acc: 0.7641 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2836  | total loss: \u001b[1m\u001b[32m0.56793\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2836 | loss: 0.56793 - acc: 0.7413 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2837  | total loss: \u001b[1m\u001b[32m0.55738\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2837 | loss: 0.55738 - acc: 0.7477 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2838  | total loss: \u001b[1m\u001b[32m0.54821\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2838 | loss: 0.54821 - acc: 0.7528 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2839  | total loss: \u001b[1m\u001b[32m0.53992\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2839 | loss: 0.53992 - acc: 0.7577 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2840  | total loss: \u001b[1m\u001b[32m0.56636\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2840 | loss: 0.56636 - acc: 0.7376 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2841  | total loss: \u001b[1m\u001b[32m0.55624\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2841 | loss: 0.55624 - acc: 0.7446 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2842  | total loss: \u001b[1m\u001b[32m0.58134\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2842 | loss: 0.58134 - acc: 0.7226 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2843  | total loss: \u001b[1m\u001b[32m0.57044\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2843 | loss: 0.57044 - acc: 0.7313 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2844  | total loss: \u001b[1m\u001b[32m0.56101\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2844 | loss: 0.56101 - acc: 0.7392 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2845  | total loss: \u001b[1m\u001b[32m0.55260\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2845 | loss: 0.55260 - acc: 0.7462 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2846  | total loss: \u001b[1m\u001b[32m0.57278\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2846 | loss: 0.57278 - acc: 0.7269 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2847  | total loss: \u001b[1m\u001b[32m0.56346\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2847 | loss: 0.56346 - acc: 0.7352 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2848  | total loss: \u001b[1m\u001b[32m0.58438\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2848 | loss: 0.58438 - acc: 0.7161 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2849  | total loss: \u001b[1m\u001b[32m0.57453\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2849 | loss: 0.57453 - acc: 0.7254 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2850  | total loss: \u001b[1m\u001b[32m0.59195\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2850 | loss: 0.59195 - acc: 0.7083 -- iter: 891/891\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 2851  | total loss: \u001b[1m\u001b[32m0.58221\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2851 | loss: 0.58221 - acc: 0.7185 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2852  | total loss: \u001b[1m\u001b[32m0.59706\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2852 | loss: 0.59706 - acc: 0.7013 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2853  | total loss: \u001b[1m\u001b[32m0.58763\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2853 | loss: 0.58763 - acc: 0.7120 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2854  | total loss: \u001b[1m\u001b[32m0.60046\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2854 | loss: 0.60046 - acc: 0.6971 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2855  | total loss: \u001b[1m\u001b[32m0.59134\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2855 | loss: 0.59134 - acc: 0.7081 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2856  | total loss: \u001b[1m\u001b[32m0.58326\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2856 | loss: 0.58326 - acc: 0.7179 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2857  | total loss: \u001b[1m\u001b[32m0.57585\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2857 | loss: 0.57585 - acc: 0.7263 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2858  | total loss: \u001b[1m\u001b[32m0.56882\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2858 | loss: 0.56882 - acc: 0.7340 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2859  | total loss: \u001b[1m\u001b[32m0.56195\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2859 | loss: 0.56195 - acc: 0.7406 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2860  | total loss: \u001b[1m\u001b[32m0.58067\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2860 | loss: 0.58067 - acc: 0.7189 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2861  | total loss: \u001b[1m\u001b[32m0.57169\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2861 | loss: 0.57169 - acc: 0.7276 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2862  | total loss: \u001b[1m\u001b[32m0.58911\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2862 | loss: 0.58911 - acc: 0.7104 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2863  | total loss: \u001b[1m\u001b[32m0.57892\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2863 | loss: 0.57892 - acc: 0.7206 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2864  | total loss: \u001b[1m\u001b[32m0.59810\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2864 | loss: 0.59810 - acc: 0.7019 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2865  | total loss: \u001b[1m\u001b[32m0.58700\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2865 | loss: 0.58700 - acc: 0.7133 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2866  | total loss: \u001b[1m\u001b[32m0.60284\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2866 | loss: 0.60284 - acc: 0.6974 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2867  | total loss: \u001b[1m\u001b[32m0.59149\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2867 | loss: 0.59149 - acc: 0.7087 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2868  | total loss: \u001b[1m\u001b[32m0.58136\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2868 | loss: 0.58136 - acc: 0.7186 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2869  | total loss: \u001b[1m\u001b[32m0.57218\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2869 | loss: 0.57218 - acc: 0.7278 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2870  | total loss: \u001b[1m\u001b[32m0.56365\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2870 | loss: 0.56365 - acc: 0.7361 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2871  | total loss: \u001b[1m\u001b[32m0.55554\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2871 | loss: 0.55554 - acc: 0.7431 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2872  | total loss: \u001b[1m\u001b[32m0.57825\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2872 | loss: 0.57825 - acc: 0.7233 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2873  | total loss: \u001b[1m\u001b[32m0.56807\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2873 | loss: 0.56807 - acc: 0.7325 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2874  | total loss: \u001b[1m\u001b[32m0.55872\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2874 | loss: 0.55872 - acc: 0.7410 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2875  | total loss: \u001b[1m\u001b[32m0.54997\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2875 | loss: 0.54997 - acc: 0.7486 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2876  | total loss: \u001b[1m\u001b[32m0.54165\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2876 | loss: 0.54165 - acc: 0.7554 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2877  | total loss: \u001b[1m\u001b[32m0.53370\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2877 | loss: 0.53370 - acc: 0.7617 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2878  | total loss: \u001b[1m\u001b[32m0.56516\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2878 | loss: 0.56516 - acc: 0.7398 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2879  | total loss: \u001b[1m\u001b[32m0.55438\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 2879 | loss: 0.55438 - acc: 0.7476 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2880  | total loss: \u001b[1m\u001b[32m0.57836\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2880 | loss: 0.57836 - acc: 0.7300 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2881  | total loss: \u001b[1m\u001b[32m0.56642\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2881 | loss: 0.56642 - acc: 0.7386 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2882  | total loss: \u001b[1m\u001b[32m0.55590\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2882 | loss: 0.55590 - acc: 0.7457 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2883  | total loss: \u001b[1m\u001b[32m0.54650\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2883 | loss: 0.54650 - acc: 0.7525 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2884  | total loss: \u001b[1m\u001b[32m0.57225\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2884 | loss: 0.57225 - acc: 0.7326 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2885  | total loss: \u001b[1m\u001b[32m0.56162\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2885 | loss: 0.56162 - acc: 0.7406 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2886  | total loss: \u001b[1m\u001b[32m0.55228\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2886 | loss: 0.55228 - acc: 0.7478 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2887  | total loss: \u001b[1m\u001b[32m0.54382\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2887 | loss: 0.54382 - acc: 0.7545 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2888  | total loss: \u001b[1m\u001b[32m0.53592\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2888 | loss: 0.53592 - acc: 0.7603 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2889  | total loss: \u001b[1m\u001b[32m0.52842\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2889 | loss: 0.52842 - acc: 0.7650 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2890  | total loss: \u001b[1m\u001b[32m0.55619\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2890 | loss: 0.55619 - acc: 0.7432 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2891  | total loss: \u001b[1m\u001b[32m0.54621\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2891 | loss: 0.54621 - acc: 0.7498 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2892  | total loss: \u001b[1m\u001b[32m0.53715\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2892 | loss: 0.53715 - acc: 0.7560 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2893  | total loss: \u001b[1m\u001b[32m0.52886\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2893 | loss: 0.52886 - acc: 0.7614 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2894  | total loss: \u001b[1m\u001b[32m0.56000\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2894 | loss: 0.56000 - acc: 0.7410 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2895  | total loss: \u001b[1m\u001b[32m0.54942\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2895 | loss: 0.54942 - acc: 0.7482 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2896  | total loss: \u001b[1m\u001b[32m0.57689\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2896 | loss: 0.57689 - acc: 0.7280 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2897  | total loss: \u001b[1m\u001b[32m0.56527\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2897 | loss: 0.56527 - acc: 0.7359 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2898  | total loss: \u001b[1m\u001b[32m0.59028\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2898 | loss: 0.59028 - acc: 0.7150 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2899  | total loss: \u001b[1m\u001b[32m0.57860\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2899 | loss: 0.57860 - acc: 0.7242 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2900  | total loss: \u001b[1m\u001b[32m0.60204\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2900 | loss: 0.60204 - acc: 0.7022 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2901  | total loss: \u001b[1m\u001b[32m0.59098\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2901 | loss: 0.59098 - acc: 0.7129 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2902  | total loss: \u001b[1m\u001b[32m0.58181\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2902 | loss: 0.58181 - acc: 0.7220 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2903  | total loss: \u001b[1m\u001b[32m0.57391\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2903 | loss: 0.57391 - acc: 0.7301 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2904  | total loss: \u001b[1m\u001b[32m0.59039\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2904 | loss: 0.59039 - acc: 0.7112 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2905  | total loss: \u001b[1m\u001b[32m0.58184\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2905 | loss: 0.58184 - acc: 0.7207 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2906  | total loss: \u001b[1m\u001b[32m0.59835\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2906 | loss: 0.59835 - acc: 0.7019 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2907  | total loss: \u001b[1m\u001b[32m0.58920\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2907 | loss: 0.58920 - acc: 0.7123 -- iter: 891/891\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 2908  | total loss: \u001b[1m\u001b[32m0.58090\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2908 | loss: 0.58090 - acc: 0.7217 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2909  | total loss: \u001b[1m\u001b[32m0.57311\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2909 | loss: 0.57311 - acc: 0.7305 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2910  | total loss: \u001b[1m\u001b[32m0.56560\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2910 | loss: 0.56560 - acc: 0.7383 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2911  | total loss: \u001b[1m\u001b[32m0.55824\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2911 | loss: 0.55824 - acc: 0.7444 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2912  | total loss: \u001b[1m\u001b[32m0.55094\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2912 | loss: 0.55094 - acc: 0.7503 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2913  | total loss: \u001b[1m\u001b[32m0.54373\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2913 | loss: 0.54373 - acc: 0.7557 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2914  | total loss: \u001b[1m\u001b[32m0.56857\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2914 | loss: 0.56857 - acc: 0.7359 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2915  | total loss: \u001b[1m\u001b[32m0.55884\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2915 | loss: 0.55884 - acc: 0.7426 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2916  | total loss: \u001b[1m\u001b[32m0.54989\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2916 | loss: 0.54989 - acc: 0.7493 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2917  | total loss: \u001b[1m\u001b[32m0.54158\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2917 | loss: 0.54158 - acc: 0.7552 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2918  | total loss: \u001b[1m\u001b[32m0.57024\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2918 | loss: 0.57024 - acc: 0.7345 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2919  | total loss: \u001b[1m\u001b[32m0.55956\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2919 | loss: 0.55956 - acc: 0.7423 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2920  | total loss: \u001b[1m\u001b[32m0.54982\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2920 | loss: 0.54982 - acc: 0.7492 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2921  | total loss: \u001b[1m\u001b[32m0.54088\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2921 | loss: 0.54088 - acc: 0.7558 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2922  | total loss: \u001b[1m\u001b[32m0.57206\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2922 | loss: 0.57206 - acc: 0.7341 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2923  | total loss: \u001b[1m\u001b[32m0.56085\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2923 | loss: 0.56085 - acc: 0.7419 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2924  | total loss: \u001b[1m\u001b[32m0.55092\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2924 | loss: 0.55092 - acc: 0.7490 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2925  | total loss: \u001b[1m\u001b[32m0.54208\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2925 | loss: 0.54208 - acc: 0.7556 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2926  | total loss: \u001b[1m\u001b[32m0.56976\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2926 | loss: 0.56976 - acc: 0.7335 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2927  | total loss: \u001b[1m\u001b[32m0.55956\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2927 | loss: 0.55956 - acc: 0.7417 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2928  | total loss: \u001b[1m\u001b[32m0.55069\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2928 | loss: 0.55069 - acc: 0.7490 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2929  | total loss: \u001b[1m\u001b[32m0.54274\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2929 | loss: 0.54274 - acc: 0.7557 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2930  | total loss: \u001b[1m\u001b[32m0.53533\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2930 | loss: 0.53533 - acc: 0.7616 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2931  | total loss: \u001b[1m\u001b[32m0.52823\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2931 | loss: 0.52823 - acc: 0.7666 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2932  | total loss: \u001b[1m\u001b[32m0.52129\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2932 | loss: 0.52129 - acc: 0.7716 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2933  | total loss: \u001b[1m\u001b[32m0.51452\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2933 | loss: 0.51452 - acc: 0.7758 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2934  | total loss: \u001b[1m\u001b[32m0.55216\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2934 | loss: 0.55216 - acc: 0.7497 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2935  | total loss: \u001b[1m\u001b[32m0.54200\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2935 | loss: 0.54200 - acc: 0.7552 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2936  | total loss: \u001b[1m\u001b[32m0.57418\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2936 | loss: 0.57418 - acc: 0.7342 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2937  | total loss: \u001b[1m\u001b[32m0.56197\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2937 | loss: 0.56197 - acc: 0.7414 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2938  | total loss: \u001b[1m\u001b[32m0.55108\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2938 | loss: 0.55108 - acc: 0.7486 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2939  | total loss: \u001b[1m\u001b[32m0.54140\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2939 | loss: 0.54140 - acc: 0.7550 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2940  | total loss: \u001b[1m\u001b[32m0.57247\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2940 | loss: 0.57247 - acc: 0.7337 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2941  | total loss: \u001b[1m\u001b[32m0.56110\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2941 | loss: 0.56110 - acc: 0.7416 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2942  | total loss: \u001b[1m\u001b[32m0.55124\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2942 | loss: 0.55124 - acc: 0.7487 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2943  | total loss: \u001b[1m\u001b[32m0.54261\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2943 | loss: 0.54261 - acc: 0.7543 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2944  | total loss: \u001b[1m\u001b[32m0.56660\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2944 | loss: 0.56660 - acc: 0.7343 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2945  | total loss: \u001b[1m\u001b[32m0.55679\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2945 | loss: 0.55679 - acc: 0.7420 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2946  | total loss: \u001b[1m\u001b[32m0.54810\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2946 | loss: 0.54810 - acc: 0.7488 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2947  | total loss: \u001b[1m\u001b[32m0.54022\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2947 | loss: 0.54022 - acc: 0.7549 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2948  | total loss: \u001b[1m\u001b[32m0.56688\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2948 | loss: 0.56688 - acc: 0.7327 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2949  | total loss: \u001b[1m\u001b[32m0.55716\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2949 | loss: 0.55716 - acc: 0.7404 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2950  | total loss: \u001b[1m\u001b[32m0.57970\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2950 | loss: 0.57970 - acc: 0.7208 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2951  | total loss: \u001b[1m\u001b[32m0.56932\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2951 | loss: 0.56932 - acc: 0.7297 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2952  | total loss: \u001b[1m\u001b[32m0.56025\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2952 | loss: 0.56025 - acc: 0.7377 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2953  | total loss: \u001b[1m\u001b[32m0.55209\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2953 | loss: 0.55209 - acc: 0.7449 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2954  | total loss: \u001b[1m\u001b[32m0.57227\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2954 | loss: 0.57227 - acc: 0.7273 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2955  | total loss: \u001b[1m\u001b[32m0.56280\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2955 | loss: 0.56280 - acc: 0.7355 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2956  | total loss: \u001b[1m\u001b[32m0.58189\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2956 | loss: 0.58189 - acc: 0.7194 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2957  | total loss: \u001b[1m\u001b[32m0.57165\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2957 | loss: 0.57165 - acc: 0.7287 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2958  | total loss: \u001b[1m\u001b[32m0.59011\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2958 | loss: 0.59011 - acc: 0.7093 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2959  | total loss: \u001b[1m\u001b[32m0.57943\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2959 | loss: 0.57943 - acc: 0.7196 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2960  | total loss: \u001b[1m\u001b[32m0.59743\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2960 | loss: 0.59743 - acc: 0.7021 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2961  | total loss: \u001b[1m\u001b[32m0.58658\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2961 | loss: 0.58658 - acc: 0.7130 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2962  | total loss: \u001b[1m\u001b[32m0.60295\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2962 | loss: 0.60295 - acc: 0.6967 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2963  | total loss: \u001b[1m\u001b[32m0.59238\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2963 | loss: 0.59238 - acc: 0.7085 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2964  | total loss: \u001b[1m\u001b[32m0.58316\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2964 | loss: 0.58316 - acc: 0.7188 -- iter: 891/891\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 2965  | total loss: \u001b[1m\u001b[32m0.57483\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2965 | loss: 0.57483 - acc: 0.7284 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2966  | total loss: \u001b[1m\u001b[32m0.58971\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2966 | loss: 0.58971 - acc: 0.7121 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2967  | total loss: \u001b[1m\u001b[32m0.58050\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2967 | loss: 0.58050 - acc: 0.7220 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2968  | total loss: \u001b[1m\u001b[32m0.59670\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2968 | loss: 0.59670 - acc: 0.7035 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2969  | total loss: \u001b[1m\u001b[32m0.58676\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2969 | loss: 0.58676 - acc: 0.7142 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2970  | total loss: \u001b[1m\u001b[32m0.60293\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2970 | loss: 0.60293 - acc: 0.6967 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2971  | total loss: \u001b[1m\u001b[32m0.59251\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2971 | loss: 0.59251 - acc: 0.7085 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2972  | total loss: \u001b[1m\u001b[32m0.60427\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2972 | loss: 0.60427 - acc: 0.6946 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2973  | total loss: \u001b[1m\u001b[32m0.59392\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 2973 | loss: 0.59392 - acc: 0.7068 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2974  | total loss: \u001b[1m\u001b[32m0.60831\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 2974 | loss: 0.60831 - acc: 0.6901 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2975  | total loss: \u001b[1m\u001b[32m0.59778\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2975 | loss: 0.59778 - acc: 0.7026 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2976  | total loss: \u001b[1m\u001b[32m0.61068\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 2976 | loss: 0.61068 - acc: 0.6868 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2977  | total loss: \u001b[1m\u001b[32m0.60018\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2977 | loss: 0.60018 - acc: 0.6996 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2978  | total loss: \u001b[1m\u001b[32m0.61326\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2978 | loss: 0.61326 - acc: 0.6847 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2979  | total loss: \u001b[1m\u001b[32m0.60286\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2979 | loss: 0.60286 - acc: 0.6969 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2980  | total loss: \u001b[1m\u001b[32m0.61474\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2980 | loss: 0.61474 - acc: 0.6827 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2981  | total loss: \u001b[1m\u001b[32m0.60449\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2981 | loss: 0.60449 - acc: 0.6954 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2982  | total loss: \u001b[1m\u001b[32m0.59522\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2982 | loss: 0.59522 - acc: 0.7066 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2983  | total loss: \u001b[1m\u001b[32m0.58660\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2983 | loss: 0.58660 - acc: 0.7172 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2984  | total loss: \u001b[1m\u001b[32m0.60278\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2984 | loss: 0.60278 - acc: 0.6990 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2985  | total loss: \u001b[1m\u001b[32m0.59283\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2985 | loss: 0.59283 - acc: 0.7105 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2986  | total loss: \u001b[1m\u001b[32m0.60559\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2986 | loss: 0.60559 - acc: 0.6953 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2987  | total loss: \u001b[1m\u001b[32m0.59499\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2987 | loss: 0.59499 - acc: 0.7073 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2988  | total loss: \u001b[1m\u001b[32m0.58517\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2988 | loss: 0.58517 - acc: 0.7181 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2989  | total loss: \u001b[1m\u001b[32m0.57589\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 2989 | loss: 0.57589 - acc: 0.7270 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2990  | total loss: \u001b[1m\u001b[32m0.56701\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 2990 | loss: 0.56701 - acc: 0.7346 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2991  | total loss: \u001b[1m\u001b[32m0.55845\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2991 | loss: 0.55845 - acc: 0.7418 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2992  | total loss: \u001b[1m\u001b[32m0.58208\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2992 | loss: 0.58208 - acc: 0.7226 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2993  | total loss: \u001b[1m\u001b[32m0.57125\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2993 | loss: 0.57125 - acc: 0.7313 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2994  | total loss: \u001b[1m\u001b[32m0.59136\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2994 | loss: 0.59136 - acc: 0.7143 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2995  | total loss: \u001b[1m\u001b[32m0.57944\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2995 | loss: 0.57944 - acc: 0.7242 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2996  | total loss: \u001b[1m\u001b[32m0.56877\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2996 | loss: 0.56877 - acc: 0.7329 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2997  | total loss: \u001b[1m\u001b[32m0.55901\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2997 | loss: 0.55901 - acc: 0.7409 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2998  | total loss: \u001b[1m\u001b[32m0.58330\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2998 | loss: 0.58330 - acc: 0.7196 -- iter: 891/891\n",
      "--\n",
      "Training Step: 2999  | total loss: \u001b[1m\u001b[32m0.57192\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 2999 | loss: 0.57192 - acc: 0.7287 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3000  | total loss: \u001b[1m\u001b[32m0.59351\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3000 | loss: 0.59351 - acc: 0.7114 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3001  | total loss: \u001b[1m\u001b[32m0.58179\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3001 | loss: 0.58179 - acc: 0.7218 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3002  | total loss: \u001b[1m\u001b[32m0.60006\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3002 | loss: 0.60006 - acc: 0.7049 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3003  | total loss: \u001b[1m\u001b[32m0.58887\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3003 | loss: 0.58887 - acc: 0.7157 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3004  | total loss: \u001b[1m\u001b[32m0.57931\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3004 | loss: 0.57931 - acc: 0.7254 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3005  | total loss: \u001b[1m\u001b[32m0.57087\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3005 | loss: 0.57087 - acc: 0.7338 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3006  | total loss: \u001b[1m\u001b[32m0.59049\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3006 | loss: 0.59049 - acc: 0.7123 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3007  | total loss: \u001b[1m\u001b[32m0.58081\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3007 | loss: 0.58081 - acc: 0.7220 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3008  | total loss: \u001b[1m\u001b[32m0.60009\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3008 | loss: 0.60009 - acc: 0.7018 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3009  | total loss: \u001b[1m\u001b[32m0.58949\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3009 | loss: 0.58949 - acc: 0.7129 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3010  | total loss: \u001b[1m\u001b[32m0.60512\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3010 | loss: 0.60512 - acc: 0.6951 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3011  | total loss: \u001b[1m\u001b[32m0.59428\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3011 | loss: 0.59428 - acc: 0.7067 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3012  | total loss: \u001b[1m\u001b[32m0.60801\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3012 | loss: 0.60801 - acc: 0.6917 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3013  | total loss: \u001b[1m\u001b[32m0.59716\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3013 | loss: 0.59716 - acc: 0.7038 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3014  | total loss: \u001b[1m\u001b[32m0.58740\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3014 | loss: 0.58740 - acc: 0.7146 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3015  | total loss: \u001b[1m\u001b[32m0.57837\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3015 | loss: 0.57837 - acc: 0.7243 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3016  | total loss: \u001b[1m\u001b[32m0.56978\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3016 | loss: 0.56978 - acc: 0.7333 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3017  | total loss: \u001b[1m\u001b[32m0.56145\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3017 | loss: 0.56145 - acc: 0.7411 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3018  | total loss: \u001b[1m\u001b[32m0.58132\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 3018 | loss: 0.58132 - acc: 0.7226 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3019  | total loss: \u001b[1m\u001b[32m0.57099\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3019 | loss: 0.57099 - acc: 0.7318 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3020  | total loss: \u001b[1m\u001b[32m0.56141\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3020 | loss: 0.56141 - acc: 0.7403 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3021  | total loss: \u001b[1m\u001b[32m0.55245\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3021 | loss: 0.55245 - acc: 0.7478 -- iter: 891/891\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 3022  | total loss: \u001b[1m\u001b[32m0.54403\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3022 | loss: 0.54403 - acc: 0.7541 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3023  | total loss: \u001b[1m\u001b[32m0.53611\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3023 | loss: 0.53611 - acc: 0.7597 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3024  | total loss: \u001b[1m\u001b[32m0.52867\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3024 | loss: 0.52867 - acc: 0.7648 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3025  | total loss: \u001b[1m\u001b[32m0.52171\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3025 | loss: 0.52171 - acc: 0.7695 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3026  | total loss: \u001b[1m\u001b[32m0.51523\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3026 | loss: 0.51523 - acc: 0.7740 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3027  | total loss: \u001b[1m\u001b[32m0.50921\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3027 | loss: 0.50921 - acc: 0.7775 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3028  | total loss: \u001b[1m\u001b[32m0.55180\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3028 | loss: 0.55180 - acc: 0.7542 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3029  | total loss: \u001b[1m\u001b[32m0.54175\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3029 | loss: 0.54175 - acc: 0.7599 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3030  | total loss: \u001b[1m\u001b[32m0.53250\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3030 | loss: 0.53250 - acc: 0.7651 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3031  | total loss: \u001b[1m\u001b[32m0.52398\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3031 | loss: 0.52398 - acc: 0.7696 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3032  | total loss: \u001b[1m\u001b[32m0.51617\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3032 | loss: 0.51617 - acc: 0.7738 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3033  | total loss: \u001b[1m\u001b[32m0.50907\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3033 | loss: 0.50907 - acc: 0.7775 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3034  | total loss: \u001b[1m\u001b[32m0.50264\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3034 | loss: 0.50264 - acc: 0.7807 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3035  | total loss: \u001b[1m\u001b[32m0.49687\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3035 | loss: 0.49687 - acc: 0.7828 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3036  | total loss: \u001b[1m\u001b[32m0.49168\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3036 | loss: 0.49168 - acc: 0.7848 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3037  | total loss: \u001b[1m\u001b[32m0.48700\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3037 | loss: 0.48700 - acc: 0.7864 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3038  | total loss: \u001b[1m\u001b[32m0.48272\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3038 | loss: 0.48272 - acc: 0.7879 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3039  | total loss: \u001b[1m\u001b[32m0.47878\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3039 | loss: 0.47878 - acc: 0.7891 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3040  | total loss: \u001b[1m\u001b[32m0.52993\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3040 | loss: 0.52993 - acc: 0.7620 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3041  | total loss: \u001b[1m\u001b[32m0.52125\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3041 | loss: 0.52125 - acc: 0.7659 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3042  | total loss: \u001b[1m\u001b[32m0.55762\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3042 | loss: 0.55762 - acc: 0.7440 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3043  | total loss: \u001b[1m\u001b[32m0.54665\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3043 | loss: 0.54665 - acc: 0.7499 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3044  | total loss: \u001b[1m\u001b[32m0.53715\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3044 | loss: 0.53715 - acc: 0.7552 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3045  | total loss: \u001b[1m\u001b[32m0.52883\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3045 | loss: 0.52883 - acc: 0.7599 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3046  | total loss: \u001b[1m\u001b[32m0.52136\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3046 | loss: 0.52136 - acc: 0.7646 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3047  | total loss: \u001b[1m\u001b[32m0.51451\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3047 | loss: 0.51451 - acc: 0.7687 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3048  | total loss: \u001b[1m\u001b[32m0.54665\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3048 | loss: 0.54665 - acc: 0.7472 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3049  | total loss: \u001b[1m\u001b[32m0.53743\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3049 | loss: 0.53743 - acc: 0.7527 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3050  | total loss: \u001b[1m\u001b[32m0.52934\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3050 | loss: 0.52934 - acc: 0.7582 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3051  | total loss: \u001b[1m\u001b[32m0.52211\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3051 | loss: 0.52211 - acc: 0.7635 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3052  | total loss: \u001b[1m\u001b[32m0.51561\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3052 | loss: 0.51561 - acc: 0.7683 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3053  | total loss: \u001b[1m\u001b[32m0.50971\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3053 | loss: 0.50971 - acc: 0.7724 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3054  | total loss: \u001b[1m\u001b[32m0.50430\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 3054 | loss: 0.50430 - acc: 0.7764 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3055  | total loss: \u001b[1m\u001b[32m0.49929\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3055 | loss: 0.49929 - acc: 0.7799 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3056  | total loss: \u001b[1m\u001b[32m0.49466\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3056 | loss: 0.49466 - acc: 0.7828 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3057  | total loss: \u001b[1m\u001b[32m0.49037\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3057 | loss: 0.49037 - acc: 0.7852 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3058  | total loss: \u001b[1m\u001b[32m0.52780\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3058 | loss: 0.52780 - acc: 0.7608 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3059  | total loss: \u001b[1m\u001b[32m0.52006\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3059 | loss: 0.52006 - acc: 0.7655 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3060  | total loss: \u001b[1m\u001b[32m0.51303\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3060 | loss: 0.51303 - acc: 0.7697 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3061  | total loss: \u001b[1m\u001b[32m0.50663\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3061 | loss: 0.50663 - acc: 0.7735 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3062  | total loss: \u001b[1m\u001b[32m0.50081\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3062 | loss: 0.50081 - acc: 0.7770 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3063  | total loss: \u001b[1m\u001b[32m0.49547\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3063 | loss: 0.49547 - acc: 0.7804 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3064  | total loss: \u001b[1m\u001b[32m0.53418\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3064 | loss: 0.53418 - acc: 0.7566 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3065  | total loss: \u001b[1m\u001b[32m0.52566\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3065 | loss: 0.52566 - acc: 0.7615 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3066  | total loss: \u001b[1m\u001b[32m0.51826\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3066 | loss: 0.51826 - acc: 0.7657 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3067  | total loss: \u001b[1m\u001b[32m0.51174\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3067 | loss: 0.51174 - acc: 0.7695 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3068  | total loss: \u001b[1m\u001b[32m0.54155\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3068 | loss: 0.54155 - acc: 0.7490 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3069  | total loss: \u001b[1m\u001b[32m0.53336\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3069 | loss: 0.53336 - acc: 0.7551 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3070  | total loss: \u001b[1m\u001b[32m0.52630\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3070 | loss: 0.52630 - acc: 0.7601 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3071  | total loss: \u001b[1m\u001b[32m0.51998\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3071 | loss: 0.51998 - acc: 0.7644 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3072  | total loss: \u001b[1m\u001b[32m0.51408\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3072 | loss: 0.51408 - acc: 0.7683 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3073  | total loss: \u001b[1m\u001b[32m0.50836\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3073 | loss: 0.50836 - acc: 0.7717 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3074  | total loss: \u001b[1m\u001b[32m0.54167\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3074 | loss: 0.54167 - acc: 0.7486 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3075  | total loss: \u001b[1m\u001b[32m0.53283\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3075 | loss: 0.53283 - acc: 0.7548 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3076  | total loss: \u001b[1m\u001b[32m0.52485\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3076 | loss: 0.52485 - acc: 0.7602 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3077  | total loss: \u001b[1m\u001b[32m0.51758\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3077 | loss: 0.51758 - acc: 0.7656 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3078  | total loss: \u001b[1m\u001b[32m0.51092\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3078 | loss: 0.51092 - acc: 0.7704 -- iter: 891/891\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 3079  | total loss: \u001b[1m\u001b[32m0.50479\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3079 | loss: 0.50479 - acc: 0.7743 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3080  | total loss: \u001b[1m\u001b[32m0.54573\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3080 | loss: 0.54573 - acc: 0.7486 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3081  | total loss: \u001b[1m\u001b[32m0.53606\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3081 | loss: 0.53606 - acc: 0.7547 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3082  | total loss: \u001b[1m\u001b[32m0.52739\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3082 | loss: 0.52739 - acc: 0.7602 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3083  | total loss: \u001b[1m\u001b[32m0.51957\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3083 | loss: 0.51957 - acc: 0.7651 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3084  | total loss: \u001b[1m\u001b[32m0.51249\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3084 | loss: 0.51249 - acc: 0.7695 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3085  | total loss: \u001b[1m\u001b[32m0.50604\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3085 | loss: 0.50604 - acc: 0.7735 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3086  | total loss: \u001b[1m\u001b[32m0.54230\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3086 | loss: 0.54230 - acc: 0.7521 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3087  | total loss: \u001b[1m\u001b[32m0.53294\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3087 | loss: 0.53294 - acc: 0.7580 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3088  | total loss: \u001b[1m\u001b[32m0.56624\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3088 | loss: 0.56624 - acc: 0.7346 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3089  | total loss: \u001b[1m\u001b[32m0.55522\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3089 | loss: 0.55522 - acc: 0.7416 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3090  | total loss: \u001b[1m\u001b[32m0.57997\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3090 | loss: 0.57997 - acc: 0.7223 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3091  | total loss: \u001b[1m\u001b[32m0.56920\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3091 | loss: 0.56920 - acc: 0.7309 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3092  | total loss: \u001b[1m\u001b[32m0.58834\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3092 | loss: 0.58834 - acc: 0.7147 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3093  | total loss: \u001b[1m\u001b[32m0.57853\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3093 | loss: 0.57853 - acc: 0.7234 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3094  | total loss: \u001b[1m\u001b[32m0.59324\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3094 | loss: 0.59324 - acc: 0.7080 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3095  | total loss: \u001b[1m\u001b[32m0.58426\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3095 | loss: 0.58426 - acc: 0.7174 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3096  | total loss: \u001b[1m\u001b[32m0.60039\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3096 | loss: 0.60039 - acc: 0.6997 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3097  | total loss: \u001b[1m\u001b[32m0.59156\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3097 | loss: 0.59156 - acc: 0.7096 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3098  | total loss: \u001b[1m\u001b[32m0.60232\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3098 | loss: 0.60232 - acc: 0.6961 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3099  | total loss: \u001b[1m\u001b[32m0.59381\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3099 | loss: 0.59381 - acc: 0.7063 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3100  | total loss: \u001b[1m\u001b[32m0.60639\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3100 | loss: 0.60639 - acc: 0.6904 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3101  | total loss: \u001b[1m\u001b[32m0.59767\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3101 | loss: 0.59767 - acc: 0.7017 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3102  | total loss: \u001b[1m\u001b[32m0.58972\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3102 | loss: 0.58972 - acc: 0.7120 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3103  | total loss: \u001b[1m\u001b[32m0.58223\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3103 | loss: 0.58223 - acc: 0.7216 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3104  | total loss: \u001b[1m\u001b[32m0.59714\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 3104 | loss: 0.59714 - acc: 0.7033 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3105  | total loss: \u001b[1m\u001b[32m0.58821\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3105 | loss: 0.58821 - acc: 0.7135 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3106  | total loss: \u001b[1m\u001b[32m0.57982\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3106 | loss: 0.57982 - acc: 0.7224 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3107  | total loss: \u001b[1m\u001b[32m0.57180\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3107 | loss: 0.57180 - acc: 0.7302 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3108  | total loss: \u001b[1m\u001b[32m0.56404\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3108 | loss: 0.56404 - acc: 0.7372 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3109  | total loss: \u001b[1m\u001b[32m0.55651\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3109 | loss: 0.55651 - acc: 0.7417 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3110  | total loss: \u001b[1m\u001b[32m0.54920\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3110 | loss: 0.54920 - acc: 0.7455 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3111  | total loss: \u001b[1m\u001b[32m0.54212\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3111 | loss: 0.54212 - acc: 0.7493 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3112  | total loss: \u001b[1m\u001b[32m0.57398\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3112 | loss: 0.57398 - acc: 0.7285 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3113  | total loss: \u001b[1m\u001b[32m0.56386\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3113 | loss: 0.56386 - acc: 0.7360 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3114  | total loss: \u001b[1m\u001b[32m0.55461\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3114 | loss: 0.55461 - acc: 0.7431 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3115  | total loss: \u001b[1m\u001b[32m0.54605\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3115 | loss: 0.54605 - acc: 0.7490 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3116  | total loss: \u001b[1m\u001b[32m0.53806\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3116 | loss: 0.53806 - acc: 0.7547 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3117  | total loss: \u001b[1m\u001b[32m0.53059\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3117 | loss: 0.53059 - acc: 0.7606 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3118  | total loss: \u001b[1m\u001b[32m0.52359\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3118 | loss: 0.52359 - acc: 0.7657 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3119  | total loss: \u001b[1m\u001b[32m0.51700\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3119 | loss: 0.51700 - acc: 0.7701 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3120  | total loss: \u001b[1m\u001b[32m0.55610\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3120 | loss: 0.55610 - acc: 0.7462 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3121  | total loss: \u001b[1m\u001b[32m0.54576\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3121 | loss: 0.54576 - acc: 0.7527 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3122  | total loss: \u001b[1m\u001b[32m0.53637\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3122 | loss: 0.53637 - acc: 0.7583 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3123  | total loss: \u001b[1m\u001b[32m0.52781\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3123 | loss: 0.52781 - acc: 0.7635 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3124  | total loss: \u001b[1m\u001b[32m0.56331\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3124 | loss: 0.56331 - acc: 0.7403 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3125  | total loss: \u001b[1m\u001b[32m0.55257\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3125 | loss: 0.55257 - acc: 0.7477 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3126  | total loss: \u001b[1m\u001b[32m0.54348\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3126 | loss: 0.54348 - acc: 0.7544 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3127  | total loss: \u001b[1m\u001b[32m0.53563\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3127 | loss: 0.53563 - acc: 0.7599 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3128  | total loss: \u001b[1m\u001b[32m0.52860\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3128 | loss: 0.52860 - acc: 0.7645 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3129  | total loss: \u001b[1m\u001b[32m0.52199\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3129 | loss: 0.52199 - acc: 0.7686 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3130  | total loss: \u001b[1m\u001b[32m0.55261\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3130 | loss: 0.55261 - acc: 0.7466 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3131  | total loss: \u001b[1m\u001b[32m0.54316\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3131 | loss: 0.54316 - acc: 0.7525 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3132  | total loss: \u001b[1m\u001b[32m0.53453\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3132 | loss: 0.53453 - acc: 0.7585 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3133  | total loss: \u001b[1m\u001b[32m0.52655\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 3133 | loss: 0.52655 - acc: 0.7641 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3134  | total loss: \u001b[1m\u001b[32m0.51911\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 3134 | loss: 0.51911 - acc: 0.7687 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3135  | total loss: \u001b[1m\u001b[32m0.51219\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3135 | loss: 0.51219 - acc: 0.7731 -- iter: 891/891\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 3136  | total loss: \u001b[1m\u001b[32m0.50581\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3136 | loss: 0.50581 - acc: 0.7767 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3137  | total loss: \u001b[1m\u001b[32m0.49999\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3137 | loss: 0.49999 - acc: 0.7797 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3138  | total loss: \u001b[1m\u001b[32m0.49472\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3138 | loss: 0.49472 - acc: 0.7826 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3139  | total loss: \u001b[1m\u001b[32m0.48998\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3139 | loss: 0.48998 - acc: 0.7848 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3140  | total loss: \u001b[1m\u001b[32m0.48571\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3140 | loss: 0.48571 - acc: 0.7868 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3141  | total loss: \u001b[1m\u001b[32m0.48184\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3141 | loss: 0.48184 - acc: 0.7886 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3142  | total loss: \u001b[1m\u001b[32m0.52490\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3142 | loss: 0.52490 - acc: 0.7669 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3143  | total loss: \u001b[1m\u001b[32m0.51684\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3143 | loss: 0.51684 - acc: 0.7708 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3144  | total loss: \u001b[1m\u001b[32m0.55858\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3144 | loss: 0.55858 - acc: 0.7476 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3145  | total loss: \u001b[1m\u001b[32m0.54718\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3145 | loss: 0.54718 - acc: 0.7530 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3146  | total loss: \u001b[1m\u001b[32m0.57927\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3146 | loss: 0.57927 - acc: 0.7327 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3147  | total loss: \u001b[1m\u001b[32m0.56748\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3147 | loss: 0.56748 - acc: 0.7401 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3148  | total loss: \u001b[1m\u001b[32m0.55798\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3148 | loss: 0.55798 - acc: 0.7461 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3149  | total loss: \u001b[1m\u001b[32m0.55002\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3149 | loss: 0.55002 - acc: 0.7513 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3150  | total loss: \u001b[1m\u001b[32m0.57354\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3150 | loss: 0.57354 - acc: 0.7326 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3151  | total loss: \u001b[1m\u001b[32m0.56451\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3151 | loss: 0.56451 - acc: 0.7392 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3152  | total loss: \u001b[1m\u001b[32m0.55629\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3152 | loss: 0.55629 - acc: 0.7452 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3153  | total loss: \u001b[1m\u001b[32m0.54845\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3153 | loss: 0.54845 - acc: 0.7515 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3154  | total loss: \u001b[1m\u001b[32m0.57171\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3154 | loss: 0.57171 - acc: 0.7301 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3155  | total loss: \u001b[1m\u001b[32m0.56174\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3155 | loss: 0.56174 - acc: 0.7382 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3156  | total loss: \u001b[1m\u001b[32m0.57960\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3156 | loss: 0.57960 - acc: 0.7222 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3157  | total loss: \u001b[1m\u001b[32m0.56897\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3157 | loss: 0.56897 - acc: 0.7312 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3158  | total loss: \u001b[1m\u001b[32m0.58973\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3158 | loss: 0.58973 - acc: 0.7126 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3159  | total loss: \u001b[1m\u001b[32m0.57866\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3159 | loss: 0.57866 - acc: 0.7228 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3160  | total loss: \u001b[1m\u001b[32m0.56895\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3160 | loss: 0.56895 - acc: 0.7317 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3161  | total loss: \u001b[1m\u001b[32m0.56025\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3161 | loss: 0.56025 - acc: 0.7400 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3162  | total loss: \u001b[1m\u001b[32m0.55233\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3162 | loss: 0.55233 - acc: 0.7467 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3163  | total loss: \u001b[1m\u001b[32m0.54497\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3163 | loss: 0.54497 - acc: 0.7528 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3164  | total loss: \u001b[1m\u001b[32m0.56715\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3164 | loss: 0.56715 - acc: 0.7330 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3165  | total loss: \u001b[1m\u001b[32m0.55801\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3165 | loss: 0.55801 - acc: 0.7408 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3166  | total loss: \u001b[1m\u001b[32m0.54969\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3166 | loss: 0.54969 - acc: 0.7479 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3167  | total loss: \u001b[1m\u001b[32m0.54202\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3167 | loss: 0.54202 - acc: 0.7543 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3168  | total loss: \u001b[1m\u001b[32m0.53481\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3168 | loss: 0.53481 - acc: 0.7601 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3169  | total loss: \u001b[1m\u001b[32m0.52797\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3169 | loss: 0.52797 - acc: 0.7656 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3170  | total loss: \u001b[1m\u001b[32m0.52144\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3170 | loss: 0.52144 - acc: 0.7704 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3171  | total loss: \u001b[1m\u001b[32m0.51520\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3171 | loss: 0.51520 - acc: 0.7748 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3172  | total loss: \u001b[1m\u001b[32m0.54857\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3172 | loss: 0.54857 - acc: 0.7521 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3173  | total loss: \u001b[1m\u001b[32m0.53913\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3173 | loss: 0.53913 - acc: 0.7584 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3174  | total loss: \u001b[1m\u001b[32m0.56888\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3174 | loss: 0.56888 - acc: 0.7378 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3175  | total loss: \u001b[1m\u001b[32m0.55754\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3175 | loss: 0.55754 - acc: 0.7451 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3176  | total loss: \u001b[1m\u001b[32m0.58248\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3176 | loss: 0.58248 - acc: 0.7239 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3177  | total loss: \u001b[1m\u001b[32m0.57108\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3177 | loss: 0.57108 - acc: 0.7322 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3178  | total loss: \u001b[1m\u001b[32m0.56170\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3178 | loss: 0.56170 - acc: 0.7395 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3179  | total loss: \u001b[1m\u001b[32m0.55373\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3179 | loss: 0.55373 - acc: 0.7462 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3180  | total loss: \u001b[1m\u001b[32m0.54657\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3180 | loss: 0.54657 - acc: 0.7526 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3181  | total loss: \u001b[1m\u001b[32m0.53969\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3181 | loss: 0.53969 - acc: 0.7587 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3182  | total loss: \u001b[1m\u001b[32m0.53280\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3182 | loss: 0.53280 - acc: 0.7642 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3183  | total loss: \u001b[1m\u001b[32m0.52584\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3183 | loss: 0.52584 - acc: 0.7689 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3184  | total loss: \u001b[1m\u001b[32m0.51895\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3184 | loss: 0.51895 - acc: 0.7731 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3185  | total loss: \u001b[1m\u001b[32m0.51231\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3185 | loss: 0.51231 - acc: 0.7768 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3186  | total loss: \u001b[1m\u001b[32m0.54926\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3186 | loss: 0.54926 - acc: 0.7537 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3187  | total loss: \u001b[1m\u001b[32m0.53938\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3187 | loss: 0.53938 - acc: 0.7592 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3188  | total loss: \u001b[1m\u001b[32m0.53045\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3188 | loss: 0.53045 - acc: 0.7641 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3189  | total loss: \u001b[1m\u001b[32m0.52238\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3189 | loss: 0.52238 - acc: 0.7684 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3190  | total loss: \u001b[1m\u001b[32m0.51504\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3190 | loss: 0.51504 - acc: 0.7723 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3191  | total loss: \u001b[1m\u001b[32m0.50835\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3191 | loss: 0.50835 - acc: 0.7757 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3192  | total loss: \u001b[1m\u001b[32m0.50221\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3192 | loss: 0.50221 - acc: 0.7790 -- iter: 891/891\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 3193  | total loss: \u001b[1m\u001b[32m0.49658\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3193 | loss: 0.49658 - acc: 0.7820 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3194  | total loss: \u001b[1m\u001b[32m0.49139\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3194 | loss: 0.49139 - acc: 0.7847 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3195  | total loss: \u001b[1m\u001b[32m0.48661\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3195 | loss: 0.48661 - acc: 0.7871 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3196  | total loss: \u001b[1m\u001b[32m0.48223\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3196 | loss: 0.48223 - acc: 0.7888 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3197  | total loss: \u001b[1m\u001b[32m0.47822\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3197 | loss: 0.47822 - acc: 0.7904 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3198  | total loss: \u001b[1m\u001b[32m0.47459\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3198 | loss: 0.47459 - acc: 0.7916 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3199  | total loss: \u001b[1m\u001b[32m0.47132\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3199 | loss: 0.47132 - acc: 0.7926 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3200  | total loss: \u001b[1m\u001b[32m0.52399\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3200 | loss: 0.52399 - acc: 0.7647 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3201  | total loss: \u001b[1m\u001b[32m0.51602\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3201 | loss: 0.51602 - acc: 0.7684 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3202  | total loss: \u001b[1m\u001b[32m0.55486\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3202 | loss: 0.55486 - acc: 0.7472 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3203  | total loss: \u001b[1m\u001b[32m0.54491\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3203 | loss: 0.54491 - acc: 0.7526 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3204  | total loss: \u001b[1m\u001b[32m0.57612\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3204 | loss: 0.57612 - acc: 0.7320 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3205  | total loss: \u001b[1m\u001b[32m0.56595\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3205 | loss: 0.56595 - acc: 0.7384 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3206  | total loss: \u001b[1m\u001b[32m0.55735\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3206 | loss: 0.55735 - acc: 0.7441 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3207  | total loss: \u001b[1m\u001b[32m0.54947\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3207 | loss: 0.54947 - acc: 0.7495 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3208  | total loss: \u001b[1m\u001b[32m0.54185\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3208 | loss: 0.54185 - acc: 0.7550 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3209  | total loss: \u001b[1m\u001b[32m0.53439\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3209 | loss: 0.53439 - acc: 0.7605 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3210  | total loss: \u001b[1m\u001b[32m0.55950\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3210 | loss: 0.55950 - acc: 0.7406 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3211  | total loss: \u001b[1m\u001b[32m0.54995\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3211 | loss: 0.54995 - acc: 0.7478 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3212  | total loss: \u001b[1m\u001b[32m0.57611\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3212 | loss: 0.57611 - acc: 0.7259 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3213  | total loss: \u001b[1m\u001b[32m0.56546\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3213 | loss: 0.56546 - acc: 0.7342 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3214  | total loss: \u001b[1m\u001b[32m0.55621\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3214 | loss: 0.55621 - acc: 0.7416 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3215  | total loss: \u001b[1m\u001b[32m0.54802\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3215 | loss: 0.54802 - acc: 0.7488 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3216  | total loss: \u001b[1m\u001b[32m0.57334\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3216 | loss: 0.57334 - acc: 0.7262 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3217  | total loss: \u001b[1m\u001b[32m0.56385\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3217 | loss: 0.56385 - acc: 0.7342 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3218  | total loss: \u001b[1m\u001b[32m0.55554\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3218 | loss: 0.55554 - acc: 0.7415 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3219  | total loss: \u001b[1m\u001b[32m0.54810\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3219 | loss: 0.54810 - acc: 0.7483 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3220  | total loss: \u001b[1m\u001b[32m0.54128\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3220 | loss: 0.54128 - acc: 0.7547 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3221  | total loss: \u001b[1m\u001b[32m0.53487\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3221 | loss: 0.53487 - acc: 0.7607 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3222  | total loss: \u001b[1m\u001b[32m0.52871\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3222 | loss: 0.52871 - acc: 0.7659 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3223  | total loss: \u001b[1m\u001b[32m0.52271\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3223 | loss: 0.52271 - acc: 0.7704 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3224  | total loss: \u001b[1m\u001b[32m0.55035\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3224 | loss: 0.55035 - acc: 0.7477 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3225  | total loss: \u001b[1m\u001b[32m0.54163\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3225 | loss: 0.54163 - acc: 0.7538 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3226  | total loss: \u001b[1m\u001b[32m0.57152\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3226 | loss: 0.57152 - acc: 0.7311 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3227  | total loss: \u001b[1m\u001b[32m0.56083\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3227 | loss: 0.56083 - acc: 0.7389 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3228  | total loss: \u001b[1m\u001b[32m0.55134\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3228 | loss: 0.55134 - acc: 0.7464 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3229  | total loss: \u001b[1m\u001b[32m0.54282\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3229 | loss: 0.54282 - acc: 0.7533 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3230  | total loss: \u001b[1m\u001b[32m0.53508\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3230 | loss: 0.53508 - acc: 0.7592 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3231  | total loss: \u001b[1m\u001b[32m0.52787\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3231 | loss: 0.52787 - acc: 0.7646 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3232  | total loss: \u001b[1m\u001b[32m0.55763\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3232 | loss: 0.55763 - acc: 0.7422 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3233  | total loss: \u001b[1m\u001b[32m0.54803\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3233 | loss: 0.54803 - acc: 0.7486 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3234  | total loss: \u001b[1m\u001b[32m0.53944\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3234 | loss: 0.53944 - acc: 0.7544 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3235  | total loss: \u001b[1m\u001b[32m0.53148\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3235 | loss: 0.53148 - acc: 0.7603 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3236  | total loss: \u001b[1m\u001b[32m0.52407\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3236 | loss: 0.52407 - acc: 0.7657 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3237  | total loss: \u001b[1m\u001b[32m0.51711\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3237 | loss: 0.51711 - acc: 0.7706 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3238  | total loss: \u001b[1m\u001b[32m0.55379\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3238 | loss: 0.55379 - acc: 0.7462 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3239  | total loss: \u001b[1m\u001b[32m0.54365\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3239 | loss: 0.54365 - acc: 0.7524 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3240  | total loss: \u001b[1m\u001b[32m0.53465\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3240 | loss: 0.53465 - acc: 0.7578 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3241  | total loss: \u001b[1m\u001b[32m0.52636\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3241 | loss: 0.52636 - acc: 0.7629 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3242  | total loss: \u001b[1m\u001b[32m0.51884\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3242 | loss: 0.51884 - acc: 0.7679 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3243  | total loss: \u001b[1m\u001b[32m0.51199\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3243 | loss: 0.51199 - acc: 0.7722 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3244  | total loss: \u001b[1m\u001b[32m0.50557\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3244 | loss: 0.50557 - acc: 0.7766 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3245  | total loss: \u001b[1m\u001b[32m0.49965\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3245 | loss: 0.49965 - acc: 0.7793 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3246  | total loss: \u001b[1m\u001b[32m0.53405\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3246 | loss: 0.53405 - acc: 0.7578 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3247  | total loss: \u001b[1m\u001b[32m0.52529\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3247 | loss: 0.52529 - acc: 0.7622 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3248  | total loss: \u001b[1m\u001b[32m0.56734\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3248 | loss: 0.56734 - acc: 0.7375 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3249  | total loss: \u001b[1m\u001b[32m0.55548\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3249 | loss: 0.55548 - acc: 0.7449 -- iter: 891/891\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 3250  | total loss: \u001b[1m\u001b[32m0.54521\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3250 | loss: 0.54521 - acc: 0.7515 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3251  | total loss: \u001b[1m\u001b[32m0.53616\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3251 | loss: 0.53616 - acc: 0.7576 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3252  | total loss: \u001b[1m\u001b[32m0.52811\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3252 | loss: 0.52811 - acc: 0.7619 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3253  | total loss: \u001b[1m\u001b[32m0.52080\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3253 | loss: 0.52080 - acc: 0.7658 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3254  | total loss: \u001b[1m\u001b[32m0.55750\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3254 | loss: 0.55750 - acc: 0.7392 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3255  | total loss: \u001b[1m\u001b[32m0.54722\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3255 | loss: 0.54722 - acc: 0.7465 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3256  | total loss: \u001b[1m\u001b[32m0.57322\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3256 | loss: 0.57322 - acc: 0.7266 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3257  | total loss: \u001b[1m\u001b[32m0.56204\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3257 | loss: 0.56204 - acc: 0.7351 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3258  | total loss: \u001b[1m\u001b[32m0.58545\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 3258 | loss: 0.58545 - acc: 0.7158 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3259  | total loss: \u001b[1m\u001b[32m0.57437\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3259 | loss: 0.57437 - acc: 0.7247 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3260  | total loss: \u001b[1m\u001b[32m0.59610\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3260 | loss: 0.59610 - acc: 0.7041 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3261  | total loss: \u001b[1m\u001b[32m0.58541\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3261 | loss: 0.58541 - acc: 0.7144 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3262  | total loss: \u001b[1m\u001b[32m0.57627\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3262 | loss: 0.57627 - acc: 0.7239 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3263  | total loss: \u001b[1m\u001b[32m0.56812\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3263 | loss: 0.56812 - acc: 0.7322 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3264  | total loss: \u001b[1m\u001b[32m0.56053\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3264 | loss: 0.56053 - acc: 0.7398 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3265  | total loss: \u001b[1m\u001b[32m0.55324\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3265 | loss: 0.55324 - acc: 0.7469 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3266  | total loss: \u001b[1m\u001b[32m0.57641\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3266 | loss: 0.57641 - acc: 0.7243 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3267  | total loss: \u001b[1m\u001b[32m0.56696\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3267 | loss: 0.56696 - acc: 0.7329 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3268  | total loss: \u001b[1m\u001b[32m0.58780\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3268 | loss: 0.58780 - acc: 0.7126 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3269  | total loss: \u001b[1m\u001b[32m0.57727\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3269 | loss: 0.57727 - acc: 0.7229 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3270  | total loss: \u001b[1m\u001b[32m0.56784\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3270 | loss: 0.56784 - acc: 0.7317 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3271  | total loss: \u001b[1m\u001b[32m0.55922\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3271 | loss: 0.55922 - acc: 0.7391 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3272  | total loss: \u001b[1m\u001b[32m0.57945\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3272 | loss: 0.57945 - acc: 0.7222 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3273  | total loss: \u001b[1m\u001b[32m0.56951\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3273 | loss: 0.56951 - acc: 0.7308 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3274  | total loss: \u001b[1m\u001b[32m0.58858\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3274 | loss: 0.58858 - acc: 0.7135 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3275  | total loss: \u001b[1m\u001b[32m0.57803\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3275 | loss: 0.57803 - acc: 0.7234 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3276  | total loss: \u001b[1m\u001b[32m0.59706\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3276 | loss: 0.59706 - acc: 0.7049 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3277  | total loss: \u001b[1m\u001b[32m0.58617\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3277 | loss: 0.58617 - acc: 0.7158 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3278  | total loss: \u001b[1m\u001b[32m0.60099\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3278 | loss: 0.60099 - acc: 0.7005 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3279  | total loss: \u001b[1m\u001b[32m0.59051\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3279 | loss: 0.59051 - acc: 0.7114 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3280  | total loss: \u001b[1m\u001b[32m0.58135\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3280 | loss: 0.58135 - acc: 0.7216 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3281  | total loss: \u001b[1m\u001b[32m0.57310\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3281 | loss: 0.57310 - acc: 0.7308 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3282  | total loss: \u001b[1m\u001b[32m0.58944\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3282 | loss: 0.58944 - acc: 0.7130 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3283  | total loss: \u001b[1m\u001b[32m0.58020\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3283 | loss: 0.58020 - acc: 0.7230 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3284  | total loss: \u001b[1m\u001b[32m0.59625\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3284 | loss: 0.59625 - acc: 0.7045 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3285  | total loss: \u001b[1m\u001b[32m0.58632\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3285 | loss: 0.58632 - acc: 0.7156 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3286  | total loss: \u001b[1m\u001b[32m0.57727\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3286 | loss: 0.57727 - acc: 0.7254 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3287  | total loss: \u001b[1m\u001b[32m0.56876\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3287 | loss: 0.56876 - acc: 0.7341 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3288  | total loss: \u001b[1m\u001b[32m0.58631\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3288 | loss: 0.58631 - acc: 0.7154 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3289  | total loss: \u001b[1m\u001b[32m0.57621\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3289 | loss: 0.57621 - acc: 0.7254 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3290  | total loss: \u001b[1m\u001b[32m0.59508\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3290 | loss: 0.59508 - acc: 0.7062 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3291  | total loss: \u001b[1m\u001b[32m0.58389\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3291 | loss: 0.58389 - acc: 0.7169 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3292  | total loss: \u001b[1m\u001b[32m0.57372\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3292 | loss: 0.57372 - acc: 0.7265 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3293  | total loss: \u001b[1m\u001b[32m0.56429\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3293 | loss: 0.56429 - acc: 0.7357 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3294  | total loss: \u001b[1m\u001b[32m0.58721\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3294 | loss: 0.58721 - acc: 0.7153 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3295  | total loss: \u001b[1m\u001b[32m0.57607\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3295 | loss: 0.57607 - acc: 0.7255 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3296  | total loss: \u001b[1m\u001b[32m0.59452\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3296 | loss: 0.59452 - acc: 0.7090 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3297  | total loss: \u001b[1m\u001b[32m0.58278\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3297 | loss: 0.58278 - acc: 0.7200 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3298  | total loss: \u001b[1m\u001b[32m0.60182\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3298 | loss: 0.60182 - acc: 0.7025 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3299  | total loss: \u001b[1m\u001b[32m0.58987\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3299 | loss: 0.58987 - acc: 0.7136 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3300  | total loss: \u001b[1m\u001b[32m0.60428\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3300 | loss: 0.60428 - acc: 0.6997 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3301  | total loss: \u001b[1m\u001b[32m0.59289\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3301 | loss: 0.59289 - acc: 0.7112 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3302  | total loss: \u001b[1m\u001b[32m0.60577\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3302 | loss: 0.60577 - acc: 0.6963 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3303  | total loss: \u001b[1m\u001b[32m0.59505\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3303 | loss: 0.59505 - acc: 0.7081 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3304  | total loss: \u001b[1m\u001b[32m0.58560\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3304 | loss: 0.58560 - acc: 0.7186 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3305  | total loss: \u001b[1m\u001b[32m0.57702\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3305 | loss: 0.57702 - acc: 0.7280 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3306  | total loss: \u001b[1m\u001b[32m0.56895\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3306 | loss: 0.56895 - acc: 0.7363 -- iter: 891/891\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 3307  | total loss: \u001b[1m\u001b[32m0.56114\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3307 | loss: 0.56114 - acc: 0.7441 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3308  | total loss: \u001b[1m\u001b[32m0.58060\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3308 | loss: 0.58060 - acc: 0.7255 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3309  | total loss: \u001b[1m\u001b[32m0.57066\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3309 | loss: 0.57066 - acc: 0.7344 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3310  | total loss: \u001b[1m\u001b[32m0.56136\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3310 | loss: 0.56136 - acc: 0.7420 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3311  | total loss: \u001b[1m\u001b[32m0.55254\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3311 | loss: 0.55254 - acc: 0.7494 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3312  | total loss: \u001b[1m\u001b[32m0.58133\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3312 | loss: 0.58133 - acc: 0.7253 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3313  | total loss: \u001b[1m\u001b[32m0.57000\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3313 | loss: 0.57000 - acc: 0.7343 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3314  | total loss: \u001b[1m\u001b[32m0.59127\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3314 | loss: 0.59127 - acc: 0.7168 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3315  | total loss: \u001b[1m\u001b[32m0.57900\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3315 | loss: 0.57900 - acc: 0.7267 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3316  | total loss: \u001b[1m\u001b[32m0.60195\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3316 | loss: 0.60195 - acc: 0.7070 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3317  | total loss: \u001b[1m\u001b[32m0.58919\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3317 | loss: 0.58919 - acc: 0.7176 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3318  | total loss: \u001b[1m\u001b[32m0.60716\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3318 | loss: 0.60716 - acc: 0.7005 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3319  | total loss: \u001b[1m\u001b[32m0.59503\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3319 | loss: 0.59503 - acc: 0.7117 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3320  | total loss: \u001b[1m\u001b[32m0.58466\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3320 | loss: 0.58466 - acc: 0.7217 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3321  | total loss: \u001b[1m\u001b[32m0.57557\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3321 | loss: 0.57557 - acc: 0.7309 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3322  | total loss: \u001b[1m\u001b[32m0.56733\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3322 | loss: 0.56733 - acc: 0.7392 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3323  | total loss: \u001b[1m\u001b[32m0.55958\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3323 | loss: 0.55958 - acc: 0.7464 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3324  | total loss: \u001b[1m\u001b[32m0.55206\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3324 | loss: 0.55206 - acc: 0.7530 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3325  | total loss: \u001b[1m\u001b[32m0.54461\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3325 | loss: 0.54461 - acc: 0.7590 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3326  | total loss: \u001b[1m\u001b[32m0.53720\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3326 | loss: 0.53720 - acc: 0.7643 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3327  | total loss: \u001b[1m\u001b[32m0.52989\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3327 | loss: 0.52989 - acc: 0.7691 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3328  | total loss: \u001b[1m\u001b[32m0.52279\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 3328 | loss: 0.52279 - acc: 0.7735 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3329  | total loss: \u001b[1m\u001b[32m0.51604\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 3329 | loss: 0.51604 - acc: 0.7775 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3330  | total loss: \u001b[1m\u001b[32m0.55307\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 3330 | loss: 0.55307 - acc: 0.7551 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3331  | total loss: \u001b[1m\u001b[32m0.54302\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 3331 | loss: 0.54302 - acc: 0.7606 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3332  | total loss: \u001b[1m\u001b[32m0.57954\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 3332 | loss: 0.57954 - acc: 0.7404 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3333  | total loss: \u001b[1m\u001b[32m0.56669\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 3333 | loss: 0.56669 - acc: 0.7472 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3334  | total loss: \u001b[1m\u001b[32m0.60114\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 3334 | loss: 0.60114 - acc: 0.7242 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3335  | total loss: \u001b[1m\u001b[32m0.58622\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 3335 | loss: 0.58622 - acc: 0.7331 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3336  | total loss: \u001b[1m\u001b[32m0.60939\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3336 | loss: 0.60939 - acc: 0.7160 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3337  | total loss: \u001b[1m\u001b[32m0.59479\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 3337 | loss: 0.59479 - acc: 0.7255 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3338  | total loss: \u001b[1m\u001b[32m0.61238\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 3338 | loss: 0.61238 - acc: 0.7089 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3339  | total loss: \u001b[1m\u001b[32m0.59977\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3339 | loss: 0.59977 - acc: 0.7181 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3340  | total loss: \u001b[1m\u001b[32m0.58945\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3340 | loss: 0.58945 - acc: 0.7269 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3341  | total loss: \u001b[1m\u001b[32m0.58066\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3341 | loss: 0.58066 - acc: 0.7347 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3342  | total loss: \u001b[1m\u001b[32m0.57275\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 3342 | loss: 0.57275 - acc: 0.7416 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3343  | total loss: \u001b[1m\u001b[32m0.56521\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3343 | loss: 0.56521 - acc: 0.7484 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3344  | total loss: \u001b[1m\u001b[32m0.55771\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 3344 | loss: 0.55771 - acc: 0.7545 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3345  | total loss: \u001b[1m\u001b[32m0.55010\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3345 | loss: 0.55010 - acc: 0.7600 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3346  | total loss: \u001b[1m\u001b[32m0.54239\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 3346 | loss: 0.54239 - acc: 0.7654 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3347  | total loss: \u001b[1m\u001b[32m0.53474\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 3347 | loss: 0.53474 - acc: 0.7704 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3348  | total loss: \u001b[1m\u001b[32m0.56389\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 3348 | loss: 0.56389 - acc: 0.7474 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3349  | total loss: \u001b[1m\u001b[32m0.55353\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 3349 | loss: 0.55353 - acc: 0.7540 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3350  | total loss: \u001b[1m\u001b[32m0.58648\u001b[0m\u001b[0m | time: 0.011s\n",
      "| Adam | epoch: 3350 | loss: 0.58648 - acc: 0.7300 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3351  | total loss: \u001b[1m\u001b[32m0.57392\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 3351 | loss: 0.57392 - acc: 0.7384 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3352  | total loss: \u001b[1m\u001b[32m0.56266\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3352 | loss: 0.56266 - acc: 0.7460 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3353  | total loss: \u001b[1m\u001b[32m0.55251\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3353 | loss: 0.55251 - acc: 0.7530 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3354  | total loss: \u001b[1m\u001b[32m0.54329\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3354 | loss: 0.54329 - acc: 0.7592 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3355  | total loss: \u001b[1m\u001b[32m0.53487\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 3355 | loss: 0.53487 - acc: 0.7649 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3356  | total loss: \u001b[1m\u001b[32m0.56674\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 3356 | loss: 0.56674 - acc: 0.7416 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3357  | total loss: \u001b[1m\u001b[32m0.55595\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 3357 | loss: 0.55595 - acc: 0.7487 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3358  | total loss: \u001b[1m\u001b[32m0.54637\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3358 | loss: 0.54637 - acc: 0.7551 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3359  | total loss: \u001b[1m\u001b[32m0.53778\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 3359 | loss: 0.53778 - acc: 0.7608 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3360  | total loss: \u001b[1m\u001b[32m0.52999\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 3360 | loss: 0.52999 - acc: 0.7665 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3361  | total loss: \u001b[1m\u001b[32m0.52287\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 3361 | loss: 0.52287 - acc: 0.7712 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3362  | total loss: \u001b[1m\u001b[32m0.51628\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 3362 | loss: 0.51628 - acc: 0.7754 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3363  | total loss: \u001b[1m\u001b[32m0.51008\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 3363 | loss: 0.51008 - acc: 0.7793 -- iter: 891/891\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 3364  | total loss: \u001b[1m\u001b[32m0.54413\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 3364 | loss: 0.54413 - acc: 0.7560 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3365  | total loss: \u001b[1m\u001b[32m0.53501\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 3365 | loss: 0.53501 - acc: 0.7612 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3366  | total loss: \u001b[1m\u001b[32m0.52683\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3366 | loss: 0.52683 - acc: 0.7656 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3367  | total loss: \u001b[1m\u001b[32m0.51933\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3367 | loss: 0.51933 - acc: 0.7700 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3368  | total loss: \u001b[1m\u001b[32m0.55235\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3368 | loss: 0.55235 - acc: 0.7473 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3369  | total loss: \u001b[1m\u001b[32m0.54241\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3369 | loss: 0.54241 - acc: 0.7536 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3370  | total loss: \u001b[1m\u001b[32m0.57073\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3370 | loss: 0.57073 - acc: 0.7331 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3371  | total loss: \u001b[1m\u001b[32m0.55973\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3371 | loss: 0.55973 - acc: 0.7398 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3372  | total loss: \u001b[1m\u001b[32m0.58512\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3372 | loss: 0.58512 - acc: 0.7188 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3373  | total loss: \u001b[1m\u001b[32m0.57387\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3373 | loss: 0.57387 - acc: 0.7278 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3374  | total loss: \u001b[1m\u001b[32m0.56433\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3374 | loss: 0.56433 - acc: 0.7359 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3375  | total loss: \u001b[1m\u001b[32m0.55597\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3375 | loss: 0.55597 - acc: 0.7431 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3376  | total loss: \u001b[1m\u001b[32m0.57826\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3376 | loss: 0.57826 - acc: 0.7227 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3377  | total loss: \u001b[1m\u001b[32m0.56864\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3377 | loss: 0.56864 - acc: 0.7316 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3378  | total loss: \u001b[1m\u001b[32m0.58893\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3378 | loss: 0.58893 - acc: 0.7109 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3379  | total loss: \u001b[1m\u001b[32m0.57875\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3379 | loss: 0.57875 - acc: 0.7211 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3380  | total loss: \u001b[1m\u001b[32m0.59569\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3380 | loss: 0.59569 - acc: 0.7026 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3381  | total loss: \u001b[1m\u001b[32m0.58528\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3381 | loss: 0.58528 - acc: 0.7135 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3382  | total loss: \u001b[1m\u001b[32m0.60038\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3382 | loss: 0.60038 - acc: 0.6972 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3383  | total loss: \u001b[1m\u001b[32m0.59028\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3383 | loss: 0.59028 - acc: 0.7080 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3384  | total loss: \u001b[1m\u001b[32m0.60314\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3384 | loss: 0.60314 - acc: 0.6939 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3385  | total loss: \u001b[1m\u001b[32m0.59333\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3385 | loss: 0.59333 - acc: 0.7048 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3386  | total loss: \u001b[1m\u001b[32m0.58464\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3386 | loss: 0.58464 - acc: 0.7153 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3387  | total loss: \u001b[1m\u001b[32m0.57676\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 3387 | loss: 0.57676 - acc: 0.7249 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3388  | total loss: \u001b[1m\u001b[32m0.59221\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 3388 | loss: 0.59221 - acc: 0.7078 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3389  | total loss: \u001b[1m\u001b[32m0.58332\u001b[0m\u001b[0m | time: 0.011s\n",
      "| Adam | epoch: 3389 | loss: 0.58332 - acc: 0.7185 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3390  | total loss: \u001b[1m\u001b[32m0.59782\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 3390 | loss: 0.59782 - acc: 0.7021 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3391  | total loss: \u001b[1m\u001b[32m0.58821\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3391 | loss: 0.58821 - acc: 0.7124 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3392  | total loss: \u001b[1m\u001b[32m0.60256\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 3392 | loss: 0.60256 - acc: 0.6962 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3393  | total loss: \u001b[1m\u001b[32m0.59249\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3393 | loss: 0.59249 - acc: 0.7070 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3394  | total loss: \u001b[1m\u001b[32m0.60552\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3394 | loss: 0.60552 - acc: 0.6921 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3395  | total loss: \u001b[1m\u001b[32m0.59532\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3395 | loss: 0.59532 - acc: 0.7033 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3396  | total loss: \u001b[1m\u001b[32m0.60994\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3396 | loss: 0.60994 - acc: 0.6878 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3397  | total loss: \u001b[1m\u001b[32m0.59955\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3397 | loss: 0.59955 - acc: 0.6994 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3398  | total loss: \u001b[1m\u001b[32m0.61328\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3398 | loss: 0.61328 - acc: 0.6841 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3399  | total loss: \u001b[1m\u001b[32m0.60294\u001b[0m\u001b[0m | time: 0.011s\n",
      "| Adam | epoch: 3399 | loss: 0.60294 - acc: 0.6969 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3400  | total loss: \u001b[1m\u001b[32m0.61420\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3400 | loss: 0.61420 - acc: 0.6819 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3401  | total loss: \u001b[1m\u001b[32m0.60404\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 3401 | loss: 0.60404 - acc: 0.6950 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3402  | total loss: \u001b[1m\u001b[32m0.61638\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3402 | loss: 0.61638 - acc: 0.6822 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3403  | total loss: \u001b[1m\u001b[32m0.60612\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3403 | loss: 0.60612 - acc: 0.6952 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3404  | total loss: \u001b[1m\u001b[32m0.61600\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3404 | loss: 0.61600 - acc: 0.6839 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3405  | total loss: \u001b[1m\u001b[32m0.60585\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3405 | loss: 0.60585 - acc: 0.6967 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3406  | total loss: \u001b[1m\u001b[32m0.59659\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3406 | loss: 0.59659 - acc: 0.7082 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3407  | total loss: \u001b[1m\u001b[32m0.58791\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3407 | loss: 0.58791 - acc: 0.7183 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3408  | total loss: \u001b[1m\u001b[32m0.57958\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3408 | loss: 0.57958 - acc: 0.7276 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3409  | total loss: \u001b[1m\u001b[32m0.57143\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3409 | loss: 0.57143 - acc: 0.7360 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3410  | total loss: \u001b[1m\u001b[32m0.58816\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3410 | loss: 0.58816 - acc: 0.7170 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3411  | total loss: \u001b[1m\u001b[32m0.57810\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3411 | loss: 0.57810 - acc: 0.7269 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3412  | total loss: \u001b[1m\u001b[32m0.59520\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3412 | loss: 0.59520 - acc: 0.7106 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3413  | total loss: \u001b[1m\u001b[32m0.58394\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3413 | loss: 0.58394 - acc: 0.7210 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3414  | total loss: \u001b[1m\u001b[32m0.60263\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3414 | loss: 0.60263 - acc: 0.7045 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3415  | total loss: \u001b[1m\u001b[32m0.59049\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3415 | loss: 0.59049 - acc: 0.7156 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3416  | total loss: \u001b[1m\u001b[32m0.60889\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 3416 | loss: 0.60889 - acc: 0.6994 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3417  | total loss: \u001b[1m\u001b[32m0.59646\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3417 | loss: 0.59646 - acc: 0.7106 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3418  | total loss: \u001b[1m\u001b[32m0.61318\u001b[0m\u001b[0m | time: 0.011s\n",
      "| Adam | epoch: 3418 | loss: 0.61318 - acc: 0.6938 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3419  | total loss: \u001b[1m\u001b[32m0.60109\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 3419 | loss: 0.60109 - acc: 0.7062 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3420  | total loss: \u001b[1m\u001b[32m0.61699\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3420 | loss: 0.61699 - acc: 0.6884 -- iter: 891/891\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 3421  | total loss: \u001b[1m\u001b[32m0.60558\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 3421 | loss: 0.60558 - acc: 0.7008 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3422  | total loss: \u001b[1m\u001b[32m0.59571\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3422 | loss: 0.59571 - acc: 0.7120 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3423  | total loss: \u001b[1m\u001b[32m0.58691\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3423 | loss: 0.58691 - acc: 0.7221 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3424  | total loss: \u001b[1m\u001b[32m0.60165\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3424 | loss: 0.60165 - acc: 0.7023 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3425  | total loss: \u001b[1m\u001b[32m0.59206\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3425 | loss: 0.59206 - acc: 0.7134 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3426  | total loss: \u001b[1m\u001b[32m0.60676\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3426 | loss: 0.60676 - acc: 0.6963 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3427  | total loss: \u001b[1m\u001b[32m0.59645\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3427 | loss: 0.59645 - acc: 0.7080 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3428  | total loss: \u001b[1m\u001b[32m0.61138\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3428 | loss: 0.61138 - acc: 0.6908 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3429  | total loss: \u001b[1m\u001b[32m0.60051\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3429 | loss: 0.60051 - acc: 0.7033 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3430  | total loss: \u001b[1m\u001b[32m0.61403\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3430 | loss: 0.61403 - acc: 0.6882 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3431  | total loss: \u001b[1m\u001b[32m0.60284\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3431 | loss: 0.60284 - acc: 0.7012 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3432  | total loss: \u001b[1m\u001b[32m0.59264\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 3432 | loss: 0.59264 - acc: 0.7126 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3433  | total loss: \u001b[1m\u001b[32m0.58311\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3433 | loss: 0.58311 - acc: 0.7232 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3434  | total loss: \u001b[1m\u001b[32m0.59924\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 3434 | loss: 0.59924 - acc: 0.7051 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3435  | total loss: \u001b[1m\u001b[32m0.58845\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3435 | loss: 0.58845 - acc: 0.7157 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3436  | total loss: \u001b[1m\u001b[32m0.60634\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3436 | loss: 0.60634 - acc: 0.6972 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3437  | total loss: \u001b[1m\u001b[32m0.59464\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3437 | loss: 0.59464 - acc: 0.7085 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3438  | total loss: \u001b[1m\u001b[32m0.58402\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3438 | loss: 0.58402 - acc: 0.7187 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3439  | total loss: \u001b[1m\u001b[32m0.57422\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3439 | loss: 0.57422 - acc: 0.7280 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3440  | total loss: \u001b[1m\u001b[32m0.59446\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3440 | loss: 0.59446 - acc: 0.7092 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3441  | total loss: \u001b[1m\u001b[32m0.58331\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3441 | loss: 0.58331 - acc: 0.7196 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3442  | total loss: \u001b[1m\u001b[32m0.57315\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3442 | loss: 0.57315 - acc: 0.7290 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3443  | total loss: \u001b[1m\u001b[32m0.56378\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 3443 | loss: 0.56378 - acc: 0.7376 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3444  | total loss: \u001b[1m\u001b[32m0.55502\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3444 | loss: 0.55502 - acc: 0.7453 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3445  | total loss: \u001b[1m\u001b[32m0.54675\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3445 | loss: 0.54675 - acc: 0.7523 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3446  | total loss: \u001b[1m\u001b[32m0.53889\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3446 | loss: 0.53889 - acc: 0.7583 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3447  | total loss: \u001b[1m\u001b[32m0.53141\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3447 | loss: 0.53141 - acc: 0.7636 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3448  | total loss: \u001b[1m\u001b[32m0.52428\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3448 | loss: 0.52428 - acc: 0.7682 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3449  | total loss: \u001b[1m\u001b[32m0.51753\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3449 | loss: 0.51753 - acc: 0.7724 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3450  | total loss: \u001b[1m\u001b[32m0.55203\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 3450 | loss: 0.55203 - acc: 0.7510 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3451  | total loss: \u001b[1m\u001b[32m0.54211\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 3451 | loss: 0.54211 - acc: 0.7565 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3452  | total loss: \u001b[1m\u001b[32m0.57697\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 3452 | loss: 0.57697 - acc: 0.7335 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3453  | total loss: \u001b[1m\u001b[32m0.56442\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 3453 | loss: 0.56442 - acc: 0.7411 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3454  | total loss: \u001b[1m\u001b[32m0.59207\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 3454 | loss: 0.59207 - acc: 0.7225 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3455  | total loss: \u001b[1m\u001b[32m0.57871\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 3455 | loss: 0.57871 - acc: 0.7314 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3456  | total loss: \u001b[1m\u001b[32m0.56738\u001b[0m\u001b[0m | time: 0.014s\n",
      "| Adam | epoch: 3456 | loss: 0.56738 - acc: 0.7396 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3457  | total loss: \u001b[1m\u001b[32m0.55759\u001b[0m\u001b[0m | time: 0.024s\n",
      "| Adam | epoch: 3457 | loss: 0.55759 - acc: 0.7468 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3458  | total loss: \u001b[1m\u001b[32m0.58080\u001b[0m\u001b[0m | time: 0.016s\n",
      "| Adam | epoch: 3458 | loss: 0.58080 - acc: 0.7268 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3459  | total loss: \u001b[1m\u001b[32m0.57034\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 3459 | loss: 0.57034 - acc: 0.7345 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3460  | total loss: \u001b[1m\u001b[32m0.56103\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 3460 | loss: 0.56103 - acc: 0.7413 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3461  | total loss: \u001b[1m\u001b[32m0.55238\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 3461 | loss: 0.55238 - acc: 0.7480 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3462  | total loss: \u001b[1m\u001b[32m0.54411\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3462 | loss: 0.54411 - acc: 0.7544 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3463  | total loss: \u001b[1m\u001b[32m0.53612\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3463 | loss: 0.53612 - acc: 0.7599 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3464  | total loss: \u001b[1m\u001b[32m0.52840\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3464 | loss: 0.52840 - acc: 0.7654 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3465  | total loss: \u001b[1m\u001b[32m0.52103\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3465 | loss: 0.52103 - acc: 0.7700 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3466  | total loss: \u001b[1m\u001b[32m0.51417\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3466 | loss: 0.51417 - acc: 0.7740 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3467  | total loss: \u001b[1m\u001b[32m0.50790\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 3467 | loss: 0.50790 - acc: 0.7773 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3468  | total loss: \u001b[1m\u001b[32m0.50227\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 3468 | loss: 0.50227 - acc: 0.7805 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3469  | total loss: \u001b[1m\u001b[32m0.49726\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3469 | loss: 0.49726 - acc: 0.7835 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3470  | total loss: \u001b[1m\u001b[32m0.49281\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3470 | loss: 0.49281 - acc: 0.7858 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3471  | total loss: \u001b[1m\u001b[32m0.48885\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3471 | loss: 0.48885 - acc: 0.7880 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3472  | total loss: \u001b[1m\u001b[32m0.54353\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3472 | loss: 0.54353 - acc: 0.7625 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3473  | total loss: \u001b[1m\u001b[32m0.53416\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3473 | loss: 0.53416 - acc: 0.7664 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3474  | total loss: \u001b[1m\u001b[32m0.52549\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3474 | loss: 0.52549 - acc: 0.7696 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3475  | total loss: \u001b[1m\u001b[32m0.51744\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3475 | loss: 0.51744 - acc: 0.7731 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3476  | total loss: \u001b[1m\u001b[32m0.51009\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3476 | loss: 0.51009 - acc: 0.7767 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3477  | total loss: \u001b[1m\u001b[32m0.50349\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3477 | loss: 0.50349 - acc: 0.7803 -- iter: 891/891\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 3478  | total loss: \u001b[1m\u001b[32m0.49762\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3478 | loss: 0.49762 - acc: 0.7833 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3479  | total loss: \u001b[1m\u001b[32m0.49240\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3479 | loss: 0.49240 - acc: 0.7855 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3480  | total loss: \u001b[1m\u001b[32m0.48779\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3480 | loss: 0.48779 - acc: 0.7869 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3481  | total loss: \u001b[1m\u001b[32m0.48364\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3481 | loss: 0.48364 - acc: 0.7883 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3482  | total loss: \u001b[1m\u001b[32m0.47979\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 3482 | loss: 0.47979 - acc: 0.7896 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3483  | total loss: \u001b[1m\u001b[32m0.47618\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3483 | loss: 0.47618 - acc: 0.7909 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3484  | total loss: \u001b[1m\u001b[32m0.47280\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 3484 | loss: 0.47280 - acc: 0.7923 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3485  | total loss: \u001b[1m\u001b[32m0.46966\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3485 | loss: 0.46966 - acc: 0.7935 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3486  | total loss: \u001b[1m\u001b[32m0.46680\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3486 | loss: 0.46680 - acc: 0.7946 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3487  | total loss: \u001b[1m\u001b[32m0.46426\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3487 | loss: 0.46426 - acc: 0.7953 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3488  | total loss: \u001b[1m\u001b[32m0.46201\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3488 | loss: 0.46201 - acc: 0.7959 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3489  | total loss: \u001b[1m\u001b[32m0.46003\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3489 | loss: 0.46003 - acc: 0.7964 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3490  | total loss: \u001b[1m\u001b[32m0.51945\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3490 | loss: 0.51945 - acc: 0.7681 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3491  | total loss: \u001b[1m\u001b[32m0.51161\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3491 | loss: 0.51161 - acc: 0.7714 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3492  | total loss: \u001b[1m\u001b[32m0.55585\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3492 | loss: 0.55585 - acc: 0.7495 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3493  | total loss: \u001b[1m\u001b[32m0.54462\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3493 | loss: 0.54462 - acc: 0.7549 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3494  | total loss: \u001b[1m\u001b[32m0.57766\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3494 | loss: 0.57766 - acc: 0.7342 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3495  | total loss: \u001b[1m\u001b[32m0.56592\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3495 | loss: 0.56592 - acc: 0.7409 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3496  | total loss: \u001b[1m\u001b[32m0.55639\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3496 | loss: 0.55639 - acc: 0.7467 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3497  | total loss: \u001b[1m\u001b[32m0.54836\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3497 | loss: 0.54836 - acc: 0.7520 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3498  | total loss: \u001b[1m\u001b[32m0.57572\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3498 | loss: 0.57572 - acc: 0.7287 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3499  | total loss: \u001b[1m\u001b[32m0.56648\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3499 | loss: 0.56648 - acc: 0.7360 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3500  | total loss: \u001b[1m\u001b[32m0.58711\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3500 | loss: 0.58711 - acc: 0.7167 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3501  | total loss: \u001b[1m\u001b[32m0.57742\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3501 | loss: 0.57742 - acc: 0.7256 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3502  | total loss: \u001b[1m\u001b[32m0.56885\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3502 | loss: 0.56885 - acc: 0.7341 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3503  | total loss: \u001b[1m\u001b[32m0.56102\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3503 | loss: 0.56102 - acc: 0.7416 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3504  | total loss: \u001b[1m\u001b[32m0.55367\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3504 | loss: 0.55367 - acc: 0.7484 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3505  | total loss: \u001b[1m\u001b[32m0.54662\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3505 | loss: 0.54662 - acc: 0.7544 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3506  | total loss: \u001b[1m\u001b[32m0.57072\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3506 | loss: 0.57072 - acc: 0.7307 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3507  | total loss: \u001b[1m\u001b[32m0.56154\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3507 | loss: 0.56154 - acc: 0.7389 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3508  | total loss: \u001b[1m\u001b[32m0.58276\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3508 | loss: 0.58276 - acc: 0.7197 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3509  | total loss: \u001b[1m\u001b[32m0.57269\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3509 | loss: 0.57269 - acc: 0.7288 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3510  | total loss: \u001b[1m\u001b[32m0.59524\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3510 | loss: 0.59524 - acc: 0.7094 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3511  | total loss: \u001b[1m\u001b[32m0.58425\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3511 | loss: 0.58425 - acc: 0.7191 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3512  | total loss: \u001b[1m\u001b[32m0.60115\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3512 | loss: 0.60115 - acc: 0.7026 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3513  | total loss: \u001b[1m\u001b[32m0.59033\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3513 | loss: 0.59033 - acc: 0.7122 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3514  | total loss: \u001b[1m\u001b[32m0.60694\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3514 | loss: 0.60694 - acc: 0.6961 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3515  | total loss: \u001b[1m\u001b[32m0.59660\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3515 | loss: 0.59660 - acc: 0.7064 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3516  | total loss: \u001b[1m\u001b[32m0.61106\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3516 | loss: 0.61106 - acc: 0.6884 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3517  | total loss: \u001b[1m\u001b[32m0.60146\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3517 | loss: 0.60146 - acc: 0.7004 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3518  | total loss: \u001b[1m\u001b[32m0.61359\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3518 | loss: 0.61359 - acc: 0.6857 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3519  | total loss: \u001b[1m\u001b[32m0.60490\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3519 | loss: 0.60490 - acc: 0.6985 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3520  | total loss: \u001b[1m\u001b[32m0.61596\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3520 | loss: 0.61596 - acc: 0.6835 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3521  | total loss: \u001b[1m\u001b[32m0.60771\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3521 | loss: 0.60771 - acc: 0.6961 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3522  | total loss: \u001b[1m\u001b[32m0.60036\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3522 | loss: 0.60036 - acc: 0.7064 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3523  | total loss: \u001b[1m\u001b[32m0.59360\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3523 | loss: 0.59360 - acc: 0.7153 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3524  | total loss: \u001b[1m\u001b[32m0.58717\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3524 | loss: 0.58717 - acc: 0.7230 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3525  | total loss: \u001b[1m\u001b[32m0.58083\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3525 | loss: 0.58083 - acc: 0.7298 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3526  | total loss: \u001b[1m\u001b[32m0.57433\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3526 | loss: 0.57433 - acc: 0.7367 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3527  | total loss: \u001b[1m\u001b[32m0.56752\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3527 | loss: 0.56752 - acc: 0.7435 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3528  | total loss: \u001b[1m\u001b[32m0.58361\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3528 | loss: 0.58361 - acc: 0.7252 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3529  | total loss: \u001b[1m\u001b[32m0.57440\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3529 | loss: 0.57440 - acc: 0.7348 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3530  | total loss: \u001b[1m\u001b[32m0.59060\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3530 | loss: 0.59060 - acc: 0.7176 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3531  | total loss: \u001b[1m\u001b[32m0.57992\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3531 | loss: 0.57992 - acc: 0.7278 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3532  | total loss: \u001b[1m\u001b[32m0.56984\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3532 | loss: 0.56984 - acc: 0.7359 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3533  | total loss: \u001b[1m\u001b[32m0.56034\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3533 | loss: 0.56034 - acc: 0.7431 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3534  | total loss: \u001b[1m\u001b[32m0.55145\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3534 | loss: 0.55145 - acc: 0.7496 -- iter: 891/891\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 3535  | total loss: \u001b[1m\u001b[32m0.54306\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3535 | loss: 0.54306 - acc: 0.7555 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3536  | total loss: \u001b[1m\u001b[32m0.53498\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3536 | loss: 0.53498 - acc: 0.7612 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3537  | total loss: \u001b[1m\u001b[32m0.52730\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3537 | loss: 0.52730 - acc: 0.7666 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3538  | total loss: \u001b[1m\u001b[32m0.52022\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3538 | loss: 0.52022 - acc: 0.7705 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3539  | total loss: \u001b[1m\u001b[32m0.51361\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3539 | loss: 0.51361 - acc: 0.7740 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3540  | total loss: \u001b[1m\u001b[32m0.50740\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3540 | loss: 0.50740 - acc: 0.7778 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3541  | total loss: \u001b[1m\u001b[32m0.50178\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3541 | loss: 0.50178 - acc: 0.7817 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3542  | total loss: \u001b[1m\u001b[32m0.55239\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3542 | loss: 0.55239 - acc: 0.7592 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3543  | total loss: \u001b[1m\u001b[32m0.54168\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3543 | loss: 0.54168 - acc: 0.7642 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3544  | total loss: \u001b[1m\u001b[32m0.53198\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3544 | loss: 0.53198 - acc: 0.7680 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3545  | total loss: \u001b[1m\u001b[32m0.52324\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3545 | loss: 0.52324 - acc: 0.7719 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3546  | total loss: \u001b[1m\u001b[32m0.51567\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3546 | loss: 0.51567 - acc: 0.7760 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3547  | total loss: \u001b[1m\u001b[32m0.50910\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3547 | loss: 0.50910 - acc: 0.7795 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3548  | total loss: \u001b[1m\u001b[32m0.54336\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3548 | loss: 0.54336 - acc: 0.7590 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3549  | total loss: \u001b[1m\u001b[32m0.53464\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3549 | loss: 0.53464 - acc: 0.7636 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3550  | total loss: \u001b[1m\u001b[32m0.56363\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3550 | loss: 0.56363 - acc: 0.7428 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3551  | total loss: \u001b[1m\u001b[32m0.55406\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 3551 | loss: 0.55406 - acc: 0.7484 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3552  | total loss: \u001b[1m\u001b[32m0.57848\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3552 | loss: 0.57848 - acc: 0.7284 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3553  | total loss: \u001b[1m\u001b[32m0.56873\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3553 | loss: 0.56873 - acc: 0.7359 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3554  | total loss: \u001b[1m\u001b[32m0.58782\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3554 | loss: 0.58782 - acc: 0.7171 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3555  | total loss: \u001b[1m\u001b[32m0.57817\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3555 | loss: 0.57817 - acc: 0.7257 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3556  | total loss: \u001b[1m\u001b[32m0.56968\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3556 | loss: 0.56968 - acc: 0.7340 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3557  | total loss: \u001b[1m\u001b[32m0.56184\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3557 | loss: 0.56184 - acc: 0.7417 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3558  | total loss: \u001b[1m\u001b[32m0.57970\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3558 | loss: 0.57970 - acc: 0.7229 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3559  | total loss: \u001b[1m\u001b[32m0.57049\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3559 | loss: 0.57049 - acc: 0.7316 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3560  | total loss: \u001b[1m\u001b[32m0.56203\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3560 | loss: 0.56203 - acc: 0.7389 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3561  | total loss: \u001b[1m\u001b[32m0.55404\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3561 | loss: 0.55404 - acc: 0.7452 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3562  | total loss: \u001b[1m\u001b[32m0.54636\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3562 | loss: 0.54636 - acc: 0.7512 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3563  | total loss: \u001b[1m\u001b[32m0.53892\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 3563 | loss: 0.53892 - acc: 0.7566 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3564  | total loss: \u001b[1m\u001b[32m0.56599\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3564 | loss: 0.56599 - acc: 0.7367 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3565  | total loss: \u001b[1m\u001b[32m0.55620\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3565 | loss: 0.55620 - acc: 0.7444 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3566  | total loss: \u001b[1m\u001b[32m0.54725\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3566 | loss: 0.54725 - acc: 0.7512 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3567  | total loss: \u001b[1m\u001b[32m0.53885\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3567 | loss: 0.53885 - acc: 0.7574 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3568  | total loss: \u001b[1m\u001b[32m0.56863\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3568 | loss: 0.56863 - acc: 0.7371 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3569  | total loss: \u001b[1m\u001b[32m0.55797\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3569 | loss: 0.55797 - acc: 0.7449 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3570  | total loss: \u001b[1m\u001b[32m0.58309\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3570 | loss: 0.58309 - acc: 0.7239 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3571  | total loss: \u001b[1m\u001b[32m0.57215\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3571 | loss: 0.57215 - acc: 0.7330 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3572  | total loss: \u001b[1m\u001b[32m0.56293\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3572 | loss: 0.56293 - acc: 0.7412 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3573  | total loss: \u001b[1m\u001b[32m0.55503\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3573 | loss: 0.55503 - acc: 0.7483 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3574  | total loss: \u001b[1m\u001b[32m0.57543\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3574 | loss: 0.57543 - acc: 0.7280 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3575  | total loss: \u001b[1m\u001b[32m0.56693\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3575 | loss: 0.56693 - acc: 0.7364 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3576  | total loss: \u001b[1m\u001b[32m0.58294\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3576 | loss: 0.58294 - acc: 0.7193 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3577  | total loss: \u001b[1m\u001b[32m0.57433\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3577 | loss: 0.57433 - acc: 0.7282 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3578  | total loss: \u001b[1m\u001b[32m0.56666\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3578 | loss: 0.56666 - acc: 0.7363 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3579  | total loss: \u001b[1m\u001b[32m0.55939\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3579 | loss: 0.55939 - acc: 0.7438 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3580  | total loss: \u001b[1m\u001b[32m0.57902\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3580 | loss: 0.57902 - acc: 0.7244 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3581  | total loss: \u001b[1m\u001b[32m0.56982\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3581 | loss: 0.56982 - acc: 0.7336 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3582  | total loss: \u001b[1m\u001b[32m0.56123\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3582 | loss: 0.56123 - acc: 0.7415 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3583  | total loss: \u001b[1m\u001b[32m0.55302\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3583 | loss: 0.55302 - acc: 0.7485 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3584  | total loss: \u001b[1m\u001b[32m0.54506\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3584 | loss: 0.54506 - acc: 0.7550 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3585  | total loss: \u001b[1m\u001b[32m0.53731\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3585 | loss: 0.53731 - acc: 0.7606 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3586  | total loss: \u001b[1m\u001b[32m0.52985\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3586 | loss: 0.52985 - acc: 0.7657 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3587  | total loss: \u001b[1m\u001b[32m0.52275\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3587 | loss: 0.52275 - acc: 0.7703 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3588  | total loss: \u001b[1m\u001b[32m0.51612\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3588 | loss: 0.51612 - acc: 0.7745 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3589  | total loss: \u001b[1m\u001b[32m0.51002\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3589 | loss: 0.51002 - acc: 0.7781 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3590  | total loss: \u001b[1m\u001b[32m0.50442\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3590 | loss: 0.50442 - acc: 0.7811 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3591  | total loss: \u001b[1m\u001b[32m0.49924\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3591 | loss: 0.49924 - acc: 0.7831 -- iter: 891/891\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 3592  | total loss: \u001b[1m\u001b[32m0.49437\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3592 | loss: 0.49437 - acc: 0.7847 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3593  | total loss: \u001b[1m\u001b[32m0.48971\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3593 | loss: 0.48971 - acc: 0.7865 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3594  | total loss: \u001b[1m\u001b[32m0.48525\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3594 | loss: 0.48525 - acc: 0.7884 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3595  | total loss: \u001b[1m\u001b[32m0.48107\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3595 | loss: 0.48107 - acc: 0.7901 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3596  | total loss: \u001b[1m\u001b[32m0.47728\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3596 | loss: 0.47728 - acc: 0.7914 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3597  | total loss: \u001b[1m\u001b[32m0.47400\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3597 | loss: 0.47400 - acc: 0.7927 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3598  | total loss: \u001b[1m\u001b[32m0.52620\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3598 | loss: 0.52620 - acc: 0.7666 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3599  | total loss: \u001b[1m\u001b[32m0.51869\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3599 | loss: 0.51869 - acc: 0.7705 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3600  | total loss: \u001b[1m\u001b[32m0.55920\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3600 | loss: 0.55920 - acc: 0.7463 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3601  | total loss: \u001b[1m\u001b[32m0.54999\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3601 | loss: 0.54999 - acc: 0.7517 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3602  | total loss: \u001b[1m\u001b[32m0.57529\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3602 | loss: 0.57529 - acc: 0.7308 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3603  | total loss: \u001b[1m\u001b[32m0.56651\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3603 | loss: 0.56651 - acc: 0.7376 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3604  | total loss: \u001b[1m\u001b[32m0.58681\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3604 | loss: 0.58681 - acc: 0.7185 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3605  | total loss: \u001b[1m\u001b[32m0.57844\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3605 | loss: 0.57844 - acc: 0.7267 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3606  | total loss: \u001b[1m\u001b[32m0.59336\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3606 | loss: 0.59336 - acc: 0.7094 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3607  | total loss: \u001b[1m\u001b[32m0.58535\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3607 | loss: 0.58535 - acc: 0.7188 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3608  | total loss: \u001b[1m\u001b[32m0.59915\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3608 | loss: 0.59915 - acc: 0.7009 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3609  | total loss: \u001b[1m\u001b[32m0.59120\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3609 | loss: 0.59120 - acc: 0.7112 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3610  | total loss: \u001b[1m\u001b[32m0.60373\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3610 | loss: 0.60373 - acc: 0.6960 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3611  | total loss: \u001b[1m\u001b[32m0.59577\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3611 | loss: 0.59577 - acc: 0.7041 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3612  | total loss: \u001b[1m\u001b[32m0.58859\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3612 | loss: 0.58859 - acc: 0.7107 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3613  | total loss: \u001b[1m\u001b[32m0.58174\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3613 | loss: 0.58174 - acc: 0.7163 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3614  | total loss: \u001b[1m\u001b[32m0.57492\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3614 | loss: 0.57492 - acc: 0.7214 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3615  | total loss: \u001b[1m\u001b[32m0.56802\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3615 | loss: 0.56802 - acc: 0.7263 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3616  | total loss: \u001b[1m\u001b[32m0.56110\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3616 | loss: 0.56110 - acc: 0.7309 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3617  | total loss: \u001b[1m\u001b[32m0.55428\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3617 | loss: 0.55428 - acc: 0.7353 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3618  | total loss: \u001b[1m\u001b[32m0.58041\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3618 | loss: 0.58041 - acc: 0.7197 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3619  | total loss: \u001b[1m\u001b[32m0.57067\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3619 | loss: 0.57067 - acc: 0.7259 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3620  | total loss: \u001b[1m\u001b[32m0.59445\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3620 | loss: 0.59445 - acc: 0.7086 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3621  | total loss: \u001b[1m\u001b[32m0.58340\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3621 | loss: 0.58340 - acc: 0.7181 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3622  | total loss: \u001b[1m\u001b[32m0.60018\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3622 | loss: 0.60018 - acc: 0.7024 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3623  | total loss: \u001b[1m\u001b[32m0.59055\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3623 | loss: 0.59055 - acc: 0.7134 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3624  | total loss: \u001b[1m\u001b[32m0.60535\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3624 | loss: 0.60535 - acc: 0.6935 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3625  | total loss: \u001b[1m\u001b[32m0.59743\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3625 | loss: 0.59743 - acc: 0.7046 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3626  | total loss: \u001b[1m\u001b[32m0.59088\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3626 | loss: 0.59088 - acc: 0.7141 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3627  | total loss: \u001b[1m\u001b[32m0.58493\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3627 | loss: 0.58493 - acc: 0.7227 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3628  | total loss: \u001b[1m\u001b[32m0.57911\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3628 | loss: 0.57911 - acc: 0.7299 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3629  | total loss: \u001b[1m\u001b[32m0.57298\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3629 | loss: 0.57298 - acc: 0.7359 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3630  | total loss: \u001b[1m\u001b[32m0.58674\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3630 | loss: 0.58674 - acc: 0.7215 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3631  | total loss: \u001b[1m\u001b[32m0.57776\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3631 | loss: 0.57776 - acc: 0.7308 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3632  | total loss: \u001b[1m\u001b[32m0.56887\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3632 | loss: 0.56887 - acc: 0.7383 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3633  | total loss: \u001b[1m\u001b[32m0.55964\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3633 | loss: 0.55964 - acc: 0.7462 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3634  | total loss: \u001b[1m\u001b[32m0.58340\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3634 | loss: 0.58340 - acc: 0.7281 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3635  | total loss: \u001b[1m\u001b[32m0.57156\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3635 | loss: 0.57156 - acc: 0.7363 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3636  | total loss: \u001b[1m\u001b[32m0.59784\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3636 | loss: 0.59784 - acc: 0.7175 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3637  | total loss: \u001b[1m\u001b[32m0.58444\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3637 | loss: 0.58444 - acc: 0.7269 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3638  | total loss: \u001b[1m\u001b[32m0.57265\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3638 | loss: 0.57265 - acc: 0.7351 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3639  | total loss: \u001b[1m\u001b[32m0.56199\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3639 | loss: 0.56199 - acc: 0.7426 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3640  | total loss: \u001b[1m\u001b[32m0.55215\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3640 | loss: 0.55215 - acc: 0.7500 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3641  | total loss: \u001b[1m\u001b[32m0.54306\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3641 | loss: 0.54306 - acc: 0.7564 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3642  | total loss: \u001b[1m\u001b[32m0.53458\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3642 | loss: 0.53458 - acc: 0.7625 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3643  | total loss: \u001b[1m\u001b[32m0.52648\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3643 | loss: 0.52648 - acc: 0.7677 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3644  | total loss: \u001b[1m\u001b[32m0.51882\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3644 | loss: 0.51882 - acc: 0.7719 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3645  | total loss: \u001b[1m\u001b[32m0.51171\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3645 | loss: 0.51171 - acc: 0.7753 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3646  | total loss: \u001b[1m\u001b[32m0.55337\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3646 | loss: 0.55337 - acc: 0.7527 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3647  | total loss: \u001b[1m\u001b[32m0.54250\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3647 | loss: 0.54250 - acc: 0.7579 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3648  | total loss: \u001b[1m\u001b[32m0.53267\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3648 | loss: 0.53267 - acc: 0.7632 -- iter: 891/891\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 3649  | total loss: \u001b[1m\u001b[32m0.52383\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3649 | loss: 0.52383 - acc: 0.7678 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3650  | total loss: \u001b[1m\u001b[32m0.51583\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3650 | loss: 0.51583 - acc: 0.7720 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3651  | total loss: \u001b[1m\u001b[32m0.50860\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3651 | loss: 0.50860 - acc: 0.7751 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3652  | total loss: \u001b[1m\u001b[32m0.50207\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3652 | loss: 0.50207 - acc: 0.7777 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3653  | total loss: \u001b[1m\u001b[32m0.49612\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3653 | loss: 0.49612 - acc: 0.7800 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3654  | total loss: \u001b[1m\u001b[32m0.49072\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3654 | loss: 0.49072 - acc: 0.7823 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3655  | total loss: \u001b[1m\u001b[32m0.48583\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3655 | loss: 0.48583 - acc: 0.7844 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3656  | total loss: \u001b[1m\u001b[32m0.48140\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3656 | loss: 0.48140 - acc: 0.7861 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3657  | total loss: \u001b[1m\u001b[32m0.47743\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3657 | loss: 0.47743 - acc: 0.7875 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3658  | total loss: \u001b[1m\u001b[32m0.47387\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 3658 | loss: 0.47387 - acc: 0.7887 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3659  | total loss: \u001b[1m\u001b[32m0.47067\u001b[0m\u001b[0m | time: 0.013s\n",
      "| Adam | epoch: 3659 | loss: 0.47067 - acc: 0.7896 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3660  | total loss: \u001b[1m\u001b[32m0.46779\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 3660 | loss: 0.46779 - acc: 0.7908 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3661  | total loss: \u001b[1m\u001b[32m0.46517\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 3661 | loss: 0.46517 - acc: 0.7917 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3662  | total loss: \u001b[1m\u001b[32m0.46278\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3662 | loss: 0.46278 - acc: 0.7926 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3663  | total loss: \u001b[1m\u001b[32m0.46061\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3663 | loss: 0.46061 - acc: 0.7935 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3664  | total loss: \u001b[1m\u001b[32m0.51426\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3664 | loss: 0.51426 - acc: 0.7673 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3665  | total loss: \u001b[1m\u001b[32m0.50712\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3665 | loss: 0.50712 - acc: 0.7706 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3666  | total loss: \u001b[1m\u001b[32m0.55130\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3666 | loss: 0.55130 - acc: 0.7448 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3667  | total loss: \u001b[1m\u001b[32m0.54221\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3667 | loss: 0.54221 - acc: 0.7504 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3668  | total loss: \u001b[1m\u001b[32m0.53512\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 3668 | loss: 0.53512 - acc: 0.7550 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3669  | total loss: \u001b[1m\u001b[32m0.52918\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 3669 | loss: 0.52918 - acc: 0.7595 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3670  | total loss: \u001b[1m\u001b[32m0.55731\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3670 | loss: 0.55731 - acc: 0.7359 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3671  | total loss: \u001b[1m\u001b[32m0.54970\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 3671 | loss: 0.54970 - acc: 0.7427 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3672  | total loss: \u001b[1m\u001b[32m0.57018\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3672 | loss: 0.57018 - acc: 0.7233 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3673  | total loss: \u001b[1m\u001b[32m0.56212\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3673 | loss: 0.56212 - acc: 0.7318 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3674  | total loss: \u001b[1m\u001b[32m0.55516\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3674 | loss: 0.55516 - acc: 0.7394 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3675  | total loss: \u001b[1m\u001b[32m0.54876\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 3675 | loss: 0.54876 - acc: 0.7464 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3676  | total loss: \u001b[1m\u001b[32m0.54261\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 3676 | loss: 0.54261 - acc: 0.7528 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3677  | total loss: \u001b[1m\u001b[32m0.53662\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3677 | loss: 0.53662 - acc: 0.7586 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3678  | total loss: \u001b[1m\u001b[32m0.53074\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 3678 | loss: 0.53074 - acc: 0.7641 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3679  | total loss: \u001b[1m\u001b[32m0.52494\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 3679 | loss: 0.52494 - acc: 0.7685 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3680  | total loss: \u001b[1m\u001b[32m0.51927\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 3680 | loss: 0.51927 - acc: 0.7720 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3681  | total loss: \u001b[1m\u001b[32m0.51381\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3681 | loss: 0.51381 - acc: 0.7753 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3682  | total loss: \u001b[1m\u001b[32m0.54729\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 3682 | loss: 0.54729 - acc: 0.7530 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3683  | total loss: \u001b[1m\u001b[32m0.53856\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3683 | loss: 0.53856 - acc: 0.7590 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3684  | total loss: \u001b[1m\u001b[32m0.56690\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3684 | loss: 0.56690 - acc: 0.7376 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3685  | total loss: \u001b[1m\u001b[32m0.55636\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3685 | loss: 0.55636 - acc: 0.7441 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3686  | total loss: \u001b[1m\u001b[32m0.54687\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3686 | loss: 0.54687 - acc: 0.7503 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3687  | total loss: \u001b[1m\u001b[32m0.53855\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3687 | loss: 0.53855 - acc: 0.7564 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3688  | total loss: \u001b[1m\u001b[32m0.56657\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3688 | loss: 0.56657 - acc: 0.7350 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3689  | total loss: \u001b[1m\u001b[32m0.55740\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3689 | loss: 0.55740 - acc: 0.7428 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3690  | total loss: \u001b[1m\u001b[32m0.58046\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3690 | loss: 0.58046 - acc: 0.7216 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3691  | total loss: \u001b[1m\u001b[32m0.57272\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3691 | loss: 0.57272 - acc: 0.7290 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3692  | total loss: \u001b[1m\u001b[32m0.56483\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3692 | loss: 0.56483 - acc: 0.7368 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3693  | total loss: \u001b[1m\u001b[32m0.55697\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3693 | loss: 0.55697 - acc: 0.7436 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3694  | total loss: \u001b[1m\u001b[32m0.57933\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3694 | loss: 0.57933 - acc: 0.7257 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3695  | total loss: \u001b[1m\u001b[32m0.56864\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3695 | loss: 0.56864 - acc: 0.7344 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3696  | total loss: \u001b[1m\u001b[32m0.55899\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3696 | loss: 0.55899 - acc: 0.7423 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3697  | total loss: \u001b[1m\u001b[32m0.55030\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 3697 | loss: 0.55030 - acc: 0.7491 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3698  | total loss: \u001b[1m\u001b[32m0.57529\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 3698 | loss: 0.57529 - acc: 0.7275 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3699  | total loss: \u001b[1m\u001b[32m0.56473\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 3699 | loss: 0.56473 - acc: 0.7364 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3700  | total loss: \u001b[1m\u001b[32m0.58980\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 3700 | loss: 0.58980 - acc: 0.7159 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3701  | total loss: \u001b[1m\u001b[32m0.57813\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 3701 | loss: 0.57813 - acc: 0.7256 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3702  | total loss: \u001b[1m\u001b[32m0.56793\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 3702 | loss: 0.56793 - acc: 0.7340 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3703  | total loss: \u001b[1m\u001b[32m0.55889\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 3703 | loss: 0.55889 - acc: 0.7416 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3704  | total loss: \u001b[1m\u001b[32m0.55070\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 3704 | loss: 0.55070 - acc: 0.7490 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3705  | total loss: \u001b[1m\u001b[32m0.54312\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 3705 | loss: 0.54312 - acc: 0.7557 -- iter: 891/891\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 3706  | total loss: \u001b[1m\u001b[32m0.53596\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 3706 | loss: 0.53596 - acc: 0.7615 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3707  | total loss: \u001b[1m\u001b[32m0.52909\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 3707 | loss: 0.52909 - acc: 0.7671 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3708  | total loss: \u001b[1m\u001b[32m0.55921\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 3708 | loss: 0.55921 - acc: 0.7432 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3709  | total loss: \u001b[1m\u001b[32m0.54952\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 3709 | loss: 0.54952 - acc: 0.7508 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3710  | total loss: \u001b[1m\u001b[32m0.54066\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 3710 | loss: 0.54066 - acc: 0.7575 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3711  | total loss: \u001b[1m\u001b[32m0.53249\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 3711 | loss: 0.53249 - acc: 0.7630 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3712  | total loss: \u001b[1m\u001b[32m0.52486\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 3712 | loss: 0.52486 - acc: 0.7678 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3713  | total loss: \u001b[1m\u001b[32m0.51769\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 3713 | loss: 0.51769 - acc: 0.7724 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3714  | total loss: \u001b[1m\u001b[32m0.55348\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3714 | loss: 0.55348 - acc: 0.7486 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3715  | total loss: \u001b[1m\u001b[32m0.54321\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3715 | loss: 0.54321 - acc: 0.7546 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3716  | total loss: \u001b[1m\u001b[32m0.57452\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3716 | loss: 0.57452 - acc: 0.7337 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3717  | total loss: \u001b[1m\u001b[32m0.56265\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3717 | loss: 0.56265 - acc: 0.7412 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3718  | total loss: \u001b[1m\u001b[32m0.55235\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3718 | loss: 0.55235 - acc: 0.7482 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3719  | total loss: \u001b[1m\u001b[32m0.54329\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3719 | loss: 0.54329 - acc: 0.7542 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3720  | total loss: \u001b[1m\u001b[32m0.53509\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3720 | loss: 0.53509 - acc: 0.7596 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3721  | total loss: \u001b[1m\u001b[32m0.52742\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3721 | loss: 0.52742 - acc: 0.7644 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3722  | total loss: \u001b[1m\u001b[32m0.55458\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 3722 | loss: 0.55458 - acc: 0.7430 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3723  | total loss: \u001b[1m\u001b[32m0.54483\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3723 | loss: 0.54483 - acc: 0.7489 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3724  | total loss: \u001b[1m\u001b[32m0.53608\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3724 | loss: 0.53608 - acc: 0.7543 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3725  | total loss: \u001b[1m\u001b[32m0.52806\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3725 | loss: 0.52806 - acc: 0.7595 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3726  | total loss: \u001b[1m\u001b[32m0.55795\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3726 | loss: 0.55795 - acc: 0.7394 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3727  | total loss: \u001b[1m\u001b[32m0.54786\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3727 | loss: 0.54786 - acc: 0.7462 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3728  | total loss: \u001b[1m\u001b[32m0.53894\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3728 | loss: 0.53894 - acc: 0.7524 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3729  | total loss: \u001b[1m\u001b[32m0.53093\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3729 | loss: 0.53093 - acc: 0.7582 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3730  | total loss: \u001b[1m\u001b[32m0.52363\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3730 | loss: 0.52363 - acc: 0.7633 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3731  | total loss: \u001b[1m\u001b[32m0.51687\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3731 | loss: 0.51687 - acc: 0.7681 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3732  | total loss: \u001b[1m\u001b[32m0.51054\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3732 | loss: 0.51054 - acc: 0.7725 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3733  | total loss: \u001b[1m\u001b[32m0.50460\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3733 | loss: 0.50460 - acc: 0.7762 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3734  | total loss: \u001b[1m\u001b[32m0.49903\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3734 | loss: 0.49903 - acc: 0.7795 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3735  | total loss: \u001b[1m\u001b[32m0.49385\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3735 | loss: 0.49385 - acc: 0.7826 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3736  | total loss: \u001b[1m\u001b[32m0.48906\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3736 | loss: 0.48906 - acc: 0.7852 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3737  | total loss: \u001b[1m\u001b[32m0.48466\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3737 | loss: 0.48466 - acc: 0.7873 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3738  | total loss: \u001b[1m\u001b[32m0.48066\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3738 | loss: 0.48066 - acc: 0.7892 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3739  | total loss: \u001b[1m\u001b[32m0.47700\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3739 | loss: 0.47700 - acc: 0.7906 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3740  | total loss: \u001b[1m\u001b[32m0.53161\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3740 | loss: 0.53161 - acc: 0.7638 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3741  | total loss: \u001b[1m\u001b[32m0.52265\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3741 | loss: 0.52265 - acc: 0.7674 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3742  | total loss: \u001b[1m\u001b[32m0.51455\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3742 | loss: 0.51455 - acc: 0.7709 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3743  | total loss: \u001b[1m\u001b[32m0.50731\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3743 | loss: 0.50731 - acc: 0.7740 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3744  | total loss: \u001b[1m\u001b[32m0.55634\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3744 | loss: 0.55634 - acc: 0.7468 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3745  | total loss: \u001b[1m\u001b[32m0.54567\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3745 | loss: 0.54567 - acc: 0.7525 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3746  | total loss: \u001b[1m\u001b[32m0.53669\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3746 | loss: 0.53669 - acc: 0.7575 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3747  | total loss: \u001b[1m\u001b[32m0.52889\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3747 | loss: 0.52889 - acc: 0.7619 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3748  | total loss: \u001b[1m\u001b[32m0.56163\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3748 | loss: 0.56163 - acc: 0.7390 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3749  | total loss: \u001b[1m\u001b[32m0.55189\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3749 | loss: 0.55189 - acc: 0.7452 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3750  | total loss: \u001b[1m\u001b[32m0.54337\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3750 | loss: 0.54337 - acc: 0.7506 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3751  | total loss: \u001b[1m\u001b[32m0.53564\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3751 | loss: 0.53564 - acc: 0.7559 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3752  | total loss: \u001b[1m\u001b[32m0.56331\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3752 | loss: 0.56331 - acc: 0.7341 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3753  | total loss: \u001b[1m\u001b[32m0.55364\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3753 | loss: 0.55364 - acc: 0.7421 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3754  | total loss: \u001b[1m\u001b[32m0.54505\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3754 | loss: 0.54505 - acc: 0.7491 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3755  | total loss: \u001b[1m\u001b[32m0.53729\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3755 | loss: 0.53729 - acc: 0.7553 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3756  | total loss: \u001b[1m\u001b[32m0.53016\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3756 | loss: 0.53016 - acc: 0.7612 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3757  | total loss: \u001b[1m\u001b[32m0.52357\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3757 | loss: 0.52357 - acc: 0.7661 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3758  | total loss: \u001b[1m\u001b[32m0.51742\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3758 | loss: 0.51742 - acc: 0.7705 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3759  | total loss: \u001b[1m\u001b[32m0.51169\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3759 | loss: 0.51169 - acc: 0.7748 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3760  | total loss: \u001b[1m\u001b[32m0.50634\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3760 | loss: 0.50634 - acc: 0.7788 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3761  | total loss: \u001b[1m\u001b[32m0.50138\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3761 | loss: 0.50138 - acc: 0.7821 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3762  | total loss: \u001b[1m\u001b[32m0.53834\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3762 | loss: 0.53834 - acc: 0.7590 -- iter: 891/891\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 3763  | total loss: \u001b[1m\u001b[32m0.52998\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3763 | loss: 0.52998 - acc: 0.7639 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3764  | total loss: \u001b[1m\u001b[32m0.56535\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3764 | loss: 0.56535 - acc: 0.7400 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3765  | total loss: \u001b[1m\u001b[32m0.55420\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3765 | loss: 0.55420 - acc: 0.7465 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3766  | total loss: \u001b[1m\u001b[32m0.54419\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3766 | loss: 0.54419 - acc: 0.7527 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3767  | total loss: \u001b[1m\u001b[32m0.53524\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3767 | loss: 0.53524 - acc: 0.7587 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3768  | total loss: \u001b[1m\u001b[32m0.52727\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3768 | loss: 0.52727 - acc: 0.7645 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3769  | total loss: \u001b[1m\u001b[32m0.52009\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3769 | loss: 0.52009 - acc: 0.7693 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3770  | total loss: \u001b[1m\u001b[32m0.51352\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3770 | loss: 0.51352 - acc: 0.7734 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3771  | total loss: \u001b[1m\u001b[32m0.50747\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3771 | loss: 0.50747 - acc: 0.7765 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3772  | total loss: \u001b[1m\u001b[32m0.50183\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3772 | loss: 0.50183 - acc: 0.7799 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3773  | total loss: \u001b[1m\u001b[32m0.49651\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3773 | loss: 0.49651 - acc: 0.7830 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3774  | total loss: \u001b[1m\u001b[32m0.49147\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3774 | loss: 0.49147 - acc: 0.7857 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3775  | total loss: \u001b[1m\u001b[32m0.48674\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3775 | loss: 0.48674 - acc: 0.7879 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3776  | total loss: \u001b[1m\u001b[32m0.48236\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3776 | loss: 0.48236 - acc: 0.7903 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3777  | total loss: \u001b[1m\u001b[32m0.47834\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3777 | loss: 0.47834 - acc: 0.7918 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3778  | total loss: \u001b[1m\u001b[32m0.47470\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3778 | loss: 0.47470 - acc: 0.7935 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3779  | total loss: \u001b[1m\u001b[32m0.47146\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3779 | loss: 0.47146 - acc: 0.7943 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3780  | total loss: \u001b[1m\u001b[32m0.52790\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3780 | loss: 0.52790 - acc: 0.7680 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3781  | total loss: \u001b[1m\u001b[32m0.51931\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3781 | loss: 0.51931 - acc: 0.7715 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3782  | total loss: \u001b[1m\u001b[32m0.56269\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3782 | loss: 0.56269 - acc: 0.7493 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3783  | total loss: \u001b[1m\u001b[32m0.55073\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3783 | loss: 0.55073 - acc: 0.7546 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3784  | total loss: \u001b[1m\u001b[32m0.58260\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3784 | loss: 0.58260 - acc: 0.7338 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3785  | total loss: \u001b[1m\u001b[32m0.57023\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3785 | loss: 0.57023 - acc: 0.7407 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3786  | total loss: \u001b[1m\u001b[32m0.59558\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3786 | loss: 0.59558 - acc: 0.7211 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3787  | total loss: \u001b[1m\u001b[32m0.58426\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3787 | loss: 0.58426 - acc: 0.7286 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3788  | total loss: \u001b[1m\u001b[32m0.60524\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3788 | loss: 0.60524 - acc: 0.7091 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3789  | total loss: \u001b[1m\u001b[32m0.59499\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3789 | loss: 0.59499 - acc: 0.7173 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3790  | total loss: \u001b[1m\u001b[32m0.61062\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3790 | loss: 0.61062 - acc: 0.6998 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3791  | total loss: \u001b[1m\u001b[32m0.60115\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3791 | loss: 0.60115 - acc: 0.7091 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3792  | total loss: \u001b[1m\u001b[32m0.61318\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3792 | loss: 0.61318 - acc: 0.6945 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3793  | total loss: \u001b[1m\u001b[32m0.60431\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3793 | loss: 0.60431 - acc: 0.7046 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3794  | total loss: \u001b[1m\u001b[32m0.59653\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 3794 | loss: 0.59653 - acc: 0.7136 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3795  | total loss: \u001b[1m\u001b[32m0.58943\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3795 | loss: 0.58943 - acc: 0.7215 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3796  | total loss: \u001b[1m\u001b[32m0.58269\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3796 | loss: 0.58269 - acc: 0.7295 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3797  | total loss: \u001b[1m\u001b[32m0.57610\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3797 | loss: 0.57610 - acc: 0.7364 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3798  | total loss: \u001b[1m\u001b[32m0.59019\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3798 | loss: 0.59019 - acc: 0.7195 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3799  | total loss: \u001b[1m\u001b[32m0.58194\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3799 | loss: 0.58194 - acc: 0.7272 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3800  | total loss: \u001b[1m\u001b[32m0.57414\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3800 | loss: 0.57414 - acc: 0.7344 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3801  | total loss: \u001b[1m\u001b[32m0.56664\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3801 | loss: 0.56664 - acc: 0.7411 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3802  | total loss: \u001b[1m\u001b[32m0.58391\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3802 | loss: 0.58391 - acc: 0.7242 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3803  | total loss: \u001b[1m\u001b[32m0.57472\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3803 | loss: 0.57472 - acc: 0.7297 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3804  | total loss: \u001b[1m\u001b[32m0.59468\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3804 | loss: 0.59468 - acc: 0.7115 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3805  | total loss: \u001b[1m\u001b[32m0.58422\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3805 | loss: 0.58422 - acc: 0.7185 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3806  | total loss: \u001b[1m\u001b[32m0.60629\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3806 | loss: 0.60629 - acc: 0.7008 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3807  | total loss: \u001b[1m\u001b[32m0.59491\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3807 | loss: 0.59491 - acc: 0.7106 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3808  | total loss: \u001b[1m\u001b[32m0.58482\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3808 | loss: 0.58482 - acc: 0.7197 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3809  | total loss: \u001b[1m\u001b[32m0.57573\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3809 | loss: 0.57573 - acc: 0.7280 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3810  | total loss: \u001b[1m\u001b[32m0.59383\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3810 | loss: 0.59383 - acc: 0.7097 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3811  | total loss: \u001b[1m\u001b[32m0.58385\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3811 | loss: 0.58385 - acc: 0.7195 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3812  | total loss: \u001b[1m\u001b[32m0.57488\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3812 | loss: 0.57488 - acc: 0.7283 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3813  | total loss: \u001b[1m\u001b[32m0.56662\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3813 | loss: 0.56662 - acc: 0.7361 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3814  | total loss: \u001b[1m\u001b[32m0.58343\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3814 | loss: 0.58343 - acc: 0.7197 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3815  | total loss: \u001b[1m\u001b[32m0.57399\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3815 | loss: 0.57399 - acc: 0.7290 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3816  | total loss: \u001b[1m\u001b[32m0.56530\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3816 | loss: 0.56530 - acc: 0.7378 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3817  | total loss: \u001b[1m\u001b[32m0.55712\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3817 | loss: 0.55712 - acc: 0.7459 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3818  | total loss: \u001b[1m\u001b[32m0.58197\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3818 | loss: 0.58197 - acc: 0.7240 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3819  | total loss: \u001b[1m\u001b[32m0.57165\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3819 | loss: 0.57165 - acc: 0.7335 -- iter: 891/891\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 3820  | total loss: \u001b[1m\u001b[32m0.59079\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3820 | loss: 0.59079 - acc: 0.7144 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3821  | total loss: \u001b[1m\u001b[32m0.57978\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3821 | loss: 0.57978 - acc: 0.7244 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3822  | total loss: \u001b[1m\u001b[32m0.59692\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3822 | loss: 0.59692 - acc: 0.7059 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3823  | total loss: \u001b[1m\u001b[32m0.58588\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3823 | loss: 0.58588 - acc: 0.7171 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3824  | total loss: \u001b[1m\u001b[32m0.60168\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3824 | loss: 0.60168 - acc: 0.7010 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3825  | total loss: \u001b[1m\u001b[32m0.59089\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3825 | loss: 0.59089 - acc: 0.7123 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3826  | total loss: \u001b[1m\u001b[32m0.58139\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3826 | loss: 0.58139 - acc: 0.7224 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3827  | total loss: \u001b[1m\u001b[32m0.57271\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3827 | loss: 0.57271 - acc: 0.7315 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3828  | total loss: \u001b[1m\u001b[32m0.56444\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3828 | loss: 0.56444 - acc: 0.7396 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3829  | total loss: \u001b[1m\u001b[32m0.55632\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3829 | loss: 0.55632 - acc: 0.7470 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3830  | total loss: \u001b[1m\u001b[32m0.57540\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3830 | loss: 0.57540 - acc: 0.7272 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3831  | total loss: \u001b[1m\u001b[32m0.56516\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3831 | loss: 0.56516 - acc: 0.7358 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3832  | total loss: \u001b[1m\u001b[32m0.58789\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3832 | loss: 0.58789 - acc: 0.7164 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3833  | total loss: \u001b[1m\u001b[32m0.57618\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3833 | loss: 0.57618 - acc: 0.7265 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3834  | total loss: \u001b[1m\u001b[32m0.59976\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3834 | loss: 0.59976 - acc: 0.7069 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3835  | total loss: \u001b[1m\u001b[32m0.58719\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3835 | loss: 0.58719 - acc: 0.7174 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3836  | total loss: \u001b[1m\u001b[32m0.60544\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3836 | loss: 0.60544 - acc: 0.7014 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3837  | total loss: \u001b[1m\u001b[32m0.59318\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3837 | loss: 0.59318 - acc: 0.7127 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3838  | total loss: \u001b[1m\u001b[32m0.58252\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3838 | loss: 0.58252 - acc: 0.7227 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3839  | total loss: \u001b[1m\u001b[32m0.57304\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3839 | loss: 0.57304 - acc: 0.7315 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3840  | total loss: \u001b[1m\u001b[32m0.59011\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3840 | loss: 0.59011 - acc: 0.7149 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3841  | total loss: \u001b[1m\u001b[32m0.57999\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3841 | loss: 0.57999 - acc: 0.7246 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3842  | total loss: \u001b[1m\u001b[32m0.57086\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3842 | loss: 0.57086 - acc: 0.7334 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3843  | total loss: \u001b[1m\u001b[32m0.56239\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3843 | loss: 0.56239 - acc: 0.7410 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3844  | total loss: \u001b[1m\u001b[32m0.55433\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3844 | loss: 0.55433 - acc: 0.7480 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3845  | total loss: \u001b[1m\u001b[32m0.54653\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3845 | loss: 0.54653 - acc: 0.7548 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3846  | total loss: \u001b[1m\u001b[32m0.53891\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 3846 | loss: 0.53891 - acc: 0.7608 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3847  | total loss: \u001b[1m\u001b[32m0.53147\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 3847 | loss: 0.53147 - acc: 0.7661 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3848  | total loss: \u001b[1m\u001b[32m0.52426\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 3848 | loss: 0.52426 - acc: 0.7706 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3849  | total loss: \u001b[1m\u001b[32m0.51738\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3849 | loss: 0.51738 - acc: 0.7748 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3850  | total loss: \u001b[1m\u001b[32m0.51094\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3850 | loss: 0.51094 - acc: 0.7785 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3851  | total loss: \u001b[1m\u001b[32m0.50500\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 3851 | loss: 0.50500 - acc: 0.7819 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3852  | total loss: \u001b[1m\u001b[32m0.55288\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3852 | loss: 0.55288 - acc: 0.7559 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3853  | total loss: \u001b[1m\u001b[32m0.54257\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3853 | loss: 0.54257 - acc: 0.7603 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3854  | total loss: \u001b[1m\u001b[32m0.58226\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 3854 | loss: 0.58226 - acc: 0.7372 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3855  | total loss: \u001b[1m\u001b[32m0.56867\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3855 | loss: 0.56867 - acc: 0.7438 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3856  | total loss: \u001b[1m\u001b[32m0.60430\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3856 | loss: 0.60430 - acc: 0.7216 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3857  | total loss: \u001b[1m\u001b[32m0.58897\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 3857 | loss: 0.58897 - acc: 0.7300 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3858  | total loss: \u001b[1m\u001b[32m0.61278\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 3858 | loss: 0.61278 - acc: 0.7116 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3859  | total loss: \u001b[1m\u001b[32m0.59889\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 3859 | loss: 0.59889 - acc: 0.7209 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3860  | total loss: \u001b[1m\u001b[32m0.61541\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3860 | loss: 0.61541 - acc: 0.7045 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3861  | total loss: \u001b[1m\u001b[32m0.60392\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 3861 | loss: 0.60392 - acc: 0.7144 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3862  | total loss: \u001b[1m\u001b[32m0.59440\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 3862 | loss: 0.59440 - acc: 0.7230 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3863  | total loss: \u001b[1m\u001b[32m0.58607\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 3863 | loss: 0.58607 - acc: 0.7307 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3864  | total loss: \u001b[1m\u001b[32m0.60247\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 3864 | loss: 0.60247 - acc: 0.7117 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3865  | total loss: \u001b[1m\u001b[32m0.59316\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 3865 | loss: 0.59316 - acc: 0.7217 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3866  | total loss: \u001b[1m\u001b[32m0.60822\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 3866 | loss: 0.60822 - acc: 0.7032 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3867  | total loss: \u001b[1m\u001b[32m0.59807\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 3867 | loss: 0.59807 - acc: 0.7135 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3868  | total loss: \u001b[1m\u001b[32m0.58869\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3868 | loss: 0.58869 - acc: 0.7233 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3869  | total loss: \u001b[1m\u001b[32m0.57983\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 3869 | loss: 0.57983 - acc: 0.7325 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3870  | total loss: \u001b[1m\u001b[32m0.59607\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 3870 | loss: 0.59607 - acc: 0.7123 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3871  | total loss: \u001b[1m\u001b[32m0.58583\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3871 | loss: 0.58583 - acc: 0.7217 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3872  | total loss: \u001b[1m\u001b[32m0.57636\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3872 | loss: 0.57636 - acc: 0.7299 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3873  | total loss: \u001b[1m\u001b[32m0.56751\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3873 | loss: 0.56751 - acc: 0.7375 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3874  | total loss: \u001b[1m\u001b[32m0.58507\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3874 | loss: 0.58507 - acc: 0.7205 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3875  | total loss: \u001b[1m\u001b[32m0.57491\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3875 | loss: 0.57491 - acc: 0.7289 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3876  | total loss: \u001b[1m\u001b[32m0.59381\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3876 | loss: 0.59381 - acc: 0.7137 -- iter: 891/891\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 3877  | total loss: \u001b[1m\u001b[32m0.58274\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3877 | loss: 0.58274 - acc: 0.7227 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3878  | total loss: \u001b[1m\u001b[32m0.59899\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3878 | loss: 0.59899 - acc: 0.7079 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3879  | total loss: \u001b[1m\u001b[32m0.58760\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3879 | loss: 0.58760 - acc: 0.7175 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3880  | total loss: \u001b[1m\u001b[32m0.60658\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3880 | loss: 0.60658 - acc: 0.6998 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3881  | total loss: \u001b[1m\u001b[32m0.59497\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3881 | loss: 0.59497 - acc: 0.7109 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3882  | total loss: \u001b[1m\u001b[32m0.61259\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3882 | loss: 0.61259 - acc: 0.6932 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3883  | total loss: \u001b[1m\u001b[32m0.60136\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3883 | loss: 0.60136 - acc: 0.7041 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3884  | total loss: \u001b[1m\u001b[32m0.59173\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3884 | loss: 0.59173 - acc: 0.7144 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3885  | total loss: \u001b[1m\u001b[32m0.58329\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3885 | loss: 0.58329 - acc: 0.7245 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3886  | total loss: \u001b[1m\u001b[32m0.57567\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3886 | loss: 0.57567 - acc: 0.7334 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3887  | total loss: \u001b[1m\u001b[32m0.56853\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3887 | loss: 0.56853 - acc: 0.7415 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3888  | total loss: \u001b[1m\u001b[32m0.56163\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3888 | loss: 0.56163 - acc: 0.7489 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3889  | total loss: \u001b[1m\u001b[32m0.55477\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3889 | loss: 0.55477 - acc: 0.7559 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3890  | total loss: \u001b[1m\u001b[32m0.57429\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3890 | loss: 0.57429 - acc: 0.7353 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3891  | total loss: \u001b[1m\u001b[32m0.56507\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3891 | loss: 0.56507 - acc: 0.7435 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3892  | total loss: \u001b[1m\u001b[32m0.55630\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3892 | loss: 0.55630 - acc: 0.7507 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3893  | total loss: \u001b[1m\u001b[32m0.54788\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3893 | loss: 0.54788 - acc: 0.7574 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3894  | total loss: \u001b[1m\u001b[32m0.53973\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3894 | loss: 0.53973 - acc: 0.7633 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3895  | total loss: \u001b[1m\u001b[32m0.53185\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3895 | loss: 0.53185 - acc: 0.7687 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3896  | total loss: \u001b[1m\u001b[32m0.56234\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3896 | loss: 0.56234 - acc: 0.7475 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3897  | total loss: \u001b[1m\u001b[32m0.55160\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3897 | loss: 0.55160 - acc: 0.7538 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3898  | total loss: \u001b[1m\u001b[32m0.54177\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3898 | loss: 0.54177 - acc: 0.7594 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3899  | total loss: \u001b[1m\u001b[32m0.53273\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3899 | loss: 0.53273 - acc: 0.7645 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3900  | total loss: \u001b[1m\u001b[32m0.52440\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3900 | loss: 0.52440 - acc: 0.7692 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3901  | total loss: \u001b[1m\u001b[32m0.51673\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3901 | loss: 0.51673 - acc: 0.7737 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3902  | total loss: \u001b[1m\u001b[32m0.55201\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3902 | loss: 0.55201 - acc: 0.7515 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3903  | total loss: \u001b[1m\u001b[32m0.54147\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3903 | loss: 0.54147 - acc: 0.7573 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3904  | total loss: \u001b[1m\u001b[32m0.57834\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3904 | loss: 0.57834 - acc: 0.7343 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3905  | total loss: \u001b[1m\u001b[32m0.56586\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3905 | loss: 0.56586 - acc: 0.7411 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3906  | total loss: \u001b[1m\u001b[32m0.55519\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3906 | loss: 0.55519 - acc: 0.7475 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3907  | total loss: \u001b[1m\u001b[32m0.54600\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3907 | loss: 0.54600 - acc: 0.7538 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3908  | total loss: \u001b[1m\u001b[32m0.53790\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3908 | loss: 0.53790 - acc: 0.7599 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3909  | total loss: \u001b[1m\u001b[32m0.53047\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3909 | loss: 0.53047 - acc: 0.7653 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3910  | total loss: \u001b[1m\u001b[32m0.52342\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3910 | loss: 0.52342 - acc: 0.7698 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3911  | total loss: \u001b[1m\u001b[32m0.51666\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3911 | loss: 0.51666 - acc: 0.7733 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3912  | total loss: \u001b[1m\u001b[32m0.54969\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3912 | loss: 0.54969 - acc: 0.7495 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3913  | total loss: \u001b[1m\u001b[32m0.53998\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3913 | loss: 0.53998 - acc: 0.7557 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3914  | total loss: \u001b[1m\u001b[32m0.57115\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3914 | loss: 0.57115 - acc: 0.7337 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3915  | total loss: \u001b[1m\u001b[32m0.55966\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3915 | loss: 0.55966 - acc: 0.7416 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3916  | total loss: \u001b[1m\u001b[32m0.54960\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3916 | loss: 0.54960 - acc: 0.7487 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3917  | total loss: \u001b[1m\u001b[32m0.54070\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3917 | loss: 0.54070 - acc: 0.7551 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3918  | total loss: \u001b[1m\u001b[32m0.56477\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3918 | loss: 0.56477 - acc: 0.7362 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3919  | total loss: \u001b[1m\u001b[32m0.55475\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3919 | loss: 0.55475 - acc: 0.7437 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3920  | total loss: \u001b[1m\u001b[32m0.57885\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3920 | loss: 0.57885 - acc: 0.7230 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3921  | total loss: \u001b[1m\u001b[32m0.56829\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 3921 | loss: 0.56829 - acc: 0.7316 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3922  | total loss: \u001b[1m\u001b[32m0.59100\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 3922 | loss: 0.59100 - acc: 0.7112 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3923  | total loss: \u001b[1m\u001b[32m0.58040\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3923 | loss: 0.58040 - acc: 0.7212 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3924  | total loss: \u001b[1m\u001b[32m0.59794\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3924 | loss: 0.59794 - acc: 0.7022 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3925  | total loss: \u001b[1m\u001b[32m0.58794\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3925 | loss: 0.58794 - acc: 0.7129 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3926  | total loss: \u001b[1m\u001b[32m0.57940\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3926 | loss: 0.57940 - acc: 0.7224 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3927  | total loss: \u001b[1m\u001b[32m0.57182\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3927 | loss: 0.57182 - acc: 0.7310 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3928  | total loss: \u001b[1m\u001b[32m0.56482\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3928 | loss: 0.56482 - acc: 0.7386 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3929  | total loss: \u001b[1m\u001b[32m0.55811\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3929 | loss: 0.55811 - acc: 0.7456 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3930  | total loss: \u001b[1m\u001b[32m0.57776\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3930 | loss: 0.57776 - acc: 0.7240 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3931  | total loss: \u001b[1m\u001b[32m0.56902\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3931 | loss: 0.56902 - acc: 0.7326 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3932  | total loss: \u001b[1m\u001b[32m0.58585\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3932 | loss: 0.58585 - acc: 0.7145 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3933  | total loss: \u001b[1m\u001b[32m0.57596\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3933 | loss: 0.57596 - acc: 0.7245 -- iter: 891/891\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 3934  | total loss: \u001b[1m\u001b[32m0.59528\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3934 | loss: 0.59528 - acc: 0.7052 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3935  | total loss: \u001b[1m\u001b[32m0.58442\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3935 | loss: 0.58442 - acc: 0.7163 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3936  | total loss: \u001b[1m\u001b[32m0.60288\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3936 | loss: 0.60288 - acc: 0.6979 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3937  | total loss: \u001b[1m\u001b[32m0.59158\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3937 | loss: 0.59158 - acc: 0.7087 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3938  | total loss: \u001b[1m\u001b[32m0.60651\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3938 | loss: 0.60651 - acc: 0.6932 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3939  | total loss: \u001b[1m\u001b[32m0.59537\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3939 | loss: 0.59537 - acc: 0.7054 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3940  | total loss: \u001b[1m\u001b[32m0.60848\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3940 | loss: 0.60848 - acc: 0.6912 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3941  | total loss: \u001b[1m\u001b[32m0.59773\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3941 | loss: 0.59773 - acc: 0.7034 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3942  | total loss: \u001b[1m\u001b[32m0.58822\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3942 | loss: 0.58822 - acc: 0.7149 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3943  | total loss: \u001b[1m\u001b[32m0.57958\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3943 | loss: 0.57958 - acc: 0.7251 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3944  | total loss: \u001b[1m\u001b[32m0.59646\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3944 | loss: 0.59646 - acc: 0.7071 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3945  | total loss: \u001b[1m\u001b[32m0.58676\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3945 | loss: 0.58676 - acc: 0.7176 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3946  | total loss: \u001b[1m\u001b[32m0.60231\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3946 | loss: 0.60231 - acc: 0.7005 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3947  | total loss: \u001b[1m\u001b[32m0.59199\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3947 | loss: 0.59199 - acc: 0.7116 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3948  | total loss: \u001b[1m\u001b[32m0.58262\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3948 | loss: 0.58262 - acc: 0.7218 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3949  | total loss: \u001b[1m\u001b[32m0.57392\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3949 | loss: 0.57392 - acc: 0.7309 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3950  | total loss: \u001b[1m\u001b[32m0.59001\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3950 | loss: 0.59001 - acc: 0.7132 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3951  | total loss: \u001b[1m\u001b[32m0.58007\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3951 | loss: 0.58007 - acc: 0.7238 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3952  | total loss: \u001b[1m\u001b[32m0.57086\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3952 | loss: 0.57086 - acc: 0.7334 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3953  | total loss: \u001b[1m\u001b[32m0.56217\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3953 | loss: 0.56217 - acc: 0.7420 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3954  | total loss: \u001b[1m\u001b[32m0.55386\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3954 | loss: 0.55386 - acc: 0.7495 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3955  | total loss: \u001b[1m\u001b[32m0.54585\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3955 | loss: 0.54585 - acc: 0.7558 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3956  | total loss: \u001b[1m\u001b[32m0.56951\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3956 | loss: 0.56951 - acc: 0.7364 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3957  | total loss: \u001b[1m\u001b[32m0.55924\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3957 | loss: 0.55924 - acc: 0.7443 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3958  | total loss: \u001b[1m\u001b[32m0.58378\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3958 | loss: 0.58378 - acc: 0.7238 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3959  | total loss: \u001b[1m\u001b[32m0.57192\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3959 | loss: 0.57192 - acc: 0.7330 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3960  | total loss: \u001b[1m\u001b[32m0.56124\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3960 | loss: 0.56124 - acc: 0.7417 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3961  | total loss: \u001b[1m\u001b[32m0.55152\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3961 | loss: 0.55152 - acc: 0.7489 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3962  | total loss: \u001b[1m\u001b[32m0.57724\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3962 | loss: 0.57724 - acc: 0.7266 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3963  | total loss: \u001b[1m\u001b[32m0.56593\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3963 | loss: 0.56593 - acc: 0.7352 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3964  | total loss: \u001b[1m\u001b[32m0.59147\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3964 | loss: 0.59147 - acc: 0.7137 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3965  | total loss: \u001b[1m\u001b[32m0.57941\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3965 | loss: 0.57941 - acc: 0.7235 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3966  | total loss: \u001b[1m\u001b[32m0.59786\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3966 | loss: 0.59786 - acc: 0.7065 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3967  | total loss: \u001b[1m\u001b[32m0.58638\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3967 | loss: 0.58638 - acc: 0.7171 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3968  | total loss: \u001b[1m\u001b[32m0.60583\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3968 | loss: 0.60583 - acc: 0.6987 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3969  | total loss: \u001b[1m\u001b[32m0.59497\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3969 | loss: 0.59497 - acc: 0.7097 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3970  | total loss: \u001b[1m\u001b[32m0.60706\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3970 | loss: 0.60706 - acc: 0.6945 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3971  | total loss: \u001b[1m\u001b[32m0.59719\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3971 | loss: 0.59719 - acc: 0.7057 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3972  | total loss: \u001b[1m\u001b[32m0.58852\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3972 | loss: 0.58852 - acc: 0.7162 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3973  | total loss: \u001b[1m\u001b[32m0.58058\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3973 | loss: 0.58058 - acc: 0.7252 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3974  | total loss: \u001b[1m\u001b[32m0.57300\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3974 | loss: 0.57300 - acc: 0.7331 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3975  | total loss: \u001b[1m\u001b[32m0.56552\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3975 | loss: 0.56552 - acc: 0.7405 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3976  | total loss: \u001b[1m\u001b[32m0.55800\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3976 | loss: 0.55800 - acc: 0.7474 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3977  | total loss: \u001b[1m\u001b[32m0.55044\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3977 | loss: 0.55044 - acc: 0.7538 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3978  | total loss: \u001b[1m\u001b[32m0.57324\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3978 | loss: 0.57324 - acc: 0.7313 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3979  | total loss: \u001b[1m\u001b[32m0.56321\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3979 | loss: 0.56321 - acc: 0.7394 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3980  | total loss: \u001b[1m\u001b[32m0.55393\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3980 | loss: 0.55393 - acc: 0.7467 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3981  | total loss: \u001b[1m\u001b[32m0.54532\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3981 | loss: 0.54532 - acc: 0.7528 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3982  | total loss: \u001b[1m\u001b[32m0.57290\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3982 | loss: 0.57290 - acc: 0.7322 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3983  | total loss: \u001b[1m\u001b[32m0.56214\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3983 | loss: 0.56214 - acc: 0.7397 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3984  | total loss: \u001b[1m\u001b[32m0.58638\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3984 | loss: 0.58638 - acc: 0.7222 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3985  | total loss: \u001b[1m\u001b[32m0.57437\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3985 | loss: 0.57437 - acc: 0.7312 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3986  | total loss: \u001b[1m\u001b[32m0.56367\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 3986 | loss: 0.56367 - acc: 0.7395 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3987  | total loss: \u001b[1m\u001b[32m0.55403\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 3987 | loss: 0.55403 - acc: 0.7467 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3988  | total loss: \u001b[1m\u001b[32m0.58327\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 3988 | loss: 0.58327 - acc: 0.7247 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3989  | total loss: \u001b[1m\u001b[32m0.57192\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3989 | loss: 0.57192 - acc: 0.7338 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3990  | total loss: \u001b[1m\u001b[32m0.59129\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 3990 | loss: 0.59129 - acc: 0.7160 -- iter: 891/891\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 3991  | total loss: \u001b[1m\u001b[32m0.57997\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3991 | loss: 0.57997 - acc: 0.7255 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3992  | total loss: \u001b[1m\u001b[32m0.59896\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3992 | loss: 0.59896 - acc: 0.7074 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3993  | total loss: \u001b[1m\u001b[32m0.58822\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3993 | loss: 0.58822 - acc: 0.7180 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3994  | total loss: \u001b[1m\u001b[32m0.57913\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3994 | loss: 0.57913 - acc: 0.7276 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3995  | total loss: \u001b[1m\u001b[32m0.57119\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3995 | loss: 0.57119 - acc: 0.7360 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3996  | total loss: \u001b[1m\u001b[32m0.58984\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3996 | loss: 0.58984 - acc: 0.7143 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3997  | total loss: \u001b[1m\u001b[32m0.58089\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3997 | loss: 0.58089 - acc: 0.7243 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3998  | total loss: \u001b[1m\u001b[32m0.57269\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 3998 | loss: 0.57269 - acc: 0.7332 -- iter: 891/891\n",
      "--\n",
      "Training Step: 3999  | total loss: \u001b[1m\u001b[32m0.56490\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 3999 | loss: 0.56490 - acc: 0.7415 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4000  | total loss: \u001b[1m\u001b[32m0.58557\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4000 | loss: 0.58557 - acc: 0.7200 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4001  | total loss: \u001b[1m\u001b[32m0.57574\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4001 | loss: 0.57574 - acc: 0.7293 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4002  | total loss: \u001b[1m\u001b[32m0.56657\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4002 | loss: 0.56657 - acc: 0.7379 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4003  | total loss: \u001b[1m\u001b[32m0.55789\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4003 | loss: 0.55789 - acc: 0.7457 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4004  | total loss: \u001b[1m\u001b[32m0.54959\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4004 | loss: 0.54959 - acc: 0.7526 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4005  | total loss: \u001b[1m\u001b[32m0.54165\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4005 | loss: 0.54165 - acc: 0.7587 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4006  | total loss: \u001b[1m\u001b[32m0.57057\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4006 | loss: 0.57057 - acc: 0.7351 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4007  | total loss: \u001b[1m\u001b[32m0.56004\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4007 | loss: 0.56004 - acc: 0.7428 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4008  | total loss: \u001b[1m\u001b[32m0.58773\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4008 | loss: 0.58773 - acc: 0.7229 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4009  | total loss: \u001b[1m\u001b[32m0.57548\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4009 | loss: 0.57548 - acc: 0.7321 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4010  | total loss: \u001b[1m\u001b[32m0.59826\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4010 | loss: 0.59826 - acc: 0.7147 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4011  | total loss: \u001b[1m\u001b[32m0.58534\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4011 | loss: 0.58534 - acc: 0.7249 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4012  | total loss: \u001b[1m\u001b[32m0.60197\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4012 | loss: 0.60197 - acc: 0.7099 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4013  | total loss: \u001b[1m\u001b[32m0.58960\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4013 | loss: 0.58960 - acc: 0.7205 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4014  | total loss: \u001b[1m\u001b[32m0.60806\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4014 | loss: 0.60806 - acc: 0.7022 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4015  | total loss: \u001b[1m\u001b[32m0.59639\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4015 | loss: 0.59639 - acc: 0.7132 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4016  | total loss: \u001b[1m\u001b[32m0.58645\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4016 | loss: 0.58645 - acc: 0.7230 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4017  | total loss: \u001b[1m\u001b[32m0.57774\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4017 | loss: 0.57774 - acc: 0.7316 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4018  | total loss: \u001b[1m\u001b[32m0.59615\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4018 | loss: 0.59615 - acc: 0.7095 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4019  | total loss: \u001b[1m\u001b[32m0.58661\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4019 | loss: 0.58661 - acc: 0.7195 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4020  | total loss: \u001b[1m\u001b[32m0.57796\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4020 | loss: 0.57796 - acc: 0.7285 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4021  | total loss: \u001b[1m\u001b[32m0.56984\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4021 | loss: 0.56984 - acc: 0.7368 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4022  | total loss: \u001b[1m\u001b[32m0.58776\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4022 | loss: 0.58776 - acc: 0.7166 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4023  | total loss: \u001b[1m\u001b[32m0.57798\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4023 | loss: 0.57798 - acc: 0.7261 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4024  | total loss: \u001b[1m\u001b[32m0.56884\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4024 | loss: 0.56884 - acc: 0.7349 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4025  | total loss: \u001b[1m\u001b[32m0.56014\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 4025 | loss: 0.56014 - acc: 0.7426 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4026  | total loss: \u001b[1m\u001b[32m0.58237\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4026 | loss: 0.58237 - acc: 0.7216 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4027  | total loss: \u001b[1m\u001b[32m0.57172\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4027 | loss: 0.57172 - acc: 0.7308 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4028  | total loss: \u001b[1m\u001b[32m0.56195\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 4028 | loss: 0.56195 - acc: 0.7394 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4029  | total loss: \u001b[1m\u001b[32m0.55289\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 4029 | loss: 0.55289 - acc: 0.7469 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4030  | total loss: \u001b[1m\u001b[32m0.57435\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 4030 | loss: 0.57435 - acc: 0.7292 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4031  | total loss: \u001b[1m\u001b[32m0.56371\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 4031 | loss: 0.56371 - acc: 0.7376 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4032  | total loss: \u001b[1m\u001b[32m0.55404\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4032 | loss: 0.55404 - acc: 0.7447 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4033  | total loss: \u001b[1m\u001b[32m0.54516\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 4033 | loss: 0.54516 - acc: 0.7513 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4034  | total loss: \u001b[1m\u001b[32m0.57012\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 4034 | loss: 0.57012 - acc: 0.7333 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4035  | total loss: \u001b[1m\u001b[32m0.55942\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 4035 | loss: 0.55942 - acc: 0.7412 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4036  | total loss: \u001b[1m\u001b[32m0.58573\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 4036 | loss: 0.58573 - acc: 0.7229 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4037  | total loss: \u001b[1m\u001b[32m0.57376\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 4037 | loss: 0.57376 - acc: 0.7320 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4038  | total loss: \u001b[1m\u001b[32m0.56323\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 4038 | loss: 0.56323 - acc: 0.7398 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4039  | total loss: \u001b[1m\u001b[32m0.55388\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 4039 | loss: 0.55388 - acc: 0.7471 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4040  | total loss: \u001b[1m\u001b[32m0.54545\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 4040 | loss: 0.54545 - acc: 0.7534 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4041  | total loss: \u001b[1m\u001b[32m0.53772\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 4041 | loss: 0.53772 - acc: 0.7592 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4042  | total loss: \u001b[1m\u001b[32m0.56300\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 4042 | loss: 0.56300 - acc: 0.7372 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4043  | total loss: \u001b[1m\u001b[32m0.55334\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 4043 | loss: 0.55334 - acc: 0.7444 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4044  | total loss: \u001b[1m\u001b[32m0.54461\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 4044 | loss: 0.54461 - acc: 0.7511 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4045  | total loss: \u001b[1m\u001b[32m0.53657\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 4045 | loss: 0.53657 - acc: 0.7570 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4046  | total loss: \u001b[1m\u001b[32m0.56033\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4046 | loss: 0.56033 - acc: 0.7377 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4047  | total loss: \u001b[1m\u001b[32m0.55049\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 4047 | loss: 0.55049 - acc: 0.7451 -- iter: 891/891\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 4048  | total loss: \u001b[1m\u001b[32m0.57478\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 4048 | loss: 0.57478 - acc: 0.7266 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4049  | total loss: \u001b[1m\u001b[32m0.56381\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 4049 | loss: 0.56381 - acc: 0.7351 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4050  | total loss: \u001b[1m\u001b[32m0.55411\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 4050 | loss: 0.55411 - acc: 0.7426 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4051  | total loss: \u001b[1m\u001b[32m0.54537\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 4051 | loss: 0.54537 - acc: 0.7494 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4052  | total loss: \u001b[1m\u001b[32m0.57286\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 4052 | loss: 0.57286 - acc: 0.7273 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4053  | total loss: \u001b[1m\u001b[32m0.56231\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4053 | loss: 0.56231 - acc: 0.7355 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4054  | total loss: \u001b[1m\u001b[32m0.58560\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 4054 | loss: 0.58560 - acc: 0.7153 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4055  | total loss: \u001b[1m\u001b[32m0.57423\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 4055 | loss: 0.57423 - acc: 0.7248 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4056  | total loss: \u001b[1m\u001b[32m0.56418\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 4056 | loss: 0.56418 - acc: 0.7332 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4057  | total loss: \u001b[1m\u001b[32m0.55510\u001b[0m\u001b[0m | time: 0.011s\n",
      "| Adam | epoch: 4057 | loss: 0.55510 - acc: 0.7410 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4058  | total loss: \u001b[1m\u001b[32m0.54673\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 4058 | loss: 0.54673 - acc: 0.7480 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4059  | total loss: \u001b[1m\u001b[32m0.53891\u001b[0m\u001b[0m | time: 0.011s\n",
      "| Adam | epoch: 4059 | loss: 0.53891 - acc: 0.7544 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4060  | total loss: \u001b[1m\u001b[32m0.53155\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 4060 | loss: 0.53155 - acc: 0.7607 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4061  | total loss: \u001b[1m\u001b[32m0.52460\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 4061 | loss: 0.52460 - acc: 0.7662 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4062  | total loss: \u001b[1m\u001b[32m0.51803\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4062 | loss: 0.51803 - acc: 0.7708 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4063  | total loss: \u001b[1m\u001b[32m0.51185\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4063 | loss: 0.51185 - acc: 0.7752 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4064  | total loss: \u001b[1m\u001b[32m0.54745\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4064 | loss: 0.54745 - acc: 0.7517 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4065  | total loss: \u001b[1m\u001b[32m0.53816\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4065 | loss: 0.53816 - acc: 0.7571 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4066  | total loss: \u001b[1m\u001b[32m0.52974\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 4066 | loss: 0.52974 - acc: 0.7619 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4067  | total loss: \u001b[1m\u001b[32m0.52200\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 4067 | loss: 0.52200 - acc: 0.7664 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4068  | total loss: \u001b[1m\u001b[32m0.55324\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 4068 | loss: 0.55324 - acc: 0.7462 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4069  | total loss: \u001b[1m\u001b[32m0.54304\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 4069 | loss: 0.54304 - acc: 0.7527 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4070  | total loss: \u001b[1m\u001b[32m0.53393\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 4070 | loss: 0.53393 - acc: 0.7587 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4071  | total loss: \u001b[1m\u001b[32m0.52580\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 4071 | loss: 0.52580 - acc: 0.7642 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4072  | total loss: \u001b[1m\u001b[32m0.55704\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 4072 | loss: 0.55704 - acc: 0.7411 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4073  | total loss: \u001b[1m\u001b[32m0.54710\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 4073 | loss: 0.54710 - acc: 0.7481 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4074  | total loss: \u001b[1m\u001b[32m0.57677\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4074 | loss: 0.57677 - acc: 0.7247 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4075  | total loss: \u001b[1m\u001b[32m0.56626\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4075 | loss: 0.56626 - acc: 0.7332 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4076  | total loss: \u001b[1m\u001b[32m0.58790\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4076 | loss: 0.58790 - acc: 0.7144 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4077  | total loss: \u001b[1m\u001b[32m0.57821\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4077 | loss: 0.57821 - acc: 0.7232 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4078  | total loss: \u001b[1m\u001b[32m0.59587\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4078 | loss: 0.59587 - acc: 0.7045 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4079  | total loss: \u001b[1m\u001b[32m0.58694\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4079 | loss: 0.58694 - acc: 0.7143 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4080  | total loss: \u001b[1m\u001b[32m0.57928\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4080 | loss: 0.57928 - acc: 0.7229 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4081  | total loss: \u001b[1m\u001b[32m0.57233\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4081 | loss: 0.57233 - acc: 0.7309 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4082  | total loss: \u001b[1m\u001b[32m0.58646\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4082 | loss: 0.58646 - acc: 0.7136 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4083  | total loss: \u001b[1m\u001b[32m0.57824\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4083 | loss: 0.57824 - acc: 0.7228 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4084  | total loss: \u001b[1m\u001b[32m0.57046\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4084 | loss: 0.57046 - acc: 0.7314 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4085  | total loss: \u001b[1m\u001b[32m0.56290\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4085 | loss: 0.56290 - acc: 0.7394 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4086  | total loss: \u001b[1m\u001b[32m0.58097\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4086 | loss: 0.58097 - acc: 0.7207 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4087  | total loss: \u001b[1m\u001b[32m0.57155\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4087 | loss: 0.57155 - acc: 0.7302 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4088  | total loss: \u001b[1m\u001b[32m0.58819\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4088 | loss: 0.58819 - acc: 0.7151 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4089  | total loss: \u001b[1m\u001b[32m0.57783\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4089 | loss: 0.57783 - acc: 0.7244 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4090  | total loss: \u001b[1m\u001b[32m0.56843\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4090 | loss: 0.56843 - acc: 0.7326 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4091  | total loss: \u001b[1m\u001b[32m0.55976\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4091 | loss: 0.55976 - acc: 0.7400 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4092  | total loss: \u001b[1m\u001b[32m0.55167\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4092 | loss: 0.55167 - acc: 0.7470 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4093  | total loss: \u001b[1m\u001b[32m0.54406\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4093 | loss: 0.54406 - acc: 0.7531 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4094  | total loss: \u001b[1m\u001b[32m0.53687\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4094 | loss: 0.53687 - acc: 0.7589 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4095  | total loss: \u001b[1m\u001b[32m0.53004\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4095 | loss: 0.53004 - acc: 0.7640 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4096  | total loss: \u001b[1m\u001b[32m0.52356\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4096 | loss: 0.52356 - acc: 0.7687 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4097  | total loss: \u001b[1m\u001b[32m0.51742\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4097 | loss: 0.51742 - acc: 0.7731 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4098  | total loss: \u001b[1m\u001b[32m0.55313\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4098 | loss: 0.55313 - acc: 0.7519 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4099  | total loss: \u001b[1m\u001b[32m0.54364\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4099 | loss: 0.54364 - acc: 0.7582 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4100  | total loss: \u001b[1m\u001b[32m0.53495\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4100 | loss: 0.53495 - acc: 0.7640 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4101  | total loss: \u001b[1m\u001b[32m0.52696\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4101 | loss: 0.52696 - acc: 0.7691 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4102  | total loss: \u001b[1m\u001b[32m0.51958\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4102 | loss: 0.51958 - acc: 0.7736 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4103  | total loss: \u001b[1m\u001b[32m0.51275\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4103 | loss: 0.51275 - acc: 0.7774 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4104  | total loss: \u001b[1m\u001b[32m0.54739\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4104 | loss: 0.54739 - acc: 0.7547 -- iter: 891/891\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 4105  | total loss: \u001b[1m\u001b[32m0.53764\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4105 | loss: 0.53764 - acc: 0.7610 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4106  | total loss: \u001b[1m\u001b[32m0.57182\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4106 | loss: 0.57182 - acc: 0.7375 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4107  | total loss: \u001b[1m\u001b[32m0.56029\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4107 | loss: 0.56029 - acc: 0.7450 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4108  | total loss: \u001b[1m\u001b[32m0.58673\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4108 | loss: 0.58673 - acc: 0.7246 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4109  | total loss: \u001b[1m\u001b[32m0.57554\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4109 | loss: 0.57554 - acc: 0.7328 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4110  | total loss: \u001b[1m\u001b[32m0.59690\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4110 | loss: 0.59690 - acc: 0.7131 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4111  | total loss: \u001b[1m\u001b[32m0.58691\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4111 | loss: 0.58691 - acc: 0.7220 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4112  | total loss: \u001b[1m\u001b[32m0.60497\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4112 | loss: 0.60497 - acc: 0.7038 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4113  | total loss: \u001b[1m\u001b[32m0.59566\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4113 | loss: 0.59566 - acc: 0.7131 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4114  | total loss: \u001b[1m\u001b[32m0.61136\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4114 | loss: 0.61136 - acc: 0.6958 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4115  | total loss: \u001b[1m\u001b[32m0.60205\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4115 | loss: 0.60205 - acc: 0.7058 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4116  | total loss: \u001b[1m\u001b[32m0.59360\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4116 | loss: 0.59360 - acc: 0.7157 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4117  | total loss: \u001b[1m\u001b[32m0.58563\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4117 | loss: 0.58563 - acc: 0.7246 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4118  | total loss: \u001b[1m\u001b[32m0.59886\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4118 | loss: 0.59886 - acc: 0.7070 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4119  | total loss: \u001b[1m\u001b[32m0.58964\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4119 | loss: 0.58964 - acc: 0.7169 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4120  | total loss: \u001b[1m\u001b[32m0.60255\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4120 | loss: 0.60255 - acc: 0.7014 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4121  | total loss: \u001b[1m\u001b[32m0.59251\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4121 | loss: 0.59251 - acc: 0.7117 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4122  | total loss: \u001b[1m\u001b[32m0.60669\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4122 | loss: 0.60669 - acc: 0.6952 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4123  | total loss: \u001b[1m\u001b[32m0.59617\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4123 | loss: 0.59617 - acc: 0.7062 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4124  | total loss: \u001b[1m\u001b[32m0.61278\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4124 | loss: 0.61278 - acc: 0.6874 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4125  | total loss: \u001b[1m\u001b[32m0.60186\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4125 | loss: 0.60186 - acc: 0.6993 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4126  | total loss: \u001b[1m\u001b[32m0.59203\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4126 | loss: 0.59203 - acc: 0.7098 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4127  | total loss: \u001b[1m\u001b[32m0.58302\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4127 | loss: 0.58302 - acc: 0.7194 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4128  | total loss: \u001b[1m\u001b[32m0.57463\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4128 | loss: 0.57463 - acc: 0.7263 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4129  | total loss: \u001b[1m\u001b[32m0.56677\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4129 | loss: 0.56677 - acc: 0.7320 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4130  | total loss: \u001b[1m\u001b[32m0.58662\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4130 | loss: 0.58662 - acc: 0.7139 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4131  | total loss: \u001b[1m\u001b[32m0.57705\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4131 | loss: 0.57705 - acc: 0.7208 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4132  | total loss: \u001b[1m\u001b[32m0.56820\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4132 | loss: 0.56820 - acc: 0.7273 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4133  | total loss: \u001b[1m\u001b[32m0.55994\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4133 | loss: 0.55994 - acc: 0.7350 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4134  | total loss: \u001b[1m\u001b[32m0.58120\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4134 | loss: 0.58120 - acc: 0.7170 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4135  | total loss: \u001b[1m\u001b[32m0.57127\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4135 | loss: 0.57127 - acc: 0.7256 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4136  | total loss: \u001b[1m\u001b[32m0.59315\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4136 | loss: 0.59315 - acc: 0.7075 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4137  | total loss: \u001b[1m\u001b[32m0.58201\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4137 | loss: 0.58201 - acc: 0.7173 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4138  | total loss: \u001b[1m\u001b[32m0.57202\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4138 | loss: 0.57202 - acc: 0.7266 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4139  | total loss: \u001b[1m\u001b[32m0.56299\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4139 | loss: 0.56299 - acc: 0.7346 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4140  | total loss: \u001b[1m\u001b[32m0.58411\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 4140 | loss: 0.58411 - acc: 0.7149 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4141  | total loss: \u001b[1m\u001b[32m0.57392\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4141 | loss: 0.57392 - acc: 0.7243 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4142  | total loss: \u001b[1m\u001b[32m0.56478\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4142 | loss: 0.56478 - acc: 0.7332 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4143  | total loss: \u001b[1m\u001b[32m0.55643\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4143 | loss: 0.55643 - acc: 0.7414 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4144  | total loss: \u001b[1m\u001b[32m0.57453\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4144 | loss: 0.57453 - acc: 0.7245 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4145  | total loss: \u001b[1m\u001b[32m0.56505\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4145 | loss: 0.56505 - acc: 0.7334 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4146  | total loss: \u001b[1m\u001b[32m0.58447\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4146 | loss: 0.58447 - acc: 0.7156 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4147  | total loss: \u001b[1m\u001b[32m0.57412\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 4147 | loss: 0.57412 - acc: 0.7253 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4148  | total loss: \u001b[1m\u001b[32m0.56481\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4148 | loss: 0.56481 - acc: 0.7338 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4149  | total loss: \u001b[1m\u001b[32m0.55622\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 4149 | loss: 0.55622 - acc: 0.7418 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4150  | total loss: \u001b[1m\u001b[32m0.54810\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 4150 | loss: 0.54810 - acc: 0.7491 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4151  | total loss: \u001b[1m\u001b[32m0.54026\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 4151 | loss: 0.54026 - acc: 0.7558 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4152  | total loss: \u001b[1m\u001b[32m0.56860\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4152 | loss: 0.56860 - acc: 0.7336 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4153  | total loss: \u001b[1m\u001b[32m0.55806\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4153 | loss: 0.55806 - acc: 0.7415 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4154  | total loss: \u001b[1m\u001b[32m0.54841\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 4154 | loss: 0.54841 - acc: 0.7485 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4155  | total loss: \u001b[1m\u001b[32m0.53948\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4155 | loss: 0.53948 - acc: 0.7549 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4156  | total loss: \u001b[1m\u001b[32m0.53116\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4156 | loss: 0.53116 - acc: 0.7608 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4157  | total loss: \u001b[1m\u001b[32m0.52338\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4157 | loss: 0.52338 - acc: 0.7663 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4158  | total loss: \u001b[1m\u001b[32m0.51612\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4158 | loss: 0.51612 - acc: 0.7711 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4159  | total loss: \u001b[1m\u001b[32m0.50939\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4159 | loss: 0.50939 - acc: 0.7752 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4160  | total loss: \u001b[1m\u001b[32m0.54682\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4160 | loss: 0.54682 - acc: 0.7536 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4161  | total loss: \u001b[1m\u001b[32m0.53686\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4161 | loss: 0.53686 - acc: 0.7586 -- iter: 891/891\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 4162  | total loss: \u001b[1m\u001b[32m0.52790\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4162 | loss: 0.52790 - acc: 0.7631 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4163  | total loss: \u001b[1m\u001b[32m0.51975\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4163 | loss: 0.51975 - acc: 0.7671 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4164  | total loss: \u001b[1m\u001b[32m0.51233\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4164 | loss: 0.51233 - acc: 0.7713 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4165  | total loss: \u001b[1m\u001b[32m0.50565\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4165 | loss: 0.50565 - acc: 0.7753 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4166  | total loss: \u001b[1m\u001b[32m0.54496\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4166 | loss: 0.54496 - acc: 0.7520 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4167  | total loss: \u001b[1m\u001b[32m0.53520\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4167 | loss: 0.53520 - acc: 0.7571 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4168  | total loss: \u001b[1m\u001b[32m0.52685\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4168 | loss: 0.52685 - acc: 0.7616 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4169  | total loss: \u001b[1m\u001b[32m0.51951\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4169 | loss: 0.51951 - acc: 0.7655 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4170  | total loss: \u001b[1m\u001b[32m0.51288\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4170 | loss: 0.51288 - acc: 0.7693 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4171  | total loss: \u001b[1m\u001b[32m0.50691\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4171 | loss: 0.50691 - acc: 0.7733 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4172  | total loss: \u001b[1m\u001b[32m0.50135\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4172 | loss: 0.50135 - acc: 0.7772 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4173  | total loss: \u001b[1m\u001b[32m0.49602\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4173 | loss: 0.49602 - acc: 0.7805 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4174  | total loss: \u001b[1m\u001b[32m0.53919\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4174 | loss: 0.53919 - acc: 0.7545 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4175  | total loss: \u001b[1m\u001b[32m0.52997\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4175 | loss: 0.52997 - acc: 0.7594 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4176  | total loss: \u001b[1m\u001b[32m0.52168\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4176 | loss: 0.52168 - acc: 0.7639 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4177  | total loss: \u001b[1m\u001b[32m0.51415\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4177 | loss: 0.51415 - acc: 0.7686 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4178  | total loss: \u001b[1m\u001b[32m0.50736\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4178 | loss: 0.50736 - acc: 0.7731 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4179  | total loss: \u001b[1m\u001b[32m0.50123\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4179 | loss: 0.50123 - acc: 0.7772 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4180  | total loss: \u001b[1m\u001b[32m0.49562\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4180 | loss: 0.49562 - acc: 0.7805 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4181  | total loss: \u001b[1m\u001b[32m0.49050\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4181 | loss: 0.49050 - acc: 0.7833 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4182  | total loss: \u001b[1m\u001b[32m0.48586\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4182 | loss: 0.48586 - acc: 0.7847 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4183  | total loss: \u001b[1m\u001b[32m0.48165\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4183 | loss: 0.48165 - acc: 0.7861 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4184  | total loss: \u001b[1m\u001b[32m0.53033\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4184 | loss: 0.53033 - acc: 0.7591 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4185  | total loss: \u001b[1m\u001b[32m0.52161\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4185 | loss: 0.52161 - acc: 0.7638 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4186  | total loss: \u001b[1m\u001b[32m0.55648\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4186 | loss: 0.55648 - acc: 0.7438 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4187  | total loss: \u001b[1m\u001b[32m0.54553\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4187 | loss: 0.54553 - acc: 0.7505 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4188  | total loss: \u001b[1m\u001b[32m0.57651\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4188 | loss: 0.57651 - acc: 0.7303 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4189  | total loss: \u001b[1m\u001b[32m0.56506\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4189 | loss: 0.56506 - acc: 0.7374 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4190  | total loss: \u001b[1m\u001b[32m0.58880\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4190 | loss: 0.58880 - acc: 0.7172 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4191  | total loss: \u001b[1m\u001b[32m0.57844\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4191 | loss: 0.57844 - acc: 0.7252 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4192  | total loss: \u001b[1m\u001b[32m0.56998\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4192 | loss: 0.56998 - acc: 0.7325 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4193  | total loss: \u001b[1m\u001b[32m0.56271\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4193 | loss: 0.56271 - acc: 0.7393 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4194  | total loss: \u001b[1m\u001b[32m0.55608\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4194 | loss: 0.55608 - acc: 0.7459 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4195  | total loss: \u001b[1m\u001b[32m0.54970\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4195 | loss: 0.54970 - acc: 0.7519 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4196  | total loss: \u001b[1m\u001b[32m0.57425\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4196 | loss: 0.57425 - acc: 0.7267 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4197  | total loss: \u001b[1m\u001b[32m0.56529\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4197 | loss: 0.56529 - acc: 0.7353 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4198  | total loss: \u001b[1m\u001b[32m0.58422\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4198 | loss: 0.58422 - acc: 0.7162 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4199  | total loss: \u001b[1m\u001b[32m0.57413\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4199 | loss: 0.57413 - acc: 0.7258 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4200  | total loss: \u001b[1m\u001b[32m0.56500\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4200 | loss: 0.56500 - acc: 0.7345 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4201  | total loss: \u001b[1m\u001b[32m0.55661\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4201 | loss: 0.55661 - acc: 0.7426 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4202  | total loss: \u001b[1m\u001b[32m0.57788\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4202 | loss: 0.57788 - acc: 0.7236 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4203  | total loss: \u001b[1m\u001b[32m0.56804\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4203 | loss: 0.56804 - acc: 0.7320 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4204  | total loss: \u001b[1m\u001b[32m0.58862\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4204 | loss: 0.58862 - acc: 0.7139 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4205  | total loss: \u001b[1m\u001b[32m0.57794\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4205 | loss: 0.57794 - acc: 0.7234 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4206  | total loss: \u001b[1m\u001b[32m0.59716\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4206 | loss: 0.59716 - acc: 0.7046 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4207  | total loss: \u001b[1m\u001b[32m0.58618\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4207 | loss: 0.58618 - acc: 0.7152 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4208  | total loss: \u001b[1m\u001b[32m0.57656\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4208 | loss: 0.57656 - acc: 0.7244 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4209  | total loss: \u001b[1m\u001b[32m0.56799\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4209 | loss: 0.56799 - acc: 0.7326 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4210  | total loss: \u001b[1m\u001b[32m0.58605\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4210 | loss: 0.58605 - acc: 0.7155 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4211  | total loss: \u001b[1m\u001b[32m0.57666\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4211 | loss: 0.57666 - acc: 0.7248 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4212  | total loss: \u001b[1m\u001b[32m0.56824\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4212 | loss: 0.56824 - acc: 0.7336 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4213  | total loss: \u001b[1m\u001b[32m0.56050\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4213 | loss: 0.56050 - acc: 0.7419 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4214  | total loss: \u001b[1m\u001b[32m0.55320\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4214 | loss: 0.55320 - acc: 0.7493 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4215  | total loss: \u001b[1m\u001b[32m0.54618\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4215 | loss: 0.54618 - acc: 0.7562 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4216  | total loss: \u001b[1m\u001b[32m0.53929\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4216 | loss: 0.53929 - acc: 0.7625 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4217  | total loss: \u001b[1m\u001b[32m0.53249\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4217 | loss: 0.53249 - acc: 0.7682 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4218  | total loss: \u001b[1m\u001b[32m0.55758\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4218 | loss: 0.55758 - acc: 0.7481 -- iter: 891/891\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 4219  | total loss: \u001b[1m\u001b[32m0.54816\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4219 | loss: 0.54816 - acc: 0.7549 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4220  | total loss: \u001b[1m\u001b[32m0.57456\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4220 | loss: 0.57456 - acc: 0.7335 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4221  | total loss: \u001b[1m\u001b[32m0.56337\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4221 | loss: 0.56337 - acc: 0.7414 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4222  | total loss: \u001b[1m\u001b[32m0.58668\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4222 | loss: 0.58668 - acc: 0.7223 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4223  | total loss: \u001b[1m\u001b[32m0.57467\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4223 | loss: 0.57467 - acc: 0.7314 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4224  | total loss: \u001b[1m\u001b[32m0.56415\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4224 | loss: 0.56415 - acc: 0.7398 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4225  | total loss: \u001b[1m\u001b[32m0.55486\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4225 | loss: 0.55486 - acc: 0.7474 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4226  | total loss: \u001b[1m\u001b[32m0.54649\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4226 | loss: 0.54649 - acc: 0.7541 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4227  | total loss: \u001b[1m\u001b[32m0.53871\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4227 | loss: 0.53871 - acc: 0.7602 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4228  | total loss: \u001b[1m\u001b[32m0.53131\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4228 | loss: 0.53131 - acc: 0.7658 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4229  | total loss: \u001b[1m\u001b[32m0.52420\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4229 | loss: 0.52420 - acc: 0.7704 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4230  | total loss: \u001b[1m\u001b[32m0.51733\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4230 | loss: 0.51733 - acc: 0.7744 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4231  | total loss: \u001b[1m\u001b[32m0.51071\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4231 | loss: 0.51071 - acc: 0.7782 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4232  | total loss: \u001b[1m\u001b[32m0.54352\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4232 | loss: 0.54352 - acc: 0.7561 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4233  | total loss: \u001b[1m\u001b[32m0.53393\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4233 | loss: 0.53393 - acc: 0.7616 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4234  | total loss: \u001b[1m\u001b[32m0.52526\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4234 | loss: 0.52526 - acc: 0.7664 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4235  | total loss: \u001b[1m\u001b[32m0.51739\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4235 | loss: 0.51739 - acc: 0.7706 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4236  | total loss: \u001b[1m\u001b[32m0.51023\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4236 | loss: 0.51023 - acc: 0.7743 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4237  | total loss: \u001b[1m\u001b[32m0.50370\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4237 | loss: 0.50370 - acc: 0.7778 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4238  | total loss: \u001b[1m\u001b[32m0.49775\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4238 | loss: 0.49775 - acc: 0.7809 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4239  | total loss: \u001b[1m\u001b[32m0.49231\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4239 | loss: 0.49231 - acc: 0.7833 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4240  | total loss: \u001b[1m\u001b[32m0.48735\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4240 | loss: 0.48735 - acc: 0.7856 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4241  | total loss: \u001b[1m\u001b[32m0.48284\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4241 | loss: 0.48284 - acc: 0.7875 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4242  | total loss: \u001b[1m\u001b[32m0.52980\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4242 | loss: 0.52980 - acc: 0.7605 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4243  | total loss: \u001b[1m\u001b[32m0.52105\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4243 | loss: 0.52105 - acc: 0.7649 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4244  | total loss: \u001b[1m\u001b[32m0.51329\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4244 | loss: 0.51329 - acc: 0.7687 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4245  | total loss: \u001b[1m\u001b[32m0.50641\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4245 | loss: 0.50641 - acc: 0.7720 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4246  | total loss: \u001b[1m\u001b[32m0.54166\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4246 | loss: 0.54166 - acc: 0.7517 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4247  | total loss: \u001b[1m\u001b[32m0.53258\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4247 | loss: 0.53258 - acc: 0.7568 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4248  | total loss: \u001b[1m\u001b[32m0.52485\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4248 | loss: 0.52485 - acc: 0.7617 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4249  | total loss: \u001b[1m\u001b[32m0.51813\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4249 | loss: 0.51813 - acc: 0.7659 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4250  | total loss: \u001b[1m\u001b[32m0.55029\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4250 | loss: 0.55029 - acc: 0.7426 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4251  | total loss: \u001b[1m\u001b[32m0.54150\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4251 | loss: 0.54150 - acc: 0.7485 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4252  | total loss: \u001b[1m\u001b[32m0.53382\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4252 | loss: 0.53382 - acc: 0.7545 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4253  | total loss: \u001b[1m\u001b[32m0.52692\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4253 | loss: 0.52692 - acc: 0.7600 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4254  | total loss: \u001b[1m\u001b[32m0.55579\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 4254 | loss: 0.55579 - acc: 0.7375 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4255  | total loss: \u001b[1m\u001b[32m0.54689\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4255 | loss: 0.54689 - acc: 0.7447 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4256  | total loss: \u001b[1m\u001b[32m0.57225\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4256 | loss: 0.57225 - acc: 0.7235 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4257  | total loss: \u001b[1m\u001b[32m0.56236\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4257 | loss: 0.56236 - acc: 0.7320 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4258  | total loss: \u001b[1m\u001b[32m0.58442\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4258 | loss: 0.58442 - acc: 0.7123 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4259  | total loss: \u001b[1m\u001b[32m0.57435\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 4259 | loss: 0.57435 - acc: 0.7224 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4260  | total loss: \u001b[1m\u001b[32m0.59222\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4260 | loss: 0.59222 - acc: 0.7045 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4261  | total loss: \u001b[1m\u001b[32m0.58245\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4261 | loss: 0.58245 - acc: 0.7152 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4262  | total loss: \u001b[1m\u001b[32m0.59536\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4262 | loss: 0.59536 - acc: 0.7005 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4263  | total loss: \u001b[1m\u001b[32m0.58617\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4263 | loss: 0.58617 - acc: 0.7113 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4264  | total loss: \u001b[1m\u001b[32m0.59760\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4264 | loss: 0.59760 - acc: 0.6972 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4265  | total loss: \u001b[1m\u001b[32m0.58890\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4265 | loss: 0.58890 - acc: 0.7078 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4266  | total loss: \u001b[1m\u001b[32m0.60159\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4266 | loss: 0.60159 - acc: 0.6941 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4267  | total loss: \u001b[1m\u001b[32m0.59294\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4267 | loss: 0.59294 - acc: 0.7054 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4268  | total loss: \u001b[1m\u001b[32m0.60471\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4268 | loss: 0.60471 - acc: 0.6899 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4269  | total loss: \u001b[1m\u001b[32m0.59603\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4269 | loss: 0.59603 - acc: 0.7020 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4270  | total loss: \u001b[1m\u001b[32m0.60754\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4270 | loss: 0.60754 - acc: 0.6879 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4271  | total loss: \u001b[1m\u001b[32m0.59867\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4271 | loss: 0.59867 - acc: 0.7001 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4272  | total loss: \u001b[1m\u001b[32m0.61086\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4272 | loss: 0.61086 - acc: 0.6856 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4273  | total loss: \u001b[1m\u001b[32m0.60162\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4273 | loss: 0.60162 - acc: 0.6976 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4274  | total loss: \u001b[1m\u001b[32m0.61302\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4274 | loss: 0.61302 - acc: 0.6847 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4275  | total loss: \u001b[1m\u001b[32m0.60354\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4275 | loss: 0.60354 - acc: 0.6963 -- iter: 891/891\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 4276  | total loss: \u001b[1m\u001b[32m0.59490\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4276 | loss: 0.59490 - acc: 0.7068 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4277  | total loss: \u001b[1m\u001b[32m0.58682\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4277 | loss: 0.58682 - acc: 0.7165 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4278  | total loss: \u001b[1m\u001b[32m0.57908\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4278 | loss: 0.57908 - acc: 0.7250 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4279  | total loss: \u001b[1m\u001b[32m0.57154\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4279 | loss: 0.57154 - acc: 0.7324 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4280  | total loss: \u001b[1m\u001b[32m0.56410\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4280 | loss: 0.56410 - acc: 0.7398 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4281  | total loss: \u001b[1m\u001b[32m0.55672\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4281 | loss: 0.55672 - acc: 0.7467 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4282  | total loss: \u001b[1m\u001b[32m0.57796\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4282 | loss: 0.57796 - acc: 0.7274 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4283  | total loss: \u001b[1m\u001b[32m0.56820\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4283 | loss: 0.56820 - acc: 0.7353 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4284  | total loss: \u001b[1m\u001b[32m0.59257\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4284 | loss: 0.59257 - acc: 0.7147 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4285  | total loss: \u001b[1m\u001b[32m0.58098\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4285 | loss: 0.58098 - acc: 0.7239 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4286  | total loss: \u001b[1m\u001b[32m0.57045\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4286 | loss: 0.57045 - acc: 0.7322 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4287  | total loss: \u001b[1m\u001b[32m0.56076\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4287 | loss: 0.56076 - acc: 0.7401 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4288  | total loss: \u001b[1m\u001b[32m0.55177\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4288 | loss: 0.55177 - acc: 0.7471 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4289  | total loss: \u001b[1m\u001b[32m0.54333\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4289 | loss: 0.54333 - acc: 0.7534 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4290  | total loss: \u001b[1m\u001b[32m0.56858\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4290 | loss: 0.56858 - acc: 0.7341 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4291  | total loss: \u001b[1m\u001b[32m0.55805\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4291 | loss: 0.55805 - acc: 0.7421 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4292  | total loss: \u001b[1m\u001b[32m0.57868\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4292 | loss: 0.57868 - acc: 0.7242 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4293  | total loss: \u001b[1m\u001b[32m0.56731\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4293 | loss: 0.56731 - acc: 0.7334 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4294  | total loss: \u001b[1m\u001b[32m0.59202\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4294 | loss: 0.59202 - acc: 0.7136 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4295  | total loss: \u001b[1m\u001b[32m0.58010\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4295 | loss: 0.58010 - acc: 0.7240 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4296  | total loss: \u001b[1m\u001b[32m0.56989\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 4296 | loss: 0.56989 - acc: 0.7331 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4297  | total loss: \u001b[1m\u001b[32m0.56097\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 4297 | loss: 0.56097 - acc: 0.7412 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4298  | total loss: \u001b[1m\u001b[32m0.58153\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 4298 | loss: 0.58153 - acc: 0.7221 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4299  | total loss: \u001b[1m\u001b[32m0.57176\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 4299 | loss: 0.57176 - acc: 0.7314 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4300  | total loss: \u001b[1m\u001b[32m0.56302\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4300 | loss: 0.56302 - acc: 0.7398 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4301  | total loss: \u001b[1m\u001b[32m0.55489\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4301 | loss: 0.55489 - acc: 0.7471 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4302  | total loss: \u001b[1m\u001b[32m0.57723\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4302 | loss: 0.57723 - acc: 0.7277 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4303  | total loss: \u001b[1m\u001b[32m0.56712\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4303 | loss: 0.56712 - acc: 0.7362 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4304  | total loss: \u001b[1m\u001b[32m0.58807\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4304 | loss: 0.58807 - acc: 0.7170 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4305  | total loss: \u001b[1m\u001b[32m0.57686\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4305 | loss: 0.57686 - acc: 0.7265 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4306  | total loss: \u001b[1m\u001b[32m0.59885\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4306 | loss: 0.59885 - acc: 0.7071 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4307  | total loss: \u001b[1m\u001b[32m0.58694\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4307 | loss: 0.58694 - acc: 0.7176 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4308  | total loss: \u001b[1m\u001b[32m0.57642\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4308 | loss: 0.57642 - acc: 0.7268 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4309  | total loss: \u001b[1m\u001b[32m0.56694\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4309 | loss: 0.56694 - acc: 0.7352 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4310  | total loss: \u001b[1m\u001b[32m0.55819\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4310 | loss: 0.55819 - acc: 0.7429 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4311  | total loss: \u001b[1m\u001b[32m0.54996\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4311 | loss: 0.54996 - acc: 0.7497 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4312  | total loss: \u001b[1m\u001b[32m0.57496\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4312 | loss: 0.57496 - acc: 0.7295 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4313  | total loss: \u001b[1m\u001b[32m0.56464\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4313 | loss: 0.56464 - acc: 0.7381 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4314  | total loss: \u001b[1m\u001b[32m0.55524\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4314 | loss: 0.55524 - acc: 0.7457 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4315  | total loss: \u001b[1m\u001b[32m0.54657\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4315 | loss: 0.54657 - acc: 0.7527 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4316  | total loss: \u001b[1m\u001b[32m0.56834\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4316 | loss: 0.56834 - acc: 0.7370 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4317  | total loss: \u001b[1m\u001b[32m0.55812\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4317 | loss: 0.55812 - acc: 0.7446 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4318  | total loss: \u001b[1m\u001b[32m0.58737\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4318 | loss: 0.58737 - acc: 0.7225 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4319  | total loss: \u001b[1m\u001b[32m0.57546\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4319 | loss: 0.57546 - acc: 0.7314 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4320  | total loss: \u001b[1m\u001b[32m0.56494\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4320 | loss: 0.56494 - acc: 0.7395 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4321  | total loss: \u001b[1m\u001b[32m0.55554\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4321 | loss: 0.55554 - acc: 0.7466 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4322  | total loss: \u001b[1m\u001b[32m0.57817\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4322 | loss: 0.57817 - acc: 0.7286 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4323  | total loss: \u001b[1m\u001b[32m0.56768\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4323 | loss: 0.56768 - acc: 0.7368 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4324  | total loss: \u001b[1m\u001b[32m0.55838\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4324 | loss: 0.55838 - acc: 0.7444 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4325  | total loss: \u001b[1m\u001b[32m0.54996\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4325 | loss: 0.54996 - acc: 0.7512 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4326  | total loss: \u001b[1m\u001b[32m0.57504\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4326 | loss: 0.57504 - acc: 0.7285 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4327  | total loss: \u001b[1m\u001b[32m0.56499\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4327 | loss: 0.56499 - acc: 0.7367 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4328  | total loss: \u001b[1m\u001b[32m0.55599\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4328 | loss: 0.55599 - acc: 0.7438 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4329  | total loss: \u001b[1m\u001b[32m0.54775\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4329 | loss: 0.54775 - acc: 0.7504 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4330  | total loss: \u001b[1m\u001b[32m0.57112\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4330 | loss: 0.57112 - acc: 0.7289 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4331  | total loss: \u001b[1m\u001b[32m0.56119\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4331 | loss: 0.56119 - acc: 0.7371 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4332  | total loss: \u001b[1m\u001b[32m0.55220\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4332 | loss: 0.55220 - acc: 0.7447 -- iter: 891/891\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 4333  | total loss: \u001b[1m\u001b[32m0.54388\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4333 | loss: 0.54388 - acc: 0.7513 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4334  | total loss: \u001b[1m\u001b[32m0.56926\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4334 | loss: 0.56926 - acc: 0.7305 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4335  | total loss: \u001b[1m\u001b[32m0.55902\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4335 | loss: 0.55902 - acc: 0.7385 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4336  | total loss: \u001b[1m\u001b[32m0.58030\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4336 | loss: 0.58030 - acc: 0.7198 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4337  | total loss: \u001b[1m\u001b[32m0.56915\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4337 | loss: 0.56915 - acc: 0.7288 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4338  | total loss: \u001b[1m\u001b[32m0.58868\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4338 | loss: 0.58868 - acc: 0.7115 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4339  | total loss: \u001b[1m\u001b[32m0.57710\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4339 | loss: 0.57710 - acc: 0.7214 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4340  | total loss: \u001b[1m\u001b[32m0.59450\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4340 | loss: 0.59450 - acc: 0.7052 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4341  | total loss: \u001b[1m\u001b[32m0.58310\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4341 | loss: 0.58310 - acc: 0.7160 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4342  | total loss: \u001b[1m\u001b[32m0.57312\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4342 | loss: 0.57312 - acc: 0.7256 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4343  | total loss: \u001b[1m\u001b[32m0.56418\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4343 | loss: 0.56418 - acc: 0.7343 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4344  | total loss: \u001b[1m\u001b[32m0.55596\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4344 | loss: 0.55596 - acc: 0.7418 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4345  | total loss: \u001b[1m\u001b[32m0.54825\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4345 | loss: 0.54825 - acc: 0.7486 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4346  | total loss: \u001b[1m\u001b[32m0.54088\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4346 | loss: 0.54088 - acc: 0.7548 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4347  | total loss: \u001b[1m\u001b[32m0.53375\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4347 | loss: 0.53375 - acc: 0.7606 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4348  | total loss: \u001b[1m\u001b[32m0.52686\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4348 | loss: 0.52686 - acc: 0.7663 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4349  | total loss: \u001b[1m\u001b[32m0.52025\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4349 | loss: 0.52025 - acc: 0.7711 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4350  | total loss: \u001b[1m\u001b[32m0.51395\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4350 | loss: 0.51395 - acc: 0.7753 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4351  | total loss: \u001b[1m\u001b[32m0.50803\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4351 | loss: 0.50803 - acc: 0.7791 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4352  | total loss: \u001b[1m\u001b[32m0.54682\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4352 | loss: 0.54682 - acc: 0.7561 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4353  | total loss: \u001b[1m\u001b[32m0.53739\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4353 | loss: 0.53739 - acc: 0.7609 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4354  | total loss: \u001b[1m\u001b[32m0.57128\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4354 | loss: 0.57128 - acc: 0.7400 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4355  | total loss: \u001b[1m\u001b[32m0.55927\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4355 | loss: 0.55927 - acc: 0.7461 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4356  | total loss: \u001b[1m\u001b[32m0.54837\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4356 | loss: 0.54837 - acc: 0.7526 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4357  | total loss: \u001b[1m\u001b[32m0.53858\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4357 | loss: 0.53858 - acc: 0.7587 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4358  | total loss: \u001b[1m\u001b[32m0.52983\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4358 | loss: 0.52983 - acc: 0.7641 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4359  | total loss: \u001b[1m\u001b[32m0.52192\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4359 | loss: 0.52192 - acc: 0.7690 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4360  | total loss: \u001b[1m\u001b[32m0.51471\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4360 | loss: 0.51471 - acc: 0.7730 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4361  | total loss: \u001b[1m\u001b[32m0.50819\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4361 | loss: 0.50819 - acc: 0.7758 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4362  | total loss: \u001b[1m\u001b[32m0.54472\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4362 | loss: 0.54472 - acc: 0.7517 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4363  | total loss: \u001b[1m\u001b[32m0.53526\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4363 | loss: 0.53526 - acc: 0.7566 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4364  | total loss: \u001b[1m\u001b[32m0.52687\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4364 | loss: 0.52687 - acc: 0.7620 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4365  | total loss: \u001b[1m\u001b[32m0.51936\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4365 | loss: 0.51936 - acc: 0.7669 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4366  | total loss: \u001b[1m\u001b[32m0.55576\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4366 | loss: 0.55576 - acc: 0.7427 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4367  | total loss: \u001b[1m\u001b[32m0.54560\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4367 | loss: 0.54560 - acc: 0.7492 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4368  | total loss: \u001b[1m\u001b[32m0.53669\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4368 | loss: 0.53669 - acc: 0.7545 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4369  | total loss: \u001b[1m\u001b[32m0.52872\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4369 | loss: 0.52872 - acc: 0.7592 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4370  | total loss: \u001b[1m\u001b[32m0.52141\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4370 | loss: 0.52141 - acc: 0.7634 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4371  | total loss: \u001b[1m\u001b[32m0.51463\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4371 | loss: 0.51463 - acc: 0.7679 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4372  | total loss: \u001b[1m\u001b[32m0.50831\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4372 | loss: 0.50831 - acc: 0.7721 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4373  | total loss: \u001b[1m\u001b[32m0.50241\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4373 | loss: 0.50241 - acc: 0.7761 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4374  | total loss: \u001b[1m\u001b[32m0.53928\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4374 | loss: 0.53928 - acc: 0.7533 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4375  | total loss: \u001b[1m\u001b[32m0.53024\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4375 | loss: 0.53024 - acc: 0.7587 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4376  | total loss: \u001b[1m\u001b[32m0.56455\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4376 | loss: 0.56455 - acc: 0.7343 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4377  | total loss: \u001b[1m\u001b[32m0.55336\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4377 | loss: 0.55336 - acc: 0.7412 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4378  | total loss: \u001b[1m\u001b[32m0.54349\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4378 | loss: 0.54349 - acc: 0.7480 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4379  | total loss: \u001b[1m\u001b[32m0.53479\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 4379 | loss: 0.53479 - acc: 0.7542 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4380  | total loss: \u001b[1m\u001b[32m0.56250\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4380 | loss: 0.56250 - acc: 0.7321 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4381  | total loss: \u001b[1m\u001b[32m0.55249\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4381 | loss: 0.55249 - acc: 0.7400 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4382  | total loss: \u001b[1m\u001b[32m0.54383\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4382 | loss: 0.54383 - acc: 0.7472 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4383  | total loss: \u001b[1m\u001b[32m0.53626\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4383 | loss: 0.53626 - acc: 0.7535 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4384  | total loss: \u001b[1m\u001b[32m0.56152\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4384 | loss: 0.56152 - acc: 0.7322 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4385  | total loss: \u001b[1m\u001b[32m0.55254\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4385 | loss: 0.55254 - acc: 0.7405 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4386  | total loss: \u001b[1m\u001b[32m0.57678\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4386 | loss: 0.57678 - acc: 0.7182 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4387  | total loss: \u001b[1m\u001b[32m0.56695\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4387 | loss: 0.56695 - acc: 0.7273 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4388  | total loss: \u001b[1m\u001b[32m0.58644\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 4388 | loss: 0.58644 - acc: 0.7094 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4389  | total loss: \u001b[1m\u001b[32m0.57659\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4389 | loss: 0.57659 - acc: 0.7193 -- iter: 891/891\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 4390  | total loss: \u001b[1m\u001b[32m0.59410\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4390 | loss: 0.59410 - acc: 0.7019 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4391  | total loss: \u001b[1m\u001b[32m0.58432\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4391 | loss: 0.58432 - acc: 0.7125 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4392  | total loss: \u001b[1m\u001b[32m0.59925\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4392 | loss: 0.59925 - acc: 0.6965 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4393  | total loss: \u001b[1m\u001b[32m0.58961\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4393 | loss: 0.58961 - acc: 0.7078 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4394  | total loss: \u001b[1m\u001b[32m0.58109\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 4394 | loss: 0.58109 - acc: 0.7180 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4395  | total loss: \u001b[1m\u001b[32m0.57329\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4395 | loss: 0.57329 - acc: 0.7273 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4396  | total loss: \u001b[1m\u001b[32m0.59042\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4396 | loss: 0.59042 - acc: 0.7072 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4397  | total loss: \u001b[1m\u001b[32m0.58131\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4397 | loss: 0.58131 - acc: 0.7176 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4398  | total loss: \u001b[1m\u001b[32m0.59881\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 4398 | loss: 0.59881 - acc: 0.6981 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4399  | total loss: \u001b[1m\u001b[32m0.58871\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4399 | loss: 0.58871 - acc: 0.7096 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4400  | total loss: \u001b[1m\u001b[32m0.57950\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4400 | loss: 0.57950 - acc: 0.7201 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4401  | total loss: \u001b[1m\u001b[32m0.57094\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4401 | loss: 0.57094 - acc: 0.7296 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4402  | total loss: \u001b[1m\u001b[32m0.58805\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4402 | loss: 0.58805 - acc: 0.7127 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4403  | total loss: \u001b[1m\u001b[32m0.57817\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4403 | loss: 0.57817 - acc: 0.7219 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4404  | total loss: \u001b[1m\u001b[32m0.56909\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4404 | loss: 0.56909 - acc: 0.7305 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4405  | total loss: \u001b[1m\u001b[32m0.56062\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4405 | loss: 0.56062 - acc: 0.7383 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4406  | total loss: \u001b[1m\u001b[32m0.55264\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 4406 | loss: 0.55264 - acc: 0.7453 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4407  | total loss: \u001b[1m\u001b[32m0.54507\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4407 | loss: 0.54507 - acc: 0.7513 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4408  | total loss: \u001b[1m\u001b[32m0.53786\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4408 | loss: 0.53786 - acc: 0.7569 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4409  | total loss: \u001b[1m\u001b[32m0.53098\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4409 | loss: 0.53098 - acc: 0.7619 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4410  | total loss: \u001b[1m\u001b[32m0.52443\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 4410 | loss: 0.52443 - acc: 0.7671 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4411  | total loss: \u001b[1m\u001b[32m0.51823\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4411 | loss: 0.51823 - acc: 0.7716 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4412  | total loss: \u001b[1m\u001b[32m0.55154\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4412 | loss: 0.55154 - acc: 0.7496 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4413  | total loss: \u001b[1m\u001b[32m0.54221\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 4413 | loss: 0.54221 - acc: 0.7562 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4414  | total loss: \u001b[1m\u001b[32m0.57687\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 4414 | loss: 0.57687 - acc: 0.7332 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4415  | total loss: \u001b[1m\u001b[32m0.56478\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 4415 | loss: 0.56478 - acc: 0.7408 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4416  | total loss: \u001b[1m\u001b[32m0.59545\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4416 | loss: 0.59545 - acc: 0.7181 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4417  | total loss: \u001b[1m\u001b[32m0.58180\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4417 | loss: 0.58180 - acc: 0.7283 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4418  | total loss: \u001b[1m\u001b[32m0.56996\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 4418 | loss: 0.56996 - acc: 0.7371 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4419  | total loss: \u001b[1m\u001b[32m0.55972\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 4419 | loss: 0.55972 - acc: 0.7450 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4420  | total loss: \u001b[1m\u001b[32m0.58036\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 4420 | loss: 0.58036 - acc: 0.7258 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4421  | total loss: \u001b[1m\u001b[32m0.57009\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 4421 | loss: 0.57009 - acc: 0.7346 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4422  | total loss: \u001b[1m\u001b[32m0.59110\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 4422 | loss: 0.59110 - acc: 0.7152 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4423  | total loss: \u001b[1m\u001b[32m0.58091\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 4423 | loss: 0.58091 - acc: 0.7243 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4424  | total loss: \u001b[1m\u001b[32m0.59652\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 4424 | loss: 0.59652 - acc: 0.7087 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4425  | total loss: \u001b[1m\u001b[32m0.58665\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 4425 | loss: 0.58665 - acc: 0.7185 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4426  | total loss: \u001b[1m\u001b[32m0.57789\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 4426 | loss: 0.57789 - acc: 0.7279 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4427  | total loss: \u001b[1m\u001b[32m0.56980\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 4427 | loss: 0.56980 - acc: 0.7362 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4428  | total loss: \u001b[1m\u001b[32m0.56205\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 4428 | loss: 0.56205 - acc: 0.7438 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4429  | total loss: \u001b[1m\u001b[32m0.55444\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 4429 | loss: 0.55444 - acc: 0.7507 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4430  | total loss: \u001b[1m\u001b[32m0.54689\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 4430 | loss: 0.54689 - acc: 0.7570 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4431  | total loss: \u001b[1m\u001b[32m0.53946\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 4431 | loss: 0.53946 - acc: 0.7625 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4432  | total loss: \u001b[1m\u001b[32m0.53223\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 4432 | loss: 0.53223 - acc: 0.7678 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4433  | total loss: \u001b[1m\u001b[32m0.52532\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 4433 | loss: 0.52532 - acc: 0.7722 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4434  | total loss: \u001b[1m\u001b[32m0.51884\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 4434 | loss: 0.51884 - acc: 0.7761 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4435  | total loss: \u001b[1m\u001b[32m0.51288\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4435 | loss: 0.51288 - acc: 0.7795 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4436  | total loss: \u001b[1m\u001b[32m0.50745\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4436 | loss: 0.50745 - acc: 0.7826 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4437  | total loss: \u001b[1m\u001b[32m0.50256\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4437 | loss: 0.50256 - acc: 0.7847 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4438  | total loss: \u001b[1m\u001b[32m0.49817\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4438 | loss: 0.49817 - acc: 0.7867 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4439  | total loss: \u001b[1m\u001b[32m0.49418\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4439 | loss: 0.49418 - acc: 0.7885 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4440  | total loss: \u001b[1m\u001b[32m0.49051\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4440 | loss: 0.49051 - acc: 0.7900 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4441  | total loss: \u001b[1m\u001b[32m0.48704\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4441 | loss: 0.48704 - acc: 0.7916 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4442  | total loss: \u001b[1m\u001b[32m0.54337\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4442 | loss: 0.54337 - acc: 0.7621 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4443  | total loss: \u001b[1m\u001b[32m0.53394\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4443 | loss: 0.53394 - acc: 0.7663 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4444  | total loss: \u001b[1m\u001b[32m0.57831\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4444 | loss: 0.57831 - acc: 0.7430 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4445  | total loss: \u001b[1m\u001b[32m0.56494\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 4445 | loss: 0.56494 - acc: 0.7491 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4446  | total loss: \u001b[1m\u001b[32m0.59514\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4446 | loss: 0.59514 - acc: 0.7292 -- iter: 891/891\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 4447  | total loss: \u001b[1m\u001b[32m0.58144\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4447 | loss: 0.58144 - acc: 0.7380 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4448  | total loss: \u001b[1m\u001b[32m0.57033\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4448 | loss: 0.57033 - acc: 0.7450 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4449  | total loss: \u001b[1m\u001b[32m0.56110\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4449 | loss: 0.56110 - acc: 0.7508 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4450  | total loss: \u001b[1m\u001b[32m0.58403\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4450 | loss: 0.58403 - acc: 0.7319 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4451  | total loss: \u001b[1m\u001b[32m0.57432\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4451 | loss: 0.57432 - acc: 0.7391 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4452  | total loss: \u001b[1m\u001b[32m0.59439\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4452 | loss: 0.59439 - acc: 0.7198 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4453  | total loss: \u001b[1m\u001b[32m0.58409\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4453 | loss: 0.58409 - acc: 0.7284 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4454  | total loss: \u001b[1m\u001b[32m0.57477\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4454 | loss: 0.57477 - acc: 0.7368 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4455  | total loss: \u001b[1m\u001b[32m0.56597\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4455 | loss: 0.56597 - acc: 0.7442 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4456  | total loss: \u001b[1m\u001b[32m0.55749\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4456 | loss: 0.55749 - acc: 0.7506 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4457  | total loss: \u001b[1m\u001b[32m0.54929\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4457 | loss: 0.54929 - acc: 0.7569 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4458  | total loss: \u001b[1m\u001b[32m0.54140\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4458 | loss: 0.54140 - acc: 0.7625 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4459  | total loss: \u001b[1m\u001b[32m0.53392\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4459 | loss: 0.53392 - acc: 0.7675 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4460  | total loss: \u001b[1m\u001b[32m0.52700\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4460 | loss: 0.52700 - acc: 0.7714 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4461  | total loss: \u001b[1m\u001b[32m0.52069\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4461 | loss: 0.52069 - acc: 0.7745 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4462  | total loss: \u001b[1m\u001b[32m0.55656\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4462 | loss: 0.55656 - acc: 0.7523 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4463  | total loss: \u001b[1m\u001b[32m0.54722\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4463 | loss: 0.54722 - acc: 0.7578 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4464  | total loss: \u001b[1m\u001b[32m0.57801\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4464 | loss: 0.57801 - acc: 0.7371 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4465  | total loss: \u001b[1m\u001b[32m0.56641\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4465 | loss: 0.56641 - acc: 0.7445 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4466  | total loss: \u001b[1m\u001b[32m0.59407\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4466 | loss: 0.59407 - acc: 0.7234 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4467  | total loss: \u001b[1m\u001b[32m0.58119\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4467 | loss: 0.58119 - acc: 0.7322 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4468  | total loss: \u001b[1m\u001b[32m0.57005\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4468 | loss: 0.57005 - acc: 0.7403 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4469  | total loss: \u001b[1m\u001b[32m0.56047\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4469 | loss: 0.56047 - acc: 0.7476 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4470  | total loss: \u001b[1m\u001b[32m0.58209\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4470 | loss: 0.58209 - acc: 0.7263 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4471  | total loss: \u001b[1m\u001b[32m0.57228\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4471 | loss: 0.57228 - acc: 0.7350 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4472  | total loss: \u001b[1m\u001b[32m0.59098\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4472 | loss: 0.59098 - acc: 0.7150 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4473  | total loss: \u001b[1m\u001b[32m0.58146\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4473 | loss: 0.58146 - acc: 0.7243 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4474  | total loss: \u001b[1m\u001b[32m0.59751\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4474 | loss: 0.59751 - acc: 0.7062 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4475  | total loss: \u001b[1m\u001b[32m0.58836\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4475 | loss: 0.58836 - acc: 0.7162 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4476  | total loss: \u001b[1m\u001b[32m0.58024\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4476 | loss: 0.58024 - acc: 0.7251 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4477  | total loss: \u001b[1m\u001b[32m0.57261\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4477 | loss: 0.57261 - acc: 0.7333 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4478  | total loss: \u001b[1m\u001b[32m0.58926\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4478 | loss: 0.58926 - acc: 0.7143 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4479  | total loss: \u001b[1m\u001b[32m0.57988\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4479 | loss: 0.57988 - acc: 0.7238 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4480  | total loss: \u001b[1m\u001b[32m0.57100\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4480 | loss: 0.57100 - acc: 0.7330 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4481  | total loss: \u001b[1m\u001b[32m0.56247\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4481 | loss: 0.56247 - acc: 0.7413 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4482  | total loss: \u001b[1m\u001b[32m0.58239\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4482 | loss: 0.58239 - acc: 0.7234 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4483  | total loss: \u001b[1m\u001b[32m0.57209\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4483 | loss: 0.57209 - acc: 0.7324 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4484  | total loss: \u001b[1m\u001b[32m0.56263\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4484 | loss: 0.56263 - acc: 0.7402 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4485  | total loss: \u001b[1m\u001b[32m0.55385\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4485 | loss: 0.55385 - acc: 0.7470 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4486  | total loss: \u001b[1m\u001b[32m0.54565\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4486 | loss: 0.54565 - acc: 0.7527 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4487  | total loss: \u001b[1m\u001b[32m0.53799\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4487 | loss: 0.53799 - acc: 0.7578 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4488  | total loss: \u001b[1m\u001b[32m0.57007\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4488 | loss: 0.57007 - acc: 0.7356 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4489  | total loss: \u001b[1m\u001b[32m0.55959\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4489 | loss: 0.55959 - acc: 0.7425 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4490  | total loss: \u001b[1m\u001b[32m0.58442\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4490 | loss: 0.58442 - acc: 0.7251 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4491  | total loss: \u001b[1m\u001b[32m0.57252\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4491 | loss: 0.57252 - acc: 0.7340 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4492  | total loss: \u001b[1m\u001b[32m0.59813\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4492 | loss: 0.59813 - acc: 0.7137 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4493  | total loss: \u001b[1m\u001b[32m0.58567\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4493 | loss: 0.58567 - acc: 0.7237 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4494  | total loss: \u001b[1m\u001b[32m0.57505\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 4494 | loss: 0.57505 - acc: 0.7325 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4495  | total loss: \u001b[1m\u001b[32m0.56592\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4495 | loss: 0.56592 - acc: 0.7405 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4496  | total loss: \u001b[1m\u001b[32m0.55787\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4496 | loss: 0.55787 - acc: 0.7478 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4497  | total loss: \u001b[1m\u001b[32m0.55050\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4497 | loss: 0.55050 - acc: 0.7544 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4498  | total loss: \u001b[1m\u001b[32m0.54351\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4498 | loss: 0.54351 - acc: 0.7605 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4499  | total loss: \u001b[1m\u001b[32m0.53667\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4499 | loss: 0.53667 - acc: 0.7657 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4500  | total loss: \u001b[1m\u001b[32m0.52990\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4500 | loss: 0.52990 - acc: 0.7705 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4501  | total loss: \u001b[1m\u001b[32m0.52318\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4501 | loss: 0.52318 - acc: 0.7749 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4502  | total loss: \u001b[1m\u001b[32m0.55034\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4502 | loss: 0.55034 - acc: 0.7536 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4503  | total loss: \u001b[1m\u001b[32m0.54088\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4503 | loss: 0.54088 - acc: 0.7599 -- iter: 891/891\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 4504  | total loss: \u001b[1m\u001b[32m0.57338\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4504 | loss: 0.57338 - acc: 0.7381 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4505  | total loss: \u001b[1m\u001b[32m0.56156\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4505 | loss: 0.56156 - acc: 0.7460 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4506  | total loss: \u001b[1m\u001b[32m0.58644\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4506 | loss: 0.58644 - acc: 0.7281 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4507  | total loss: \u001b[1m\u001b[32m0.57373\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4507 | loss: 0.57373 - acc: 0.7364 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4508  | total loss: \u001b[1m\u001b[32m0.59739\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4508 | loss: 0.59739 - acc: 0.7170 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4509  | total loss: \u001b[1m\u001b[32m0.58463\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4509 | loss: 0.58463 - acc: 0.7265 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4510  | total loss: \u001b[1m\u001b[32m0.57374\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4510 | loss: 0.57374 - acc: 0.7351 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4511  | total loss: \u001b[1m\u001b[32m0.56427\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4511 | loss: 0.56427 - acc: 0.7428 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4512  | total loss: \u001b[1m\u001b[32m0.58292\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4512 | loss: 0.58292 - acc: 0.7252 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4513  | total loss: \u001b[1m\u001b[32m0.57301\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4513 | loss: 0.57301 - acc: 0.7336 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4514  | total loss: \u001b[1m\u001b[32m0.59343\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4514 | loss: 0.59343 - acc: 0.7126 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4515  | total loss: \u001b[1m\u001b[32m0.58303\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4515 | loss: 0.58303 - acc: 0.7221 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4516  | total loss: \u001b[1m\u001b[32m0.57377\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4516 | loss: 0.57377 - acc: 0.7309 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4517  | total loss: \u001b[1m\u001b[32m0.56524\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4517 | loss: 0.56524 - acc: 0.7387 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4518  | total loss: \u001b[1m\u001b[32m0.58367\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4518 | loss: 0.58367 - acc: 0.7200 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4519  | total loss: \u001b[1m\u001b[32m0.57378\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4519 | loss: 0.57378 - acc: 0.7292 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4520  | total loss: \u001b[1m\u001b[32m0.56466\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4520 | loss: 0.56466 - acc: 0.7374 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4521  | total loss: \u001b[1m\u001b[32m0.55610\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4521 | loss: 0.55610 - acc: 0.7446 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4522  | total loss: \u001b[1m\u001b[32m0.54799\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4522 | loss: 0.54799 - acc: 0.7514 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4523  | total loss: \u001b[1m\u001b[32m0.54027\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4523 | loss: 0.54027 - acc: 0.7579 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4524  | total loss: \u001b[1m\u001b[32m0.53292\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4524 | loss: 0.53292 - acc: 0.7634 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4525  | total loss: \u001b[1m\u001b[32m0.52595\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4525 | loss: 0.52595 - acc: 0.7685 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4526  | total loss: \u001b[1m\u001b[32m0.55660\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4526 | loss: 0.55660 - acc: 0.7459 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4527  | total loss: \u001b[1m\u001b[32m0.54691\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4527 | loss: 0.54691 - acc: 0.7526 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4528  | total loss: \u001b[1m\u001b[32m0.53809\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4528 | loss: 0.53809 - acc: 0.7587 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4529  | total loss: \u001b[1m\u001b[32m0.53001\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4529 | loss: 0.53001 - acc: 0.7641 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4530  | total loss: \u001b[1m\u001b[32m0.56832\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4530 | loss: 0.56832 - acc: 0.7377 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4531  | total loss: \u001b[1m\u001b[32m0.55704\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4531 | loss: 0.55704 - acc: 0.7451 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4532  | total loss: \u001b[1m\u001b[32m0.54685\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4532 | loss: 0.54685 - acc: 0.7521 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4533  | total loss: \u001b[1m\u001b[32m0.53763\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4533 | loss: 0.53763 - acc: 0.7582 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4534  | total loss: \u001b[1m\u001b[32m0.57088\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4534 | loss: 0.57088 - acc: 0.7343 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4535  | total loss: \u001b[1m\u001b[32m0.55945\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4535 | loss: 0.55945 - acc: 0.7420 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4536  | total loss: \u001b[1m\u001b[32m0.58250\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4536 | loss: 0.58250 - acc: 0.7223 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4537  | total loss: \u001b[1m\u001b[32m0.57087\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4537 | loss: 0.57087 - acc: 0.7311 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4538  | total loss: \u001b[1m\u001b[32m0.59470\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4538 | loss: 0.59470 - acc: 0.7114 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4539  | total loss: \u001b[1m\u001b[32m0.58341\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4539 | loss: 0.58341 - acc: 0.7209 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4540  | total loss: \u001b[1m\u001b[32m0.57391\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4540 | loss: 0.57391 - acc: 0.7290 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4541  | total loss: \u001b[1m\u001b[32m0.56562\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4541 | loss: 0.56562 - acc: 0.7364 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4542  | total loss: \u001b[1m\u001b[32m0.55806\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4542 | loss: 0.55806 - acc: 0.7431 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4543  | total loss: \u001b[1m\u001b[32m0.55085\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 4543 | loss: 0.55085 - acc: 0.7495 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4544  | total loss: \u001b[1m\u001b[32m0.54376\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 4544 | loss: 0.54376 - acc: 0.7556 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4545  | total loss: \u001b[1m\u001b[32m0.53670\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 4545 | loss: 0.53670 - acc: 0.7611 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4546  | total loss: \u001b[1m\u001b[32m0.56337\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4546 | loss: 0.56337 - acc: 0.7385 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4547  | total loss: \u001b[1m\u001b[32m0.55357\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4547 | loss: 0.55357 - acc: 0.7461 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4548  | total loss: \u001b[1m\u001b[32m0.57499\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4548 | loss: 0.57499 - acc: 0.7286 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4549  | total loss: \u001b[1m\u001b[32m0.56395\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4549 | loss: 0.56395 - acc: 0.7370 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4550  | total loss: \u001b[1m\u001b[32m0.58809\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4550 | loss: 0.58809 - acc: 0.7170 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4551  | total loss: \u001b[1m\u001b[32m0.57607\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4551 | loss: 0.57607 - acc: 0.7264 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4552  | total loss: \u001b[1m\u001b[32m0.59932\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4552 | loss: 0.59932 - acc: 0.7059 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4553  | total loss: \u001b[1m\u001b[32m0.58696\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4553 | loss: 0.58696 - acc: 0.7165 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4554  | total loss: \u001b[1m\u001b[32m0.60306\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4554 | loss: 0.60306 - acc: 0.7026 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4555  | total loss: \u001b[1m\u001b[32m0.59144\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4555 | loss: 0.59144 - acc: 0.7136 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4556  | total loss: \u001b[1m\u001b[32m0.58150\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4556 | loss: 0.58150 - acc: 0.7236 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4557  | total loss: \u001b[1m\u001b[32m0.57283\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4557 | loss: 0.57283 - acc: 0.7324 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4558  | total loss: \u001b[1m\u001b[32m0.56504\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4558 | loss: 0.56504 - acc: 0.7403 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4559  | total loss: \u001b[1m\u001b[32m0.55782\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4559 | loss: 0.55782 - acc: 0.7474 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4560  | total loss: \u001b[1m\u001b[32m0.55094\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4560 | loss: 0.55094 - acc: 0.7541 -- iter: 891/891\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 4561  | total loss: \u001b[1m\u001b[32m0.54424\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4561 | loss: 0.54424 - acc: 0.7600 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4562  | total loss: \u001b[1m\u001b[32m0.53762\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4562 | loss: 0.53762 - acc: 0.7652 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4563  | total loss: \u001b[1m\u001b[32m0.53107\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4563 | loss: 0.53107 - acc: 0.7700 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4564  | total loss: \u001b[1m\u001b[32m0.55696\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4564 | loss: 0.55696 - acc: 0.7476 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4565  | total loss: \u001b[1m\u001b[32m0.54773\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4565 | loss: 0.54773 - acc: 0.7542 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4566  | total loss: \u001b[1m\u001b[32m0.57490\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4566 | loss: 0.57490 - acc: 0.7312 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4567  | total loss: \u001b[1m\u001b[32m0.56372\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4567 | loss: 0.56372 - acc: 0.7391 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4568  | total loss: \u001b[1m\u001b[32m0.58424\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4568 | loss: 0.58424 - acc: 0.7237 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4569  | total loss: \u001b[1m\u001b[32m0.57237\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4569 | loss: 0.57237 - acc: 0.7328 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4570  | total loss: \u001b[1m\u001b[32m0.59006\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4570 | loss: 0.59006 - acc: 0.7170 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4571  | total loss: \u001b[1m\u001b[32m0.57823\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4571 | loss: 0.57823 - acc: 0.7265 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4572  | total loss: \u001b[1m\u001b[32m0.59948\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4572 | loss: 0.59948 - acc: 0.7046 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4573  | total loss: \u001b[1m\u001b[32m0.58776\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4573 | loss: 0.58776 - acc: 0.7154 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4574  | total loss: \u001b[1m\u001b[32m0.60375\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4574 | loss: 0.60375 - acc: 0.6992 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4575  | total loss: \u001b[1m\u001b[32m0.59278\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4575 | loss: 0.59278 - acc: 0.7102 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4576  | total loss: \u001b[1m\u001b[32m0.58331\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4576 | loss: 0.58331 - acc: 0.7202 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4577  | total loss: \u001b[1m\u001b[32m0.57487\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4577 | loss: 0.57487 - acc: 0.7293 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4578  | total loss: \u001b[1m\u001b[32m0.56709\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4578 | loss: 0.56709 - acc: 0.7374 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4579  | total loss: \u001b[1m\u001b[32m0.55969\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4579 | loss: 0.55969 - acc: 0.7450 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4580  | total loss: \u001b[1m\u001b[32m0.57741\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4580 | loss: 0.57741 - acc: 0.7265 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4581  | total loss: \u001b[1m\u001b[32m0.56823\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4581 | loss: 0.56823 - acc: 0.7350 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4582  | total loss: \u001b[1m\u001b[32m0.58862\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4582 | loss: 0.58862 - acc: 0.7141 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4583  | total loss: \u001b[1m\u001b[32m0.57803\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4583 | loss: 0.57803 - acc: 0.7241 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4584  | total loss: \u001b[1m\u001b[32m0.59754\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4584 | loss: 0.59754 - acc: 0.7053 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4585  | total loss: \u001b[1m\u001b[32m0.58616\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4585 | loss: 0.58616 - acc: 0.7160 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4586  | total loss: \u001b[1m\u001b[32m0.57591\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4586 | loss: 0.57591 - acc: 0.7256 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4587  | total loss: \u001b[1m\u001b[32m0.56654\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4587 | loss: 0.56654 - acc: 0.7347 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4588  | total loss: \u001b[1m\u001b[32m0.55784\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4588 | loss: 0.55784 - acc: 0.7426 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4589  | total loss: \u001b[1m\u001b[32m0.54968\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4589 | loss: 0.54968 - acc: 0.7496 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4590  | total loss: \u001b[1m\u001b[32m0.57383\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4590 | loss: 0.57383 - acc: 0.7284 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4591  | total loss: \u001b[1m\u001b[32m0.56360\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4591 | loss: 0.56360 - acc: 0.7374 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4592  | total loss: \u001b[1m\u001b[32m0.58490\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4592 | loss: 0.58490 - acc: 0.7182 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4593  | total loss: \u001b[1m\u001b[32m0.57360\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4593 | loss: 0.57360 - acc: 0.7275 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4594  | total loss: \u001b[1m\u001b[32m0.56347\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4594 | loss: 0.56347 - acc: 0.7360 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4595  | total loss: \u001b[1m\u001b[32m0.55426\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4595 | loss: 0.55426 - acc: 0.7438 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4596  | total loss: \u001b[1m\u001b[32m0.54579\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4596 | loss: 0.54579 - acc: 0.7506 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4597  | total loss: \u001b[1m\u001b[32m0.53792\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4597 | loss: 0.53792 - acc: 0.7571 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4598  | total loss: \u001b[1m\u001b[32m0.53054\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4598 | loss: 0.53054 - acc: 0.7631 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4599  | total loss: \u001b[1m\u001b[32m0.52355\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4599 | loss: 0.52355 - acc: 0.7680 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4600  | total loss: \u001b[1m\u001b[32m0.55348\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4600 | loss: 0.55348 - acc: 0.7460 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4601  | total loss: \u001b[1m\u001b[32m0.54388\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4601 | loss: 0.54388 - acc: 0.7522 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4602  | total loss: \u001b[1m\u001b[32m0.57164\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4602 | loss: 0.57164 - acc: 0.7305 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4603  | total loss: \u001b[1m\u001b[32m0.56036\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4603 | loss: 0.56036 - acc: 0.7381 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4604  | total loss: \u001b[1m\u001b[32m0.55027\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4604 | loss: 0.55027 - acc: 0.7455 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4605  | total loss: \u001b[1m\u001b[32m0.54126\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4605 | loss: 0.54126 - acc: 0.7524 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4606  | total loss: \u001b[1m\u001b[32m0.53318\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4606 | loss: 0.53318 - acc: 0.7586 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4607  | total loss: \u001b[1m\u001b[32m0.52576\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4607 | loss: 0.52576 - acc: 0.7641 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4608  | total loss: \u001b[1m\u001b[32m0.55343\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4608 | loss: 0.55343 - acc: 0.7435 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4609  | total loss: \u001b[1m\u001b[32m0.54389\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4609 | loss: 0.54389 - acc: 0.7499 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4610  | total loss: \u001b[1m\u001b[32m0.57348\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4610 | loss: 0.57348 - acc: 0.7281 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4611  | total loss: \u001b[1m\u001b[32m0.56239\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4611 | loss: 0.56239 - acc: 0.7354 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4612  | total loss: \u001b[1m\u001b[32m0.55261\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4612 | loss: 0.55261 - acc: 0.7428 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4613  | total loss: \u001b[1m\u001b[32m0.54390\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 4613 | loss: 0.54390 - acc: 0.7496 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4614  | total loss: \u001b[1m\u001b[32m0.53605\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4614 | loss: 0.53605 - acc: 0.7558 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4615  | total loss: \u001b[1m\u001b[32m0.52877\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4615 | loss: 0.52877 - acc: 0.7613 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4616  | total loss: \u001b[1m\u001b[32m0.52188\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4616 | loss: 0.52188 - acc: 0.7662 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4617  | total loss: \u001b[1m\u001b[32m0.51538\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4617 | loss: 0.51538 - acc: 0.7706 -- iter: 891/891\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 4618  | total loss: \u001b[1m\u001b[32m0.54859\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4618 | loss: 0.54859 - acc: 0.7464 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4619  | total loss: \u001b[1m\u001b[32m0.53922\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4619 | loss: 0.53922 - acc: 0.7525 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4620  | total loss: \u001b[1m\u001b[32m0.53073\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4620 | loss: 0.53073 - acc: 0.7577 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4621  | total loss: \u001b[1m\u001b[32m0.52300\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4621 | loss: 0.52300 - acc: 0.7630 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4622  | total loss: \u001b[1m\u001b[32m0.51598\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4622 | loss: 0.51598 - acc: 0.7678 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4623  | total loss: \u001b[1m\u001b[32m0.50955\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4623 | loss: 0.50955 - acc: 0.7722 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4624  | total loss: \u001b[1m\u001b[32m0.50361\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4624 | loss: 0.50361 - acc: 0.7758 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4625  | total loss: \u001b[1m\u001b[32m0.49812\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4625 | loss: 0.49812 - acc: 0.7790 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4626  | total loss: \u001b[1m\u001b[32m0.53944\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4626 | loss: 0.53944 - acc: 0.7531 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4627  | total loss: \u001b[1m\u001b[32m0.53031\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4627 | loss: 0.53031 - acc: 0.7577 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4628  | total loss: \u001b[1m\u001b[32m0.52205\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4628 | loss: 0.52205 - acc: 0.7626 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4629  | total loss: \u001b[1m\u001b[32m0.51458\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4629 | loss: 0.51458 - acc: 0.7674 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4630  | total loss: \u001b[1m\u001b[32m0.50787\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4630 | loss: 0.50787 - acc: 0.7719 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4631  | total loss: \u001b[1m\u001b[32m0.50178\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4631 | loss: 0.50178 - acc: 0.7757 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4632  | total loss: \u001b[1m\u001b[32m0.54028\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4632 | loss: 0.54028 - acc: 0.7532 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4633  | total loss: \u001b[1m\u001b[32m0.53115\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4633 | loss: 0.53115 - acc: 0.7581 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4634  | total loss: \u001b[1m\u001b[32m0.56233\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4634 | loss: 0.56233 - acc: 0.7379 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4635  | total loss: \u001b[1m\u001b[32m0.55187\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4635 | loss: 0.55187 - acc: 0.7445 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4636  | total loss: \u001b[1m\u001b[32m0.54287\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4636 | loss: 0.54287 - acc: 0.7500 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4637  | total loss: \u001b[1m\u001b[32m0.53507\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4637 | loss: 0.53507 - acc: 0.7557 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4638  | total loss: \u001b[1m\u001b[32m0.52808\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4638 | loss: 0.52808 - acc: 0.7610 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4639  | total loss: \u001b[1m\u001b[32m0.52148\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4639 | loss: 0.52148 - acc: 0.7658 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4640  | total loss: \u001b[1m\u001b[32m0.51514\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4640 | loss: 0.51514 - acc: 0.7698 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4641  | total loss: \u001b[1m\u001b[32m0.50907\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4641 | loss: 0.50907 - acc: 0.7731 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4642  | total loss: \u001b[1m\u001b[32m0.54382\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4642 | loss: 0.54382 - acc: 0.7502 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4643  | total loss: \u001b[1m\u001b[32m0.53452\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4643 | loss: 0.53452 - acc: 0.7563 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4644  | total loss: \u001b[1m\u001b[32m0.52617\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4644 | loss: 0.52617 - acc: 0.7620 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4645  | total loss: \u001b[1m\u001b[32m0.51865\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4645 | loss: 0.51865 - acc: 0.7671 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4646  | total loss: \u001b[1m\u001b[32m0.55263\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4646 | loss: 0.55263 - acc: 0.7468 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4647  | total loss: \u001b[1m\u001b[32m0.54251\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4647 | loss: 0.54251 - acc: 0.7529 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4648  | total loss: \u001b[1m\u001b[32m0.56965\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4648 | loss: 0.56965 - acc: 0.7335 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4649  | total loss: \u001b[1m\u001b[32m0.55842\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4649 | loss: 0.55842 - acc: 0.7404 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4650  | total loss: \u001b[1m\u001b[32m0.58337\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 4650 | loss: 0.58337 - acc: 0.7218 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4651  | total loss: \u001b[1m\u001b[32m0.57180\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4651 | loss: 0.57180 - acc: 0.7307 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4652  | total loss: \u001b[1m\u001b[32m0.56204\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4652 | loss: 0.56204 - acc: 0.7387 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4653  | total loss: \u001b[1m\u001b[32m0.55371\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4653 | loss: 0.55371 - acc: 0.7458 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4654  | total loss: \u001b[1m\u001b[32m0.57649\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4654 | loss: 0.57649 - acc: 0.7236 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4655  | total loss: \u001b[1m\u001b[32m0.56735\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4655 | loss: 0.56735 - acc: 0.7322 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4656  | total loss: \u001b[1m\u001b[32m0.55931\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4656 | loss: 0.55931 - acc: 0.7398 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4657  | total loss: \u001b[1m\u001b[32m0.55201\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4657 | loss: 0.55201 - acc: 0.7469 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4658  | total loss: \u001b[1m\u001b[32m0.57453\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4658 | loss: 0.57453 - acc: 0.7257 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4659  | total loss: \u001b[1m\u001b[32m0.56552\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4659 | loss: 0.56552 - acc: 0.7344 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4660  | total loss: \u001b[1m\u001b[32m0.58216\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4660 | loss: 0.58216 - acc: 0.7177 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4661  | total loss: \u001b[1m\u001b[32m0.57245\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4661 | loss: 0.57245 - acc: 0.7272 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4662  | total loss: \u001b[1m\u001b[32m0.59306\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4662 | loss: 0.59306 - acc: 0.7064 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4663  | total loss: \u001b[1m\u001b[32m0.58254\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4663 | loss: 0.58254 - acc: 0.7172 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4664  | total loss: \u001b[1m\u001b[32m0.57316\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4664 | loss: 0.57316 - acc: 0.7268 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4665  | total loss: \u001b[1m\u001b[32m0.56464\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4665 | loss: 0.56464 - acc: 0.7355 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4666  | total loss: \u001b[1m\u001b[32m0.58338\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4666 | loss: 0.58338 - acc: 0.7161 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4667  | total loss: \u001b[1m\u001b[32m0.57367\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4667 | loss: 0.57367 - acc: 0.7257 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4668  | total loss: \u001b[1m\u001b[32m0.59594\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4668 | loss: 0.59594 - acc: 0.7035 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4669  | total loss: \u001b[1m\u001b[32m0.58512\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4669 | loss: 0.58512 - acc: 0.7145 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4670  | total loss: \u001b[1m\u001b[32m0.57542\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4670 | loss: 0.57542 - acc: 0.7248 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4671  | total loss: \u001b[1m\u001b[32m0.56657\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4671 | loss: 0.56657 - acc: 0.7336 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4672  | total loss: \u001b[1m\u001b[32m0.55838\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4672 | loss: 0.55838 - acc: 0.7411 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4673  | total loss: \u001b[1m\u001b[32m0.55070\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4673 | loss: 0.55070 - acc: 0.7477 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4674  | total loss: \u001b[1m\u001b[32m0.57298\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4674 | loss: 0.57298 - acc: 0.7274 -- iter: 891/891\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 4675  | total loss: \u001b[1m\u001b[32m0.56333\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4675 | loss: 0.56333 - acc: 0.7360 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4676  | total loss: \u001b[1m\u001b[32m0.58685\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4676 | loss: 0.58685 - acc: 0.7154 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4677  | total loss: \u001b[1m\u001b[32m0.57581\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4677 | loss: 0.57581 - acc: 0.7252 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4678  | total loss: \u001b[1m\u001b[32m0.59403\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4678 | loss: 0.59403 - acc: 0.7082 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4679  | total loss: \u001b[1m\u001b[32m0.58265\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4679 | loss: 0.58265 - acc: 0.7191 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4680  | total loss: \u001b[1m\u001b[32m0.59794\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4680 | loss: 0.59794 - acc: 0.7046 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4681  | total loss: \u001b[1m\u001b[32m0.58673\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4681 | loss: 0.58673 - acc: 0.7154 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4682  | total loss: \u001b[1m\u001b[32m0.60347\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4682 | loss: 0.60347 - acc: 0.6991 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4683  | total loss: \u001b[1m\u001b[32m0.59258\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4683 | loss: 0.59258 - acc: 0.7104 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4684  | total loss: \u001b[1m\u001b[32m0.60627\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4684 | loss: 0.60627 - acc: 0.6948 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4685  | total loss: \u001b[1m\u001b[32m0.59581\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4685 | loss: 0.59581 - acc: 0.7066 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4686  | total loss: \u001b[1m\u001b[32m0.58661\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4686 | loss: 0.58661 - acc: 0.7170 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4687  | total loss: \u001b[1m\u001b[32m0.57831\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4687 | loss: 0.57831 - acc: 0.7263 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4688  | total loss: \u001b[1m\u001b[32m0.57053\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4688 | loss: 0.57053 - acc: 0.7346 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4689  | total loss: \u001b[1m\u001b[32m0.56300\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4689 | loss: 0.56300 - acc: 0.7422 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4690  | total loss: \u001b[1m\u001b[32m0.58041\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4690 | loss: 0.58041 - acc: 0.7227 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4691  | total loss: \u001b[1m\u001b[32m0.57098\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4691 | loss: 0.57098 - acc: 0.7319 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4692  | total loss: \u001b[1m\u001b[32m0.59228\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4692 | loss: 0.59228 - acc: 0.7114 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4693  | total loss: \u001b[1m\u001b[32m0.58128\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4693 | loss: 0.58128 - acc: 0.7214 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4694  | total loss: \u001b[1m\u001b[32m0.57120\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4694 | loss: 0.57120 - acc: 0.7305 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4695  | total loss: \u001b[1m\u001b[32m0.56187\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4695 | loss: 0.56187 - acc: 0.7387 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4696  | total loss: \u001b[1m\u001b[32m0.55313\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4696 | loss: 0.55313 - acc: 0.7461 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4697  | total loss: \u001b[1m\u001b[32m0.54488\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4697 | loss: 0.54488 - acc: 0.7528 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4698  | total loss: \u001b[1m\u001b[32m0.53704\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4698 | loss: 0.53704 - acc: 0.7590 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4699  | total loss: \u001b[1m\u001b[32m0.52959\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4699 | loss: 0.52959 - acc: 0.7645 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4700  | total loss: \u001b[1m\u001b[32m0.52257\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4700 | loss: 0.52257 - acc: 0.7695 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4701  | total loss: \u001b[1m\u001b[32m0.51597\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4701 | loss: 0.51597 - acc: 0.7740 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4702  | total loss: \u001b[1m\u001b[32m0.50981\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4702 | loss: 0.50981 - acc: 0.7778 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4703  | total loss: \u001b[1m\u001b[32m0.50408\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4703 | loss: 0.50408 - acc: 0.7813 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4704  | total loss: \u001b[1m\u001b[32m0.49880\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4704 | loss: 0.49880 - acc: 0.7840 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4705  | total loss: \u001b[1m\u001b[32m0.49394\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4705 | loss: 0.49394 - acc: 0.7862 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4706  | total loss: \u001b[1m\u001b[32m0.48944\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4706 | loss: 0.48944 - acc: 0.7880 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4707  | total loss: \u001b[1m\u001b[32m0.48527\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4707 | loss: 0.48527 - acc: 0.7898 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4708  | total loss: \u001b[1m\u001b[32m0.48138\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4708 | loss: 0.48138 - acc: 0.7914 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4709  | total loss: \u001b[1m\u001b[32m0.47774\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4709 | loss: 0.47774 - acc: 0.7930 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4710  | total loss: \u001b[1m\u001b[32m0.47433\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4710 | loss: 0.47433 - acc: 0.7941 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4711  | total loss: \u001b[1m\u001b[32m0.47117\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4711 | loss: 0.47117 - acc: 0.7950 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4712  | total loss: \u001b[1m\u001b[32m0.52291\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4712 | loss: 0.52291 - acc: 0.7696 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4713  | total loss: \u001b[1m\u001b[32m0.51499\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4713 | loss: 0.51499 - acc: 0.7727 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4714  | total loss: \u001b[1m\u001b[32m0.50819\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4714 | loss: 0.50819 - acc: 0.7758 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4715  | total loss: \u001b[1m\u001b[32m0.50237\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4715 | loss: 0.50237 - acc: 0.7787 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4716  | total loss: \u001b[1m\u001b[32m0.49723\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4716 | loss: 0.49723 - acc: 0.7810 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4717  | total loss: \u001b[1m\u001b[32m0.49244\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4717 | loss: 0.49244 - acc: 0.7831 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4718  | total loss: \u001b[1m\u001b[32m0.48783\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4718 | loss: 0.48783 - acc: 0.7852 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4719  | total loss: \u001b[1m\u001b[32m0.48339\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4719 | loss: 0.48339 - acc: 0.7866 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4720  | total loss: \u001b[1m\u001b[32m0.52898\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4720 | loss: 0.52898 - acc: 0.7625 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4721  | total loss: \u001b[1m\u001b[32m0.52033\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4721 | loss: 0.52033 - acc: 0.7674 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4722  | total loss: \u001b[1m\u001b[32m0.51260\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4722 | loss: 0.51260 - acc: 0.7717 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4723  | total loss: \u001b[1m\u001b[32m0.50562\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4723 | loss: 0.50562 - acc: 0.7753 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4724  | total loss: \u001b[1m\u001b[32m0.54836\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4724 | loss: 0.54836 - acc: 0.7492 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4725  | total loss: \u001b[1m\u001b[32m0.53824\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4725 | loss: 0.53824 - acc: 0.7542 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4726  | total loss: \u001b[1m\u001b[32m0.52932\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4726 | loss: 0.52932 - acc: 0.7587 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4727  | total loss: \u001b[1m\u001b[32m0.52133\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4727 | loss: 0.52133 - acc: 0.7634 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4728  | total loss: \u001b[1m\u001b[32m0.55240\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4728 | loss: 0.55240 - acc: 0.7428 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4729  | total loss: \u001b[1m\u001b[32m0.54288\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4729 | loss: 0.54288 - acc: 0.7493 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4730  | total loss: \u001b[1m\u001b[32m0.53466\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4730 | loss: 0.53466 - acc: 0.7554 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4731  | total loss: \u001b[1m\u001b[32m0.52745\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4731 | loss: 0.52745 - acc: 0.7608 -- iter: 891/891\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 4732  | total loss: \u001b[1m\u001b[32m0.52111\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4732 | loss: 0.52111 - acc: 0.7646 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4733  | total loss: \u001b[1m\u001b[32m0.51538\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4733 | loss: 0.51538 - acc: 0.7683 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4734  | total loss: \u001b[1m\u001b[32m0.50998\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4734 | loss: 0.50998 - acc: 0.7720 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4735  | total loss: \u001b[1m\u001b[32m0.50481\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4735 | loss: 0.50481 - acc: 0.7758 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4736  | total loss: \u001b[1m\u001b[32m0.53807\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4736 | loss: 0.53807 - acc: 0.7529 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4737  | total loss: \u001b[1m\u001b[32m0.53000\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4737 | loss: 0.53000 - acc: 0.7587 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4738  | total loss: \u001b[1m\u001b[32m0.52271\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4738 | loss: 0.52271 - acc: 0.7639 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4739  | total loss: \u001b[1m\u001b[32m0.51604\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4739 | loss: 0.51604 - acc: 0.7685 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4740  | total loss: \u001b[1m\u001b[32m0.54631\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4740 | loss: 0.54631 - acc: 0.7458 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4741  | total loss: \u001b[1m\u001b[32m0.53744\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4741 | loss: 0.53744 - acc: 0.7516 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4742  | total loss: \u001b[1m\u001b[32m0.52953\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4742 | loss: 0.52953 - acc: 0.7570 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4743  | total loss: \u001b[1m\u001b[32m0.52234\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4743 | loss: 0.52234 - acc: 0.7627 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4744  | total loss: \u001b[1m\u001b[32m0.55688\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4744 | loss: 0.55688 - acc: 0.7390 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4745  | total loss: \u001b[1m\u001b[32m0.54726\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4745 | loss: 0.54726 - acc: 0.7462 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4746  | total loss: \u001b[1m\u001b[32m0.53877\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4746 | loss: 0.53877 - acc: 0.7525 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4747  | total loss: \u001b[1m\u001b[32m0.53113\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4747 | loss: 0.53113 - acc: 0.7584 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4748  | total loss: \u001b[1m\u001b[32m0.52419\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4748 | loss: 0.52419 - acc: 0.7634 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4749  | total loss: \u001b[1m\u001b[32m0.51778\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4749 | loss: 0.51778 - acc: 0.7679 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4750  | total loss: \u001b[1m\u001b[32m0.55159\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4750 | loss: 0.55159 - acc: 0.7431 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4751  | total loss: \u001b[1m\u001b[32m0.54226\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4751 | loss: 0.54226 - acc: 0.7499 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4752  | total loss: \u001b[1m\u001b[32m0.53389\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4752 | loss: 0.53389 - acc: 0.7562 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4753  | total loss: \u001b[1m\u001b[32m0.52629\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4753 | loss: 0.52629 - acc: 0.7618 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4754  | total loss: \u001b[1m\u001b[32m0.51929\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4754 | loss: 0.51929 - acc: 0.7669 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4755  | total loss: \u001b[1m\u001b[32m0.51277\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4755 | loss: 0.51277 - acc: 0.7714 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4756  | total loss: \u001b[1m\u001b[32m0.54583\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4756 | loss: 0.54583 - acc: 0.7470 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4757  | total loss: \u001b[1m\u001b[32m0.53656\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4757 | loss: 0.53656 - acc: 0.7530 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4758  | total loss: \u001b[1m\u001b[32m0.52827\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4758 | loss: 0.52827 - acc: 0.7586 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4759  | total loss: \u001b[1m\u001b[32m0.52073\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4759 | loss: 0.52073 - acc: 0.7634 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4760  | total loss: \u001b[1m\u001b[32m0.55440\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4760 | loss: 0.55440 - acc: 0.7402 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4761  | total loss: \u001b[1m\u001b[32m0.54441\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4761 | loss: 0.54441 - acc: 0.7474 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4762  | total loss: \u001b[1m\u001b[32m0.57301\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4762 | loss: 0.57301 - acc: 0.7265 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4763  | total loss: \u001b[1m\u001b[32m0.56187\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4763 | loss: 0.56187 - acc: 0.7349 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4764  | total loss: \u001b[1m\u001b[32m0.58531\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4764 | loss: 0.58531 - acc: 0.7152 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4765  | total loss: \u001b[1m\u001b[32m0.57427\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4765 | loss: 0.57427 - acc: 0.7244 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4766  | total loss: \u001b[1m\u001b[32m0.56495\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4766 | loss: 0.56495 - acc: 0.7325 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4767  | total loss: \u001b[1m\u001b[32m0.55684\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4767 | loss: 0.55684 - acc: 0.7404 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4768  | total loss: \u001b[1m\u001b[32m0.54955\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 4768 | loss: 0.54955 - acc: 0.7474 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4769  | total loss: \u001b[1m\u001b[32m0.54277\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4769 | loss: 0.54277 - acc: 0.7537 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4770  | total loss: \u001b[1m\u001b[32m0.56685\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 4770 | loss: 0.56685 - acc: 0.7324 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4771  | total loss: \u001b[1m\u001b[32m0.55796\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 4771 | loss: 0.55796 - acc: 0.7403 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4772  | total loss: \u001b[1m\u001b[32m0.54980\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 4772 | loss: 0.54980 - acc: 0.7475 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4773  | total loss: \u001b[1m\u001b[32m0.54218\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 4773 | loss: 0.54218 - acc: 0.7543 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4774  | total loss: \u001b[1m\u001b[32m0.53499\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4774 | loss: 0.53499 - acc: 0.7606 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4775  | total loss: \u001b[1m\u001b[32m0.52815\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4775 | loss: 0.52815 - acc: 0.7661 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4776  | total loss: \u001b[1m\u001b[32m0.52164\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4776 | loss: 0.52164 - acc: 0.7710 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4777  | total loss: \u001b[1m\u001b[32m0.51548\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 4777 | loss: 0.51548 - acc: 0.7752 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4778  | total loss: \u001b[1m\u001b[32m0.50969\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4778 | loss: 0.50969 - acc: 0.7793 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4779  | total loss: \u001b[1m\u001b[32m0.50429\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4779 | loss: 0.50429 - acc: 0.7831 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4780  | total loss: \u001b[1m\u001b[32m0.49927\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4780 | loss: 0.49927 - acc: 0.7860 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4781  | total loss: \u001b[1m\u001b[32m0.49464\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4781 | loss: 0.49464 - acc: 0.7888 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4782  | total loss: \u001b[1m\u001b[32m0.49040\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4782 | loss: 0.49040 - acc: 0.7904 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4783  | total loss: \u001b[1m\u001b[32m0.48649\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4783 | loss: 0.48649 - acc: 0.7915 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4784  | total loss: \u001b[1m\u001b[32m0.48286\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4784 | loss: 0.48286 - acc: 0.7928 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4785  | total loss: \u001b[1m\u001b[32m0.47947\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4785 | loss: 0.47947 - acc: 0.7943 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4786  | total loss: \u001b[1m\u001b[32m0.53586\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4786 | loss: 0.53586 - acc: 0.7669 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4787  | total loss: \u001b[1m\u001b[32m0.52675\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4787 | loss: 0.52675 - acc: 0.7713 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4788  | total loss: \u001b[1m\u001b[32m0.57295\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4788 | loss: 0.57295 - acc: 0.7467 -- iter: 891/891\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 4789  | total loss: \u001b[1m\u001b[32m0.55999\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4789 | loss: 0.55999 - acc: 0.7529 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4790  | total loss: \u001b[1m\u001b[32m0.54870\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4790 | loss: 0.54870 - acc: 0.7577 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4791  | total loss: \u001b[1m\u001b[32m0.53905\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4791 | loss: 0.53905 - acc: 0.7623 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4792  | total loss: \u001b[1m\u001b[32m0.53077\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4792 | loss: 0.53077 - acc: 0.7663 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4793  | total loss: \u001b[1m\u001b[32m0.52346\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4793 | loss: 0.52346 - acc: 0.7698 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4794  | total loss: \u001b[1m\u001b[32m0.51675\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4794 | loss: 0.51675 - acc: 0.7731 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4795  | total loss: \u001b[1m\u001b[32m0.51037\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4795 | loss: 0.51037 - acc: 0.7761 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4796  | total loss: \u001b[1m\u001b[32m0.50423\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4796 | loss: 0.50423 - acc: 0.7788 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4797  | total loss: \u001b[1m\u001b[32m0.49837\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4797 | loss: 0.49837 - acc: 0.7811 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4798  | total loss: \u001b[1m\u001b[32m0.49290\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4798 | loss: 0.49290 - acc: 0.7839 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4799  | total loss: \u001b[1m\u001b[32m0.48792\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4799 | loss: 0.48792 - acc: 0.7865 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4800  | total loss: \u001b[1m\u001b[32m0.48347\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4800 | loss: 0.48347 - acc: 0.7884 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4801  | total loss: \u001b[1m\u001b[32m0.47955\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4801 | loss: 0.47955 - acc: 0.7903 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4802  | total loss: \u001b[1m\u001b[32m0.47612\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4802 | loss: 0.47612 - acc: 0.7917 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4803  | total loss: \u001b[1m\u001b[32m0.47310\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4803 | loss: 0.47310 - acc: 0.7927 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4804  | total loss: \u001b[1m\u001b[32m0.52794\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 4804 | loss: 0.52794 - acc: 0.7649 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4805  | total loss: \u001b[1m\u001b[32m0.51965\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4805 | loss: 0.51965 - acc: 0.7680 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4806  | total loss: \u001b[1m\u001b[32m0.51205\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4806 | loss: 0.51205 - acc: 0.7711 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4807  | total loss: \u001b[1m\u001b[32m0.50509\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4807 | loss: 0.50509 - acc: 0.7744 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4808  | total loss: \u001b[1m\u001b[32m0.49885\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4808 | loss: 0.49885 - acc: 0.7780 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4809  | total loss: \u001b[1m\u001b[32m0.49331\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4809 | loss: 0.49331 - acc: 0.7812 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4810  | total loss: \u001b[1m\u001b[32m0.48836\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4810 | loss: 0.48836 - acc: 0.7841 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4811  | total loss: \u001b[1m\u001b[32m0.48395\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 4811 | loss: 0.48395 - acc: 0.7862 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4812  | total loss: \u001b[1m\u001b[32m0.48004\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4812 | loss: 0.48004 - acc: 0.7881 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4813  | total loss: \u001b[1m\u001b[32m0.47650\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4813 | loss: 0.47650 - acc: 0.7898 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4814  | total loss: \u001b[1m\u001b[32m0.51836\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4814 | loss: 0.51836 - acc: 0.7646 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4815  | total loss: \u001b[1m\u001b[32m0.51114\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4815 | loss: 0.51114 - acc: 0.7684 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4816  | total loss: \u001b[1m\u001b[32m0.50482\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4816 | loss: 0.50482 - acc: 0.7718 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4817  | total loss: \u001b[1m\u001b[32m0.49922\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4817 | loss: 0.49922 - acc: 0.7748 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4818  | total loss: \u001b[1m\u001b[32m0.53552\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4818 | loss: 0.53552 - acc: 0.7518 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4819  | total loss: \u001b[1m\u001b[32m0.52720\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4819 | loss: 0.52720 - acc: 0.7570 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4820  | total loss: \u001b[1m\u001b[32m0.55809\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4820 | loss: 0.55809 - acc: 0.7354 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4821  | total loss: \u001b[1m\u001b[32m0.54838\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4821 | loss: 0.54838 - acc: 0.7422 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4822  | total loss: \u001b[1m\u001b[32m0.57542\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 4822 | loss: 0.57542 - acc: 0.7200 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4823  | total loss: \u001b[1m\u001b[32m0.56525\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 4823 | loss: 0.56525 - acc: 0.7292 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4824  | total loss: \u001b[1m\u001b[32m0.55674\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 4824 | loss: 0.55674 - acc: 0.7371 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4825  | total loss: \u001b[1m\u001b[32m0.54937\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 4825 | loss: 0.54937 - acc: 0.7442 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4826  | total loss: \u001b[1m\u001b[32m0.56802\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 4826 | loss: 0.56802 - acc: 0.7264 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4827  | total loss: \u001b[1m\u001b[32m0.55980\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 4827 | loss: 0.55980 - acc: 0.7348 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4828  | total loss: \u001b[1m\u001b[32m0.55249\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 4828 | loss: 0.55249 - acc: 0.7424 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4829  | total loss: \u001b[1m\u001b[32m0.54577\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 4829 | loss: 0.54577 - acc: 0.7494 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4830  | total loss: \u001b[1m\u001b[32m0.56733\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4830 | loss: 0.56733 - acc: 0.7290 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4831  | total loss: \u001b[1m\u001b[32m0.55886\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4831 | loss: 0.55886 - acc: 0.7375 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4832  | total loss: \u001b[1m\u001b[32m0.57840\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 4832 | loss: 0.57840 - acc: 0.7195 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4833  | total loss: \u001b[1m\u001b[32m0.56895\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 4833 | loss: 0.56895 - acc: 0.7289 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4834  | total loss: \u001b[1m\u001b[32m0.58791\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 4834 | loss: 0.58791 - acc: 0.7111 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4835  | total loss: \u001b[1m\u001b[32m0.57786\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4835 | loss: 0.57786 - acc: 0.7216 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4836  | total loss: \u001b[1m\u001b[32m0.59559\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 4836 | loss: 0.59559 - acc: 0.7027 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4837  | total loss: \u001b[1m\u001b[32m0.58531\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 4837 | loss: 0.58531 - acc: 0.7139 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4838  | total loss: \u001b[1m\u001b[32m0.57626\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 4838 | loss: 0.57626 - acc: 0.7237 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4839  | total loss: \u001b[1m\u001b[32m0.56813\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4839 | loss: 0.56813 - acc: 0.7324 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4840  | total loss: \u001b[1m\u001b[32m0.58441\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 4840 | loss: 0.58441 - acc: 0.7152 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4841  | total loss: \u001b[1m\u001b[32m0.57545\u001b[0m\u001b[0m | time: 0.015s\n",
      "| Adam | epoch: 4841 | loss: 0.57545 - acc: 0.7240 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4842  | total loss: \u001b[1m\u001b[32m0.56733\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4842 | loss: 0.56733 - acc: 0.7319 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4843  | total loss: \u001b[1m\u001b[32m0.55980\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4843 | loss: 0.55980 - acc: 0.7393 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4844  | total loss: \u001b[1m\u001b[32m0.55268\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4844 | loss: 0.55268 - acc: 0.7464 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4845  | total loss: \u001b[1m\u001b[32m0.54584\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4845 | loss: 0.54584 - acc: 0.7533 -- iter: 891/891\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 4846  | total loss: \u001b[1m\u001b[32m0.53920\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 4846 | loss: 0.53920 - acc: 0.7597 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4847  | total loss: \u001b[1m\u001b[32m0.53272\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4847 | loss: 0.53272 - acc: 0.7653 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4848  | total loss: \u001b[1m\u001b[32m0.52638\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4848 | loss: 0.52638 - acc: 0.7702 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4849  | total loss: \u001b[1m\u001b[32m0.52022\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4849 | loss: 0.52022 - acc: 0.7744 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4850  | total loss: \u001b[1m\u001b[32m0.55061\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4850 | loss: 0.55061 - acc: 0.7517 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4851  | total loss: \u001b[1m\u001b[32m0.54148\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4851 | loss: 0.54148 - acc: 0.7583 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4852  | total loss: \u001b[1m\u001b[32m0.57043\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4852 | loss: 0.57043 - acc: 0.7375 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4853  | total loss: \u001b[1m\u001b[32m0.55915\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4853 | loss: 0.55915 - acc: 0.7453 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4854  | total loss: \u001b[1m\u001b[32m0.58641\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4854 | loss: 0.58641 - acc: 0.7266 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4855  | total loss: \u001b[1m\u001b[32m0.57377\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4855 | loss: 0.57377 - acc: 0.7356 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4856  | total loss: \u001b[1m\u001b[32m0.59749\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4856 | loss: 0.59749 - acc: 0.7168 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4857  | total loss: \u001b[1m\u001b[32m0.58483\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4857 | loss: 0.58483 - acc: 0.7266 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4858  | total loss: \u001b[1m\u001b[32m0.60768\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4858 | loss: 0.60768 - acc: 0.7062 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4859  | total loss: \u001b[1m\u001b[32m0.59577\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4859 | loss: 0.59577 - acc: 0.7171 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4860  | total loss: \u001b[1m\u001b[32m0.58587\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4860 | loss: 0.58587 - acc: 0.7256 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4861  | total loss: \u001b[1m\u001b[32m0.57734\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4861 | loss: 0.57734 - acc: 0.7333 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4862  | total loss: \u001b[1m\u001b[32m0.56961\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 4862 | loss: 0.56961 - acc: 0.7404 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4863  | total loss: \u001b[1m\u001b[32m0.56227\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4863 | loss: 0.56227 - acc: 0.7475 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4864  | total loss: \u001b[1m\u001b[32m0.55509\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4864 | loss: 0.55509 - acc: 0.7538 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4865  | total loss: \u001b[1m\u001b[32m0.54793\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4865 | loss: 0.54793 - acc: 0.7595 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4866  | total loss: \u001b[1m\u001b[32m0.54072\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4866 | loss: 0.54072 - acc: 0.7648 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4867  | total loss: \u001b[1m\u001b[32m0.53353\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4867 | loss: 0.53353 - acc: 0.7696 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4868  | total loss: \u001b[1m\u001b[32m0.52651\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4868 | loss: 0.52651 - acc: 0.7739 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4869  | total loss: \u001b[1m\u001b[32m0.51982\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4869 | loss: 0.51982 - acc: 0.7781 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4870  | total loss: \u001b[1m\u001b[32m0.55060\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4870 | loss: 0.55060 - acc: 0.7573 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4871  | total loss: \u001b[1m\u001b[32m0.54120\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4871 | loss: 0.54120 - acc: 0.7627 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4872  | total loss: \u001b[1m\u001b[32m0.53267\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4872 | loss: 0.53267 - acc: 0.7676 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4873  | total loss: \u001b[1m\u001b[32m0.52489\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4873 | loss: 0.52489 - acc: 0.7720 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4874  | total loss: \u001b[1m\u001b[32m0.56151\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4874 | loss: 0.56151 - acc: 0.7485 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4875  | total loss: \u001b[1m\u001b[32m0.55066\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4875 | loss: 0.55066 - acc: 0.7548 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4876  | total loss: \u001b[1m\u001b[32m0.54082\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4876 | loss: 0.54082 - acc: 0.7604 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4877  | total loss: \u001b[1m\u001b[32m0.53190\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4877 | loss: 0.53190 - acc: 0.7654 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4878  | total loss: \u001b[1m\u001b[32m0.56456\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4878 | loss: 0.56456 - acc: 0.7436 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4879  | total loss: \u001b[1m\u001b[32m0.55349\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4879 | loss: 0.55349 - acc: 0.7504 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4880  | total loss: \u001b[1m\u001b[32m0.58382\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4880 | loss: 0.58382 - acc: 0.7274 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4881  | total loss: \u001b[1m\u001b[32m0.57220\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4881 | loss: 0.57220 - acc: 0.7362 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4882  | total loss: \u001b[1m\u001b[32m0.56269\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 4882 | loss: 0.56269 - acc: 0.7440 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4883  | total loss: \u001b[1m\u001b[32m0.55465\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4883 | loss: 0.55465 - acc: 0.7508 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4884  | total loss: \u001b[1m\u001b[32m0.57590\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4884 | loss: 0.57590 - acc: 0.7316 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4885  | total loss: \u001b[1m\u001b[32m0.56702\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4885 | loss: 0.56702 - acc: 0.7395 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4886  | total loss: \u001b[1m\u001b[32m0.55901\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4886 | loss: 0.55901 - acc: 0.7469 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4887  | total loss: \u001b[1m\u001b[32m0.55143\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4887 | loss: 0.55143 - acc: 0.7532 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4888  | total loss: \u001b[1m\u001b[32m0.57432\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4888 | loss: 0.57432 - acc: 0.7314 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4889  | total loss: \u001b[1m\u001b[32m0.56461\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4889 | loss: 0.56461 - acc: 0.7396 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4890  | total loss: \u001b[1m\u001b[32m0.58524\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4890 | loss: 0.58524 - acc: 0.7204 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4891  | total loss: \u001b[1m\u001b[32m0.57443\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4891 | loss: 0.57443 - acc: 0.7297 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4892  | total loss: \u001b[1m\u001b[32m0.59453\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4892 | loss: 0.59453 - acc: 0.7107 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4893  | total loss: \u001b[1m\u001b[32m0.58320\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4893 | loss: 0.58320 - acc: 0.7213 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4894  | total loss: \u001b[1m\u001b[32m0.59974\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4894 | loss: 0.59974 - acc: 0.7050 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4895  | total loss: \u001b[1m\u001b[32m0.58854\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4895 | loss: 0.58854 - acc: 0.7163 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4896  | total loss: \u001b[1m\u001b[32m0.60558\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4896 | loss: 0.60558 - acc: 0.6998 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4897  | total loss: \u001b[1m\u001b[32m0.59461\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4897 | loss: 0.59461 - acc: 0.7113 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4898  | total loss: \u001b[1m\u001b[32m0.60947\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4898 | loss: 0.60947 - acc: 0.6927 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4899  | total loss: \u001b[1m\u001b[32m0.59906\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4899 | loss: 0.59906 - acc: 0.7047 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4900  | total loss: \u001b[1m\u001b[32m0.59000\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4900 | loss: 0.59000 - acc: 0.7152 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4901  | total loss: \u001b[1m\u001b[32m0.58182\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 4901 | loss: 0.58182 - acc: 0.7247 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4902  | total loss: \u001b[1m\u001b[32m0.59614\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 4902 | loss: 0.59614 - acc: 0.7060 -- iter: 891/891\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 4903  | total loss: \u001b[1m\u001b[32m0.58705\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4903 | loss: 0.58705 - acc: 0.7165 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4904  | total loss: \u001b[1m\u001b[32m0.57863\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4904 | loss: 0.57863 - acc: 0.7256 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4905  | total loss: \u001b[1m\u001b[32m0.57062\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4905 | loss: 0.57062 - acc: 0.7340 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4906  | total loss: \u001b[1m\u001b[32m0.56287\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4906 | loss: 0.56287 - acc: 0.7417 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4907  | total loss: \u001b[1m\u001b[32m0.55529\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4907 | loss: 0.55529 - acc: 0.7488 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4908  | total loss: \u001b[1m\u001b[32m0.57731\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4908 | loss: 0.57731 - acc: 0.7276 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4909  | total loss: \u001b[1m\u001b[32m0.56749\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4909 | loss: 0.56749 - acc: 0.7362 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4910  | total loss: \u001b[1m\u001b[32m0.55839\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4910 | loss: 0.55839 - acc: 0.7441 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4911  | total loss: \u001b[1m\u001b[32m0.54988\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4911 | loss: 0.54988 - acc: 0.7509 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4912  | total loss: \u001b[1m\u001b[32m0.54187\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4912 | loss: 0.54187 - acc: 0.7568 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4913  | total loss: \u001b[1m\u001b[32m0.53432\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4913 | loss: 0.53432 - acc: 0.7623 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4914  | total loss: \u001b[1m\u001b[32m0.56398\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4914 | loss: 0.56398 - acc: 0.7397 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4915  | total loss: \u001b[1m\u001b[32m0.55375\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4915 | loss: 0.55375 - acc: 0.7473 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4916  | total loss: \u001b[1m\u001b[32m0.58173\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4916 | loss: 0.58173 - acc: 0.7277 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4917  | total loss: \u001b[1m\u001b[32m0.56974\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4917 | loss: 0.56974 - acc: 0.7367 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4918  | total loss: \u001b[1m\u001b[32m0.55910\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4918 | loss: 0.55910 - acc: 0.7441 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4919  | total loss: \u001b[1m\u001b[32m0.54960\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4919 | loss: 0.54960 - acc: 0.7512 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4920  | total loss: \u001b[1m\u001b[32m0.54102\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4920 | loss: 0.54102 - acc: 0.7572 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4921  | total loss: \u001b[1m\u001b[32m0.53314\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4921 | loss: 0.53314 - acc: 0.7626 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4922  | total loss: \u001b[1m\u001b[32m0.52579\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4922 | loss: 0.52579 - acc: 0.7675 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4923  | total loss: \u001b[1m\u001b[32m0.51882\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4923 | loss: 0.51882 - acc: 0.7719 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4924  | total loss: \u001b[1m\u001b[32m0.54896\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4924 | loss: 0.54896 - acc: 0.7513 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4925  | total loss: \u001b[1m\u001b[32m0.53933\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4925 | loss: 0.53933 - acc: 0.7573 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4926  | total loss: \u001b[1m\u001b[32m0.56924\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4926 | loss: 0.56924 - acc: 0.7367 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4927  | total loss: \u001b[1m\u001b[32m0.55786\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4927 | loss: 0.55786 - acc: 0.7442 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4928  | total loss: \u001b[1m\u001b[32m0.58068\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4928 | loss: 0.58068 - acc: 0.7264 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4929  | total loss: \u001b[1m\u001b[32m0.56906\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4929 | loss: 0.56906 - acc: 0.7341 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4930  | total loss: \u001b[1m\u001b[32m0.59291\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4930 | loss: 0.59291 - acc: 0.7144 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4931  | total loss: \u001b[1m\u001b[32m0.58148\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4931 | loss: 0.58148 - acc: 0.7237 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4932  | total loss: \u001b[1m\u001b[32m0.57175\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4932 | loss: 0.57175 - acc: 0.7321 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4933  | total loss: \u001b[1m\u001b[32m0.56312\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4933 | loss: 0.56312 - acc: 0.7397 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4934  | total loss: \u001b[1m\u001b[32m0.58488\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4934 | loss: 0.58488 - acc: 0.7183 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4935  | total loss: \u001b[1m\u001b[32m0.57502\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4935 | loss: 0.57502 - acc: 0.7275 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4936  | total loss: \u001b[1m\u001b[32m0.59368\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4936 | loss: 0.59368 - acc: 0.7076 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4937  | total loss: \u001b[1m\u001b[32m0.58331\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4937 | loss: 0.58331 - acc: 0.7179 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4938  | total loss: \u001b[1m\u001b[32m0.60145\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4938 | loss: 0.60145 - acc: 0.6997 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4939  | total loss: \u001b[1m\u001b[32m0.59094\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4939 | loss: 0.59094 - acc: 0.7107 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4940  | total loss: \u001b[1m\u001b[32m0.60514\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4940 | loss: 0.60514 - acc: 0.6964 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4941  | total loss: \u001b[1m\u001b[32m0.59501\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4941 | loss: 0.59501 - acc: 0.7081 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4942  | total loss: \u001b[1m\u001b[32m0.60850\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4942 | loss: 0.60850 - acc: 0.6928 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4943  | total loss: \u001b[1m\u001b[32m0.59879\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4943 | loss: 0.59879 - acc: 0.7049 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4944  | total loss: \u001b[1m\u001b[32m0.61074\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4944 | loss: 0.61074 - acc: 0.6903 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4945  | total loss: \u001b[1m\u001b[32m0.60140\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4945 | loss: 0.60140 - acc: 0.7019 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4946  | total loss: \u001b[1m\u001b[32m0.61440\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4946 | loss: 0.61440 - acc: 0.6854 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4947  | total loss: \u001b[1m\u001b[32m0.60515\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4947 | loss: 0.60515 - acc: 0.6971 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4948  | total loss: \u001b[1m\u001b[32m0.59676\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4948 | loss: 0.59676 - acc: 0.7076 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4949  | total loss: \u001b[1m\u001b[32m0.58868\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4949 | loss: 0.58868 - acc: 0.7173 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4950  | total loss: \u001b[1m\u001b[32m0.60251\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4950 | loss: 0.60251 - acc: 0.7017 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4951  | total loss: \u001b[1m\u001b[32m0.59278\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4951 | loss: 0.59278 - acc: 0.7122 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4952  | total loss: \u001b[1m\u001b[32m0.60559\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4952 | loss: 0.60559 - acc: 0.6981 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4953  | total loss: \u001b[1m\u001b[32m0.59495\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4953 | loss: 0.59495 - acc: 0.7091 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4954  | total loss: \u001b[1m\u001b[32m0.58505\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4954 | loss: 0.58505 - acc: 0.7198 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4955  | total loss: \u001b[1m\u001b[32m0.57565\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4955 | loss: 0.57565 - acc: 0.7294 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4956  | total loss: \u001b[1m\u001b[32m0.56657\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4956 | loss: 0.56657 - acc: 0.7376 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4957  | total loss: \u001b[1m\u001b[32m0.55780\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4957 | loss: 0.55780 - acc: 0.7453 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4958  | total loss: \u001b[1m\u001b[32m0.54936\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4958 | loss: 0.54936 - acc: 0.7520 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4959  | total loss: \u001b[1m\u001b[32m0.54124\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4959 | loss: 0.54124 - acc: 0.7583 -- iter: 891/891\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 4960  | total loss: \u001b[1m\u001b[32m0.57169\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4960 | loss: 0.57169 - acc: 0.7356 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4961  | total loss: \u001b[1m\u001b[32m0.56075\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4961 | loss: 0.56075 - acc: 0.7436 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4962  | total loss: \u001b[1m\u001b[32m0.55087\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4962 | loss: 0.55087 - acc: 0.7503 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4963  | total loss: \u001b[1m\u001b[32m0.54174\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4963 | loss: 0.54174 - acc: 0.7562 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4964  | total loss: \u001b[1m\u001b[32m0.57211\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4964 | loss: 0.57211 - acc: 0.7345 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4965  | total loss: \u001b[1m\u001b[32m0.56065\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4965 | loss: 0.56065 - acc: 0.7430 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4966  | total loss: \u001b[1m\u001b[32m0.58789\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 4966 | loss: 0.58789 - acc: 0.7224 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4967  | total loss: \u001b[1m\u001b[32m0.57557\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4967 | loss: 0.57557 - acc: 0.7316 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4968  | total loss: \u001b[1m\u001b[32m0.59513\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4968 | loss: 0.59513 - acc: 0.7157 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4969  | total loss: \u001b[1m\u001b[32m0.58376\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4969 | loss: 0.58376 - acc: 0.7254 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4970  | total loss: \u001b[1m\u001b[32m0.60040\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4970 | loss: 0.60040 - acc: 0.7076 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4971  | total loss: \u001b[1m\u001b[32m0.59047\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4971 | loss: 0.59047 - acc: 0.7175 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4972  | total loss: \u001b[1m\u001b[32m0.60211\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4972 | loss: 0.60211 - acc: 0.7033 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4973  | total loss: \u001b[1m\u001b[32m0.59345\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4973 | loss: 0.59345 - acc: 0.7134 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4974  | total loss: \u001b[1m\u001b[32m0.58584\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4974 | loss: 0.58584 - acc: 0.7222 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4975  | total loss: \u001b[1m\u001b[32m0.57862\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 4975 | loss: 0.57862 - acc: 0.7308 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4976  | total loss: \u001b[1m\u001b[32m0.57135\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4976 | loss: 0.57135 - acc: 0.7386 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4977  | total loss: \u001b[1m\u001b[32m0.56375\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4977 | loss: 0.56375 - acc: 0.7457 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4978  | total loss: \u001b[1m\u001b[32m0.55579\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4978 | loss: 0.55579 - acc: 0.7526 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4979  | total loss: \u001b[1m\u001b[32m0.54757\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4979 | loss: 0.54757 - acc: 0.7589 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4980  | total loss: \u001b[1m\u001b[32m0.57686\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4980 | loss: 0.57686 - acc: 0.7367 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4981  | total loss: \u001b[1m\u001b[32m0.56562\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4981 | loss: 0.56562 - acc: 0.7443 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4982  | total loss: \u001b[1m\u001b[32m0.59221\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4982 | loss: 0.59221 - acc: 0.7247 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4983  | total loss: \u001b[1m\u001b[32m0.57930\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4983 | loss: 0.57930 - acc: 0.7338 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4984  | total loss: \u001b[1m\u001b[32m0.56780\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 4984 | loss: 0.56780 - acc: 0.7421 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4985  | total loss: \u001b[1m\u001b[32m0.55739\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 4985 | loss: 0.55739 - acc: 0.7496 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4986  | total loss: \u001b[1m\u001b[32m0.58019\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4986 | loss: 0.58019 - acc: 0.7307 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4987  | total loss: \u001b[1m\u001b[32m0.56879\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 4987 | loss: 0.56879 - acc: 0.7388 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4988  | total loss: \u001b[1m\u001b[32m0.55875\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4988 | loss: 0.55875 - acc: 0.7459 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4989  | total loss: \u001b[1m\u001b[32m0.54965\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4989 | loss: 0.54965 - acc: 0.7525 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4990  | total loss: \u001b[1m\u001b[32m0.57211\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4990 | loss: 0.57211 - acc: 0.7329 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4991  | total loss: \u001b[1m\u001b[32m0.56187\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4991 | loss: 0.56187 - acc: 0.7407 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4992  | total loss: \u001b[1m\u001b[32m0.58525\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4992 | loss: 0.58525 - acc: 0.7192 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4993  | total loss: \u001b[1m\u001b[32m0.57444\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4993 | loss: 0.57444 - acc: 0.7278 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4994  | total loss: \u001b[1m\u001b[32m0.59711\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4994 | loss: 0.59711 - acc: 0.7054 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4995  | total loss: \u001b[1m\u001b[32m0.58616\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4995 | loss: 0.58616 - acc: 0.7149 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4996  | total loss: \u001b[1m\u001b[32m0.57649\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4996 | loss: 0.57649 - acc: 0.7244 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4997  | total loss: \u001b[1m\u001b[32m0.56765\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4997 | loss: 0.56765 - acc: 0.7329 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4998  | total loss: \u001b[1m\u001b[32m0.58676\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4998 | loss: 0.58676 - acc: 0.7136 -- iter: 891/891\n",
      "--\n",
      "Training Step: 4999  | total loss: \u001b[1m\u001b[32m0.57656\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 4999 | loss: 0.57656 - acc: 0.7235 -- iter: 891/891\n",
      "--\n",
      "Training Step: 5000  | total loss: \u001b[1m\u001b[32m0.56718\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 5000 | loss: 0.56718 - acc: 0.7321 -- iter: 891/891\n",
      "--\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data = df\n",
    "net = tflearn.input_data(shape=[None, 9]) \n",
    "net = tflearn.fully_connected(net, 100)\n",
    "net = tflearn.fully_connected(net, 100)\n",
    "net = tflearn.fully_connected(net, 100)\n",
    "net = tflearn.fully_connected(net, 100)\n",
    "net = tflearn.fully_connected(net, 100)\n",
    "net = tflearn.fully_connected(net, 2, activation='softmax') \n",
    "net = tflearn.regression(net)\n",
    "model = tflearn.DNN(net, tensorboard_verbose=0)\n",
    "model.fit(data, labels, n_epoch=5000, batch_size=891, show_metric=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.789820</td>\n",
       "      <td>0.210180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.622158</td>\n",
       "      <td>0.377842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.783657</td>\n",
       "      <td>0.216343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.802927</td>\n",
       "      <td>0.197073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.496296</td>\n",
       "      <td>0.503704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.754429</td>\n",
       "      <td>0.245571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.434437</td>\n",
       "      <td>0.565563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.708751</td>\n",
       "      <td>0.291249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.372603</td>\n",
       "      <td>0.627397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.821632</td>\n",
       "      <td>0.178368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.814203</td>\n",
       "      <td>0.185797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.668122</td>\n",
       "      <td>0.331878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.213932</td>\n",
       "      <td>0.786068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.839618</td>\n",
       "      <td>0.160382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.326844</td>\n",
       "      <td>0.673156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.283493</td>\n",
       "      <td>0.716507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.667404</td>\n",
       "      <td>0.332597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.737835</td>\n",
       "      <td>0.262165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.516049</td>\n",
       "      <td>0.483951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.515478</td>\n",
       "      <td>0.484522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.668597</td>\n",
       "      <td>0.331403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.745226</td>\n",
       "      <td>0.254774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.240991</td>\n",
       "      <td>0.759009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.463831</td>\n",
       "      <td>0.536170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.197839</td>\n",
       "      <td>0.802161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.884213</td>\n",
       "      <td>0.115787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.165745</td>\n",
       "      <td>0.834255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.744052</td>\n",
       "      <td>0.255948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.640791</td>\n",
       "      <td>0.359209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.816114</td>\n",
       "      <td>0.183886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>388</th>\n",
       "      <td>0.737415</td>\n",
       "      <td>0.262585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389</th>\n",
       "      <td>0.802551</td>\n",
       "      <td>0.197449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>390</th>\n",
       "      <td>0.502222</td>\n",
       "      <td>0.497778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>391</th>\n",
       "      <td>0.336164</td>\n",
       "      <td>0.663836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>392</th>\n",
       "      <td>0.759879</td>\n",
       "      <td>0.240121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393</th>\n",
       "      <td>0.769604</td>\n",
       "      <td>0.230396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394</th>\n",
       "      <td>0.869461</td>\n",
       "      <td>0.130539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>0.206643</td>\n",
       "      <td>0.793357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>0.750033</td>\n",
       "      <td>0.249967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>0.277718</td>\n",
       "      <td>0.722282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.214286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>0.777048</td>\n",
       "      <td>0.222952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400</th>\n",
       "      <td>0.177071</td>\n",
       "      <td>0.822929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>401</th>\n",
       "      <td>0.755834</td>\n",
       "      <td>0.244166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>402</th>\n",
       "      <td>0.166773</td>\n",
       "      <td>0.833227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403</th>\n",
       "      <td>0.503208</td>\n",
       "      <td>0.496792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404</th>\n",
       "      <td>0.630389</td>\n",
       "      <td>0.369611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>405</th>\n",
       "      <td>0.591192</td>\n",
       "      <td>0.408808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>406</th>\n",
       "      <td>0.697685</td>\n",
       "      <td>0.302315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>407</th>\n",
       "      <td>0.549281</td>\n",
       "      <td>0.450719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>408</th>\n",
       "      <td>0.435819</td>\n",
       "      <td>0.564181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>409</th>\n",
       "      <td>0.394283</td>\n",
       "      <td>0.605717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>410</th>\n",
       "      <td>0.435798</td>\n",
       "      <td>0.564202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411</th>\n",
       "      <td>0.219852</td>\n",
       "      <td>0.780149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>412</th>\n",
       "      <td>0.484458</td>\n",
       "      <td>0.515542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>0.814136</td>\n",
       "      <td>0.185864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>414</th>\n",
       "      <td>0.193802</td>\n",
       "      <td>0.806198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>415</th>\n",
       "      <td>0.839843</td>\n",
       "      <td>0.160157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416</th>\n",
       "      <td>0.814136</td>\n",
       "      <td>0.185864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>0.799205</td>\n",
       "      <td>0.200795</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>418 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1\n",
       "0    0.789820  0.210180\n",
       "1    0.622158  0.377842\n",
       "2    0.783657  0.216343\n",
       "3    0.802927  0.197073\n",
       "4    0.496296  0.503704\n",
       "5    0.754429  0.245571\n",
       "6    0.434437  0.565563\n",
       "7    0.708751  0.291249\n",
       "8    0.372603  0.627397\n",
       "9    0.821632  0.178368\n",
       "10   0.814203  0.185797\n",
       "11   0.668122  0.331878\n",
       "12   0.213932  0.786068\n",
       "13   0.839618  0.160382\n",
       "14   0.326844  0.673156\n",
       "15   0.283493  0.716507\n",
       "16   0.667404  0.332597\n",
       "17   0.737835  0.262165\n",
       "18   0.516049  0.483951\n",
       "19   0.515478  0.484522\n",
       "20   0.668597  0.331403\n",
       "21   0.745226  0.254774\n",
       "22   0.240991  0.759009\n",
       "23   0.463831  0.536170\n",
       "24   0.197839  0.802161\n",
       "25   0.884213  0.115787\n",
       "26   0.165745  0.834255\n",
       "27   0.744052  0.255948\n",
       "28   0.640791  0.359209\n",
       "29   0.816114  0.183886\n",
       "..        ...       ...\n",
       "388  0.737415  0.262585\n",
       "389  0.802551  0.197449\n",
       "390  0.502222  0.497778\n",
       "391  0.336164  0.663836\n",
       "392  0.759879  0.240121\n",
       "393  0.769604  0.230396\n",
       "394  0.869461  0.130539\n",
       "395  0.206643  0.793357\n",
       "396  0.750033  0.249967\n",
       "397  0.277718  0.722282\n",
       "398  0.785714  0.214286\n",
       "399  0.777048  0.222952\n",
       "400  0.177071  0.822929\n",
       "401  0.755834  0.244166\n",
       "402  0.166773  0.833227\n",
       "403  0.503208  0.496792\n",
       "404  0.630389  0.369611\n",
       "405  0.591192  0.408808\n",
       "406  0.697685  0.302315\n",
       "407  0.549281  0.450719\n",
       "408  0.435819  0.564181\n",
       "409  0.394283  0.605717\n",
       "410  0.435798  0.564202\n",
       "411  0.219852  0.780149\n",
       "412  0.484458  0.515542\n",
       "413  0.814136  0.185864\n",
       "414  0.193802  0.806198\n",
       "415  0.839843  0.160157\n",
       "416  0.814136  0.185864\n",
       "417  0.799205  0.200795\n",
       "\n",
       "[418 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = model.predict(test)\n",
    "pred = pd.DataFrame(pred)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "end = pd.read_csv('../titanic_kaggle/test.csv')\n",
    "end['Survival_chance'] = pred[1]\n",
    "end['Die_chance'] = pred[0]\n",
    "state = []\n",
    "for i in range(len(end.Survival_chance)):\n",
    "    if end.Survival_chance[i] > end.Die_chance[i]:\n",
    "        state.append(1)\n",
    "    else:\n",
    "        state.append(0)\n",
    "state\n",
    "end['Survived'] = state\n",
    "submit = end[['PassengerId', 'Survived']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>892</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>893</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>894</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>895</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>896</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>897</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>898</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>899</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>900</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>901</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>902</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>903</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>904</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>905</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>906</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>907</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>908</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>909</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>910</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>911</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>912</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>913</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>914</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>915</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>916</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>917</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>918</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>919</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>920</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>921</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>388</th>\n",
       "      <td>1280</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389</th>\n",
       "      <td>1281</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>390</th>\n",
       "      <td>1282</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>391</th>\n",
       "      <td>1283</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>392</th>\n",
       "      <td>1284</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393</th>\n",
       "      <td>1285</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394</th>\n",
       "      <td>1286</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>1287</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>1288</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>1289</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>1290</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>1291</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400</th>\n",
       "      <td>1292</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>401</th>\n",
       "      <td>1293</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>402</th>\n",
       "      <td>1294</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403</th>\n",
       "      <td>1295</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404</th>\n",
       "      <td>1296</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>405</th>\n",
       "      <td>1297</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>406</th>\n",
       "      <td>1298</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>407</th>\n",
       "      <td>1299</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>408</th>\n",
       "      <td>1300</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>409</th>\n",
       "      <td>1301</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>410</th>\n",
       "      <td>1302</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411</th>\n",
       "      <td>1303</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>412</th>\n",
       "      <td>1304</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>1305</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>414</th>\n",
       "      <td>1306</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>415</th>\n",
       "      <td>1307</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416</th>\n",
       "      <td>1308</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>1309</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>418 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     PassengerId  Survived\n",
       "0            892         0\n",
       "1            893         0\n",
       "2            894         0\n",
       "3            895         0\n",
       "4            896         1\n",
       "5            897         0\n",
       "6            898         1\n",
       "7            899         0\n",
       "8            900         1\n",
       "9            901         0\n",
       "10           902         0\n",
       "11           903         0\n",
       "12           904         1\n",
       "13           905         0\n",
       "14           906         1\n",
       "15           907         1\n",
       "16           908         0\n",
       "17           909         0\n",
       "18           910         0\n",
       "19           911         0\n",
       "20           912         0\n",
       "21           913         0\n",
       "22           914         1\n",
       "23           915         1\n",
       "24           916         1\n",
       "25           917         0\n",
       "26           918         1\n",
       "27           919         0\n",
       "28           920         0\n",
       "29           921         0\n",
       "..           ...       ...\n",
       "388         1280         0\n",
       "389         1281         0\n",
       "390         1282         0\n",
       "391         1283         1\n",
       "392         1284         0\n",
       "393         1285         0\n",
       "394         1286         0\n",
       "395         1287         1\n",
       "396         1288         0\n",
       "397         1289         1\n",
       "398         1290         0\n",
       "399         1291         0\n",
       "400         1292         1\n",
       "401         1293         0\n",
       "402         1294         1\n",
       "403         1295         0\n",
       "404         1296         0\n",
       "405         1297         0\n",
       "406         1298         0\n",
       "407         1299         0\n",
       "408         1300         1\n",
       "409         1301         1\n",
       "410         1302         1\n",
       "411         1303         1\n",
       "412         1304         1\n",
       "413         1305         0\n",
       "414         1306         1\n",
       "415         1307         0\n",
       "416         1308         0\n",
       "417         1309         0\n",
       "\n",
       "[418 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submit.to_csv('titanic_pred.csv',  index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
